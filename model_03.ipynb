{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# from preprocessing.preprocessing import ecb_pipeline_en, fast_detect\n",
    "from preprocessing.outlier_detection import remove_outlier\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"data/train_series.csv\"\n",
    "FILENAME_ECB = \"data/ecb_data_preprocessed.csv\"\n",
    "FILENAME_FED = \"data/fed_data_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv(FILENAME, index_col=0)\n",
    "ecb = pd.read_csv(FILENAME_ECB, index_col=0)\n",
    "fed = pd.read_csv(FILENAME_FED, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = remove_outlier(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.get_dummies(returns, columns=[\"Index Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[\"Sign\"] = (returns[\"Index + 1\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>-0.027519</td>\n",
       "      <td>-0.103565</td>\n",
       "      <td>-0.045086</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.054050</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.008436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-0.011304</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004980</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.001083</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.001623</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.006444</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "0   0.001045   0.005841   0.003832  -0.027519  -0.103565  -0.045086   \n",
       "1  -0.021497   0.007891  -0.013175  -0.008436   0.000000   0.026303   \n",
       "2  -0.001872  -0.008154   0.023588   0.004086   0.003493   0.003300   \n",
       "3   0.004980  -0.000864   0.001677   0.000000   0.006030  -0.001083   \n",
       "4   0.000360  -0.001893   0.005579  -0.003056  -0.001171  -0.001623   \n",
       "\n",
       "   Index - 3  Index - 2  Index - 1  Index - 0  ... Index Name_CVIX Index  \\\n",
       "0  -0.011265   0.005164   0.054050   0.015779  ...                     0   \n",
       "1   0.000556   0.001455   0.007422   0.000000  ...                     0   \n",
       "2   0.000885  -0.011304   0.005040   0.000156  ...                     0   \n",
       "3   0.000419   0.001492   0.001018  -0.002582  ...                     0   \n",
       "4  -0.002350  -0.006444  -0.000729  -0.000365  ...                     0   \n",
       "\n",
       "  Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "0                        0                            0   \n",
       "1                        0                            0   \n",
       "2                        0                            0   \n",
       "3                        0                            0   \n",
       "4                        1                            0   \n",
       "\n",
       "   Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "0                      0                     0                       0   \n",
       "1                      1                     0                       0   \n",
       "2                      0                     1                       0   \n",
       "3                      0                     1                       0   \n",
       "4                      0                     0                       0   \n",
       "\n",
       "   Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  Sign  \n",
       "0                      0                     1                     0     1  \n",
       "1                      0                     0                     0     1  \n",
       "2                      0                     0                     0     1  \n",
       "3                      0                     0                     0     1  \n",
       "4                      0                     0                     0     1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontextual_cols = ['Index - 9',\n",
    " 'Index - 8',\n",
    " 'Index - 7',\n",
    " 'Index - 6',\n",
    " 'Index - 5',\n",
    " 'Index - 4',\n",
    " 'Index - 3',\n",
    " 'Index - 2',\n",
    " 'Index - 1',\n",
    " 'Index - 0',\n",
    " 'Index Name_CVIX Index',\n",
    " 'Index Name_EURUSD Curncy',\n",
    " 'Index Name_EURUSDV1M Curncy',\n",
    " 'Index Name_MOVE Index',\n",
    " 'Index Name_SPX Index',\n",
    " 'Index Name_SRVIX Index',\n",
    " 'Index Name_SX5E Index',\n",
    " 'Index Name_V2X Index',\n",
    " 'Index Name_VIX Index']\n",
    "nb_nontextfeatures = len(nontextual_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4519\n",
       "1    4007\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = returns[\"Sign\"]\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns.drop([\"Sign\", \"Index + 1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% val, 20% test\n",
    "\n",
    "returns_, returns_test, y_, y_test = train_test_split(\n",
    "    returns, y, test_size=0.2, train_size=0.8,\n",
    "    random_state=0, stratify=y\n",
    "    )\n",
    "\n",
    "returns_train, returns_val, y_train, y_val = train_test_split(\n",
    "    returns_, y_, test_size=0.25, train_size=0.75,\n",
    "    random_state=42, stratify=y_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reattach labels to train\n",
    "returns_train = pd.concat([returns_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_train[\"index ecb\"] = returns_train[\"index ecb\"].str.split(\",\")\n",
    "returns_train[\"index fed\"] = returns_train[\"index fed\"].str.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat samples containing multiple ECB and FED announcements, so that each sample contains only one ECB and one FED announcement\n",
    "returns_train = returns_train.explode(\"index ecb\")\n",
    "returns_train = returns_train.explode(\"index fed\")\n",
    "\n",
    "returns_train = returns_train.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4645</th>\n",
       "      <td>-0.004264</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>-0.042925</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>-0.008969</td>\n",
       "      <td>0.039740</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>-0.008518</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>-0.040893</td>\n",
       "      <td>-0.008025</td>\n",
       "      <td>0.069172</td>\n",
       "      <td>0.026382</td>\n",
       "      <td>-0.036458</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>0.005284</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>0.010213</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>-0.011616</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-0.003657</td>\n",
       "      <td>-0.024653</td>\n",
       "      <td>-0.006352</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>-0.039405</td>\n",
       "      <td>-0.016078</td>\n",
       "      <td>0.018308</td>\n",
       "      <td>0.061411</td>\n",
       "      <td>-0.008718</td>\n",
       "      <td>-0.004843</td>\n",
       "      <td>-0.030184</td>\n",
       "      <td>0.039245</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6586</th>\n",
       "      <td>-0.002471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.030129</td>\n",
       "      <td>0.008506</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>-0.026157</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.028218</td>\n",
       "      <td>-0.016704</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>0.044951</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>-0.056247</td>\n",
       "      <td>0.059887</td>\n",
       "      <td>-0.064720</td>\n",
       "      <td>-0.079639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>0.027124</td>\n",
       "      <td>0.038878</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>-0.000974</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>-0.009217</td>\n",
       "      <td>-0.003837</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>-0.011104</td>\n",
       "      <td>-0.002672</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>-0.068951</td>\n",
       "      <td>-0.049201</td>\n",
       "      <td>-0.025539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>-0.009557</td>\n",
       "      <td>-0.017437</td>\n",
       "      <td>-0.048725</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.080720</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>-0.068951</td>\n",
       "      <td>-0.049201</td>\n",
       "      <td>-0.025539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>-0.009557</td>\n",
       "      <td>-0.017437</td>\n",
       "      <td>-0.048725</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.080720</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8301</th>\n",
       "      <td>-0.004144</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>-0.007618</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>-0.009423</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004799</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7210 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "4645  -0.004264   0.008511   0.016807   0.008299   0.000000  -0.016667   \n",
       "3230  -0.008518   0.003416  -0.040893  -0.008025   0.069172   0.026382   \n",
       "1301   0.005284  -0.000998   0.010213  -0.004101  -0.011616   0.001504   \n",
       "1088  -0.039405  -0.016078   0.018308   0.061411  -0.008718  -0.004843   \n",
       "6586  -0.002471   0.000000   0.000000  -0.030129   0.008506   0.004636   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2623   0.044951   0.004568  -0.056247   0.059887  -0.064720  -0.079639   \n",
       "4855   0.002891   0.012038  -0.000974   0.000905   0.008521  -0.009217   \n",
       "1076  -0.068951  -0.049201  -0.025539   0.000000   0.070942  -0.009557   \n",
       "1076  -0.068951  -0.049201  -0.025539   0.000000   0.070942  -0.009557   \n",
       "8301  -0.004144   0.011801   0.011421  -0.007618   0.005473  -0.009423   \n",
       "\n",
       "      Index - 3  Index - 2  Index - 1  Index - 0  ... Index Name_CVIX Index  \\\n",
       "4645  -0.042925  -0.017700  -0.008969   0.039740  ...                     0   \n",
       "3230  -0.036458   0.010076   0.000000   0.000000  ...                     0   \n",
       "1301  -0.003657  -0.024653  -0.006352   0.012444  ...                     0   \n",
       "1088  -0.030184   0.039245   0.028455   0.016807  ...                     0   \n",
       "6586  -0.026157   0.004215   0.028218  -0.016704  ...                     0   \n",
       "...         ...        ...        ...        ...  ...                   ...   \n",
       "2623   0.000000   0.029976   0.027124   0.038878  ...                     0   \n",
       "4855  -0.003837   0.006340  -0.011104  -0.002672  ...                     0   \n",
       "1076  -0.017437  -0.048725   0.008853   0.080720  ...                     0   \n",
       "1076  -0.017437  -0.048725   0.008853   0.080720  ...                     0   \n",
       "8301  -0.002443   0.000000  -0.004799  -0.001219  ...                     0   \n",
       "\n",
       "     Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "4645                        0                            1   \n",
       "3230                        0                            0   \n",
       "1301                        1                            0   \n",
       "1088                        0                            1   \n",
       "6586                        0                            0   \n",
       "...                       ...                          ...   \n",
       "2623                        0                            0   \n",
       "4855                        1                            0   \n",
       "1076                        0                            0   \n",
       "1076                        0                            0   \n",
       "8301                        0                            0   \n",
       "\n",
       "      Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "4645                      0                     0                       0   \n",
       "3230                      1                     0                       0   \n",
       "1301                      0                     0                       0   \n",
       "1088                      0                     0                       0   \n",
       "6586                      0                     0                       0   \n",
       "...                     ...                   ...                     ...   \n",
       "2623                      1                     0                       0   \n",
       "4855                      0                     0                       0   \n",
       "1076                      0                     0                       0   \n",
       "1076                      0                     0                       0   \n",
       "8301                      0                     1                       0   \n",
       "\n",
       "      Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  Sign  \n",
       "4645                      0                     0                     0     0  \n",
       "3230                      0                     0                     0     0  \n",
       "1301                      0                     0                     0     1  \n",
       "1088                      0                     0                     0     0  \n",
       "6586                      1                     0                     0     0  \n",
       "...                     ...                   ...                   ...   ...  \n",
       "2623                      0                     0                     0     1  \n",
       "4855                      0                     0                     0     1  \n",
       "1076                      0                     0                     1     1  \n",
       "1076                      0                     0                     1     1  \n",
       "8301                      0                     0                     0     0  \n",
       "\n",
       "[7210 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3810\n",
       "1    3400\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = returns_train[\"Sign\"]\n",
    "returns_train = returns_train.drop([\"Sign\"], axis=1)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    904\n",
       "1    801\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1273\n",
       "1    1113\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make another version of the validation set with only one ECB and one FED announcement per sample\n",
    "\n",
    "returns_val_t = pd.concat([returns_val, y_val], axis=1)\n",
    "\n",
    "returns_val_t[\"index ecb\"] = returns_val_t[\"index ecb\"].str.split(\",\")\n",
    "returns_val_t[\"index fed\"] = returns_val_t[\"index fed\"].str.split(\",\")\n",
    "returns_val_t = returns_val_t.explode(\"index ecb\")\n",
    "returns_val_t = returns_val_t.explode(\"index fed\")\n",
    "returns_val_t = returns_val_t.dropna()\n",
    "\n",
    "y_val_t = returns_val_t[\"Sign\"]\n",
    "returns_val_t = returns_val_t.drop([\"Sign\"], axis=1)\n",
    "y_val_t.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    904\n",
       "1    802\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textual data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_dataset import get_data_loader\n",
    "from model.framework_model import CorpusEncoder, ClassificationHead, MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_03\",\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.,\n",
    "\n",
    "    \"batch_size\": 32,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.3,\n",
    "\n",
    "    \"separate\": True,\n",
    "    \n",
    "    \"max_corpus_len\": 1\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set_t, val_loader_t, tokenizer, steps = get_data_loader(\n",
    "    returns_val_t, ecb, fed, y_val_t, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import distilbert-base-uncased\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_model import MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.to(device)\n",
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226/226 [09:51<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use distilbert to compute the embeddings of each text\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        cls_ecb = bert(batch[\"X_ecb\"].squeeze(1).to(device), batch[\"X_ecb_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "        cls_fed = bert(batch[\"X_fed\"].squeeze(1).to(device), batch[\"X_fed_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate X_ind and cls_ecb and cls_fed\n",
    "        X_ind = batch[\"X_ind\"].to(device)\n",
    "        X = torch.cat((X_ind, cls_ecb, cls_fed), dim=1)\n",
    "\n",
    "        # Add a copy of the result to the train samples\n",
    "        train_samples.append(X.detach().cpu().numpy())\n",
    "        train_labels.append(batch[\"label\"].detach().cpu().numpy())\n",
    "\n",
    "train_samples = np.concatenate(train_samples)\n",
    "train_labels = np.concatenate(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [02:22<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# On validation set\n",
    "val_samples = []\n",
    "val_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        cls_ecb = bert(batch[\"X_ecb\"].squeeze(1).to(device), batch[\"X_ecb_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "        cls_fed = bert(batch[\"X_fed\"].squeeze(1).to(device), batch[\"X_fed_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate X_ind and cls_ecb and cls_fed\n",
    "        X_ind = batch[\"X_ind\"].to(device)\n",
    "        X = torch.cat((X_ind, cls_ecb, cls_fed), dim=1)\n",
    "\n",
    "        # Add a copy of the result to the train samples\n",
    "        val_samples.append(X.detach().cpu().numpy())\n",
    "        val_labels.append(batch[\"label\"].detach().cpu().numpy())\n",
    "\n",
    "val_samples = np.concatenate(val_samples)\n",
    "val_labels = np.concatenate(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [03:23<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use distilbert to compute the embeddings of each text for the validation set with only one ECB and one FED announcement per sample\n",
    "val_samples_t = []\n",
    "val_labels_t = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader_t):\n",
    "        cls_ecb = bert(batch[\"X_ecb\"].squeeze(1).to(device), batch[\"X_ecb_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "        cls_fed = bert(batch[\"X_fed\"].squeeze(1).to(device), batch[\"X_fed_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate X_ind and cls_ecb and cls_fed\n",
    "        X_ind = batch[\"X_ind\"].to(device)\n",
    "        X = torch.cat((X_ind, cls_ecb, cls_fed), dim=1)\n",
    "\n",
    "        # Add a copy of the result to the train samples\n",
    "        val_samples_t.append(X.detach().cpu().numpy())\n",
    "        val_labels_t.append(batch[\"label\"].detach().cpu().numpy())\n",
    "    \n",
    "val_samples_t = np.concatenate(val_samples_t)\n",
    "val_labels_t = np.concatenate(val_labels_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [02:28<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use distilbert to compute the embeddings of each text for the test set\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        cls_ecb = bert(batch[\"X_ecb\"].squeeze(1).to(device), batch[\"X_ecb_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "        cls_fed = bert(batch[\"X_fed\"].squeeze(1).to(device), batch[\"X_fed_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate X_ind and cls_ecb and cls_fed\n",
    "        X_ind = batch[\"X_ind\"].to(device)\n",
    "        X = torch.cat((X_ind, cls_ecb, cls_fed), dim=1)\n",
    "\n",
    "        # Add a copy of the result to the train samples\n",
    "        test_samples.append(X.detach().cpu().numpy())\n",
    "        test_labels.append(batch[\"label\"].detach().cpu().numpy())\n",
    "\n",
    "test_samples = np.concatenate(test_samples)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5662368112543963"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test LGBMClassifier on the data.\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(train_samples, train_labels)\n",
    "\n",
    "lgbm.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5773739742086753"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test XGBClassifier on the data.\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(train_samples, train_labels)\n",
    "\n",
    "xgb.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-05 09:42:16,768]\u001b[0m A new study created in memory with name: no-name-90e7da6c-8c4e-4d14-88f0-3e51ef468e9f\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:42:25,941]\u001b[0m Trial 0 finished with value: 0.5302052785923753 and parameters: {'max_depth': 10, 'learning_rate': 0.001023277511457471, 'colsample_bytree': 0.637353750597351}. Best is trial 0 with value: 0.5302052785923753.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:42:36,062]\u001b[0m Trial 1 finished with value: 0.5513196480938416 and parameters: {'max_depth': 13, 'learning_rate': 0.016408079007132387, 'colsample_bytree': 0.8209747283017248}. Best is trial 1 with value: 0.5513196480938416.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:42:45,820]\u001b[0m Trial 2 finished with value: 0.555425219941349 and parameters: {'max_depth': 9, 'learning_rate': 0.09996910268840384, 'colsample_bytree': 0.8615577999008968}. Best is trial 2 with value: 0.555425219941349.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:42:55,465]\u001b[0m Trial 3 finished with value: 0.5595307917888563 and parameters: {'max_depth': 13, 'learning_rate': 0.0630535075347635, 'colsample_bytree': 0.7702359830616647}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:06,766]\u001b[0m Trial 4 finished with value: 0.5530791788856305 and parameters: {'max_depth': 16, 'learning_rate': 0.013961893933896482, 'colsample_bytree': 0.9131487518311685}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:12,776]\u001b[0m Trial 5 finished with value: 0.543108504398827 and parameters: {'max_depth': 5, 'learning_rate': 0.06742247576269862, 'colsample_bytree': 0.9589568527990455}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:21,852]\u001b[0m Trial 6 finished with value: 0.5272727272727272 and parameters: {'max_depth': 20, 'learning_rate': 0.0026176920015263355, 'colsample_bytree': 0.6427846658261286}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:32,351]\u001b[0m Trial 7 finished with value: 0.5278592375366569 and parameters: {'max_depth': 19, 'learning_rate': 0.0028462294787562274, 'colsample_bytree': 0.7942122812337367}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:40,853]\u001b[0m Trial 8 finished with value: 0.5519061583577712 and parameters: {'max_depth': 14, 'learning_rate': 0.016685093477355965, 'colsample_bytree': 0.6285509967130201}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:50,473]\u001b[0m Trial 9 finished with value: 0.5302052785923753 and parameters: {'max_depth': 20, 'learning_rate': 0.004531768332783942, 'colsample_bytree': 0.6243898311932117}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:56,910]\u001b[0m Trial 10 finished with value: 0.5348973607038123 and parameters: {'max_depth': 9, 'learning_rate': 0.04134426667962119, 'colsample_bytree': 0.5126327641519337}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:06,738]\u001b[0m Trial 11 finished with value: 0.5489736070381231 and parameters: {'max_depth': 9, 'learning_rate': 0.07857240696115296, 'colsample_bytree': 0.8578507814011191}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:11,920]\u001b[0m Trial 12 finished with value: 0.5542521994134897 and parameters: {'max_depth': 5, 'learning_rate': 0.03593207469637426, 'colsample_bytree': 0.7331365649255068}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:20,755]\u001b[0m Trial 13 finished with value: 0.5472140762463343 and parameters: {'max_depth': 11, 'learning_rate': 0.09683502384439244, 'colsample_bytree': 0.7335439115737579}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:29,175]\u001b[0m Trial 14 finished with value: 0.5466275659824047 and parameters: {'max_depth': 7, 'learning_rate': 0.03386346653511086, 'colsample_bytree': 0.8919918986353758}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:40,640]\u001b[0m Trial 15 finished with value: 0.5419354838709678 and parameters: {'max_depth': 15, 'learning_rate': 0.04114248226447609, 'colsample_bytree': 0.9900449208263666}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:50,807]\u001b[0m Trial 16 finished with value: 0.5395894428152492 and parameters: {'max_depth': 17, 'learning_rate': 0.00751303114085499, 'colsample_bytree': 0.7793440052206259}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:59,933]\u001b[0m Trial 17 finished with value: 0.5507331378299121 and parameters: {'max_depth': 12, 'learning_rate': 0.02536532019429633, 'colsample_bytree': 0.71948009532876}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:08,848]\u001b[0m Trial 18 finished with value: 0.535483870967742 and parameters: {'max_depth': 8, 'learning_rate': 0.0586232710309152, 'colsample_bytree': 0.8499539488226538}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:20,222]\u001b[0m Trial 19 finished with value: 0.5348973607038123 and parameters: {'max_depth': 12, 'learning_rate': 0.008091753281364944, 'colsample_bytree': 0.9246714032259463}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:26,885]\u001b[0m Trial 20 finished with value: 0.5577712609970674 and parameters: {'max_depth': 7, 'learning_rate': 0.09700170225709961, 'colsample_bytree': 0.6778151187216029}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:34,255]\u001b[0m Trial 21 finished with value: 0.5571847507331378 and parameters: {'max_depth': 7, 'learning_rate': 0.09998540266863785, 'colsample_bytree': 0.6891668589087513}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:41,649]\u001b[0m Trial 22 finished with value: 0.5536656891495602 and parameters: {'max_depth': 7, 'learning_rate': 0.05366488272686656, 'colsample_bytree': 0.6889384316936391}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:47,387]\u001b[0m Trial 23 finished with value: 0.533724340175953 and parameters: {'max_depth': 6, 'learning_rate': 0.025123974108785765, 'colsample_bytree': 0.5790514633828099}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:54,406]\u001b[0m Trial 24 finished with value: 0.5419354838709678 and parameters: {'max_depth': 7, 'learning_rate': 0.06000411259868715, 'colsample_bytree': 0.6874862869726936}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:01,630]\u001b[0m Trial 25 finished with value: 0.5607038123167155 and parameters: {'max_depth': 11, 'learning_rate': 0.09525071744941882, 'colsample_bytree': 0.5622215403878025}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:08,794]\u001b[0m Trial 26 finished with value: 0.546041055718475 and parameters: {'max_depth': 11, 'learning_rate': 0.05225717993160472, 'colsample_bytree': 0.506873349929452}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:16,452]\u001b[0m Trial 27 finished with value: 0.5413489736070382 and parameters: {'max_depth': 14, 'learning_rate': 0.02432200358081281, 'colsample_bytree': 0.5476445324011641}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:24,023]\u001b[0m Trial 28 finished with value: 0.5536656891495602 and parameters: {'max_depth': 17, 'learning_rate': 0.07654035268978754, 'colsample_bytree': 0.5820156481935392}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:34,380]\u001b[0m Trial 29 finished with value: 0.5302052785923753 and parameters: {'max_depth': 10, 'learning_rate': 0.0011246275911650765, 'colsample_bytree': 0.7759660348119538}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:42,120]\u001b[0m Trial 30 finished with value: 0.547800586510264 and parameters: {'max_depth': 11, 'learning_rate': 0.046844075243737016, 'colsample_bytree': 0.6032741966237659}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:49,682]\u001b[0m Trial 31 finished with value: 0.5548387096774193 and parameters: {'max_depth': 8, 'learning_rate': 0.09505496585286473, 'colsample_bytree': 0.67476733822891}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:58,091]\u001b[0m Trial 32 finished with value: 0.5419354838709678 and parameters: {'max_depth': 13, 'learning_rate': 0.07068278224972671, 'colsample_bytree': 0.6563313318422814}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:47:06,901]\u001b[0m Trial 33 finished with value: 0.5472140762463343 and parameters: {'max_depth': 10, 'learning_rate': 0.09568757876102378, 'colsample_bytree': 0.7147941269354628}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Use optuna to find the best parameters for LGBMClassifier\n",
    "\n",
    "def objective(trial):\n",
    "    lgbm = LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 5, 20),\n",
    "        learning_rate=trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "        colsample_bytree=trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        # reg_lambda=trial.suggest_loguniform(\"reg_lambda\", 1e-3, 1e3),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    lgbm.fit(train_samples, train_labels)\n",
    "\n",
    "    return lgbm.score(val_samples, val_labels)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5949720670391061"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best parameters\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=study.best_params[\"max_depth\"],\n",
    "    learning_rate=study.best_params[\"learning_rate\"],\n",
    "    colsample_bytree=study.best_params[\"colsample_bytree\"],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(train_samples, train_labels)\n",
    "\n",
    "lgbm.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concate the train and validation set\n",
    "train_samples_ = np.concatenate((train_samples, val_samples_t))\n",
    "train_labels_ = np.concatenate((train_labels, val_labels_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.fit(train_samples_, train_labels_)\n",
    "lgbm.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45107971207677955"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_labels == 1.0).sum() / len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-05 09:14:50,925]\u001b[0m A new study created in memory with name: no-name-7cbe8eeb-6f50-4d4e-839d-9783521da015\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:15:19,188]\u001b[0m Trial 0 finished with value: 0.5818893236444941 and parameters: {'max_depth': 28, 'learning_rate': 0.38063093808450105, 'colsample_bytree': 0.5303332889921912}. Best is trial 0 with value: 0.5818893236444941.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:16:07,532]\u001b[0m Trial 1 finished with value: 0.5874790385690329 and parameters: {'max_depth': 15, 'learning_rate': 0.4419071278985453, 'colsample_bytree': 0.9518440783911843}. Best is trial 1 with value: 0.5874790385690329.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:16:19,202]\u001b[0m Trial 2 finished with value: 0.548351034097261 and parameters: {'max_depth': 4, 'learning_rate': 0.048468207910939184, 'colsample_bytree': 0.7825867665274766}. Best is trial 1 with value: 0.5874790385690329.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:17:06,377]\u001b[0m Trial 3 finished with value: 0.5975405254332029 and parameters: {'max_depth': 23, 'learning_rate': 0.14467088915501494, 'colsample_bytree': 0.578606289080609}. Best is trial 3 with value: 0.5975405254332029.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m xgb\u001b[39m.\u001b[39mscore(val_samples, val_labels)\n\u001b[0;32m     16\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m _optimize(\n\u001b[0;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    410\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[41], line 13\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobjective\u001b[39m(trial):\n\u001b[0;32m      4\u001b[0m     xgb \u001b[39m=\u001b[39m XGBClassifier(\n\u001b[0;32m      5\u001b[0m         n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m      6\u001b[0m         max_depth\u001b[39m=\u001b[39mtrial\u001b[39m.\u001b[39msuggest_int(\u001b[39m\"\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m30\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m         n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     12\u001b[0m     )\n\u001b[1;32m---> 13\u001b[0m     xgb\u001b[39m.\u001b[39;49mfit(train_samples, train_labels)\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m xgb\u001b[39m.\u001b[39mscore(val_samples, val_labels)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\xgboost\\sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1462\u001b[0m (\n\u001b[0;32m   1463\u001b[0m     model,\n\u001b[0;32m   1464\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1470\u001b[0m )\n\u001b[0;32m   1471\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1472\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[0;32m   1473\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1487\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[0;32m   1488\u001b[0m )\n\u001b[1;32m-> 1490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[0;32m   1491\u001b[0m     params,\n\u001b[0;32m   1492\u001b[0m     train_dmatrix,\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[0;32m   1494\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[0;32m   1495\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[0;32m   1496\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[0;32m   1497\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[0;32m   1498\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m   1499\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m   1500\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1501\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1502\u001b[0m )\n\u001b[0;32m   1504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[0;32m   1505\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[0;32m    186\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1919\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[0;32m   1920\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[0;32m   1921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use optuna to find the best parameters for the XGBClassifier\n",
    "\n",
    "def objective(trial):\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.5),\n",
    "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        # reg_lambda=trial.suggest_float(\"reg_lambda\", 0.01, 1.0),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(train_samples, train_labels)\n",
    "    return xgb.score(val_samples, val_labels)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5916201117318436"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best parameters\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=study.best_params[\"max_depth\"],\n",
    "    learning_rate=study.best_params[\"learning_rate\"],\n",
    "    colsample_bytree=study.best_params[\"colsample_bytree\"],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(train_samples, train_labels)\n",
    "\n",
    "xgb.score(test_samples, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_01\",\n",
    "\n",
    "    \"learning_rate\": 0.01,\n",
    "\n",
    "    \"weight_decay\": 0,\n",
    "\n",
    "    \"batch_size\": 2,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.5,\n",
    "\n",
    "    \"separate\": True,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 0,  ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sample = next(iter(train_loader))\n",
    "# first_sample['X_ecb_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(first_sample['X_ecb_mask'].sum(dim=-1) > 2).any(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    X_att = batch[\"X_ecb_mask\"]\n",
    "    condition = (first_sample['X_ecb_mask'].sum(dim=-1) > 2).any(dim=-1)\n",
    "    if not condition.all():\n",
    "        print(idx, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model3 = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, evaluate, train_with_accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2684/2684 [1:03:07<00:00,  1.41s/batch, accuracy=54, loss=1.11]  \n",
      "Evaluation:   0%|          | 0/895 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6068\\3208734144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train(model=model3, train_loader=train_loader, val_loader=val_loader, config=config, device=device, \n\u001b[0m\u001b[0;32m      2\u001b[0m             max_epochs=1, eval_every=1, name=\"model_03_test\")\n",
      "\u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, config, device, max_epochs, eval_every, name)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m# Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             eval_loss, eval_accu, eval_f1 = evaluate(\n\u001b[0m\u001b[0;32m    137\u001b[0m                 model, val_loader, config, device)\n\u001b[0;32m    138\u001b[0m             \u001b[0meval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, val_loader, config, device)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;31m# Computing predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3093\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3095\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "train(model=model3, train_loader=train_loader, val_loader=val_loader, config=config, device=device, \n",
    "            max_epochs=2, eval_every=1, name=\"model_03_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4c350da27618d5732fc58ebcab8d2c0381c51b7361f332741f21e30512bbdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
