{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# from preprocessing.preprocessing import ecb_pipeline_en, fast_detect\n",
    "from preprocessing.outlier_detection import remove_outlier\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"data/train_series.csv\"\n",
    "FILENAME_ECB = \"data/ecb_data_preprocessed.csv\"\n",
    "FILENAME_FED = \"data/fed_data_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv(FILENAME, index_col=0)\n",
    "ecb = pd.read_csv(FILENAME_ECB, index_col=0)\n",
    "fed = pd.read_csv(FILENAME_FED, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = remove_outlier(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.get_dummies(returns, columns=[\"Index Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[\"Sign\"] = (returns[\"Index + 1\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>-0.027519</td>\n",
       "      <td>-0.103565</td>\n",
       "      <td>-0.045086</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.054050</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.008436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-0.011304</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004980</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.001083</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.001623</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.006444</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "0   0.001045   0.005841   0.003832  -0.027519  -0.103565  -0.045086   \n",
       "1  -0.021497   0.007891  -0.013175  -0.008436   0.000000   0.026303   \n",
       "2  -0.001872  -0.008154   0.023588   0.004086   0.003493   0.003300   \n",
       "3   0.004980  -0.000864   0.001677   0.000000   0.006030  -0.001083   \n",
       "4   0.000360  -0.001893   0.005579  -0.003056  -0.001171  -0.001623   \n",
       "\n",
       "   Index - 3  Index - 2  Index - 1  Index - 0  ... Index Name_CVIX Index  \\\n",
       "0  -0.011265   0.005164   0.054050   0.015779  ...                     0   \n",
       "1   0.000556   0.001455   0.007422   0.000000  ...                     0   \n",
       "2   0.000885  -0.011304   0.005040   0.000156  ...                     0   \n",
       "3   0.000419   0.001492   0.001018  -0.002582  ...                     0   \n",
       "4  -0.002350  -0.006444  -0.000729  -0.000365  ...                     0   \n",
       "\n",
       "  Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "0                        0                            0   \n",
       "1                        0                            0   \n",
       "2                        0                            0   \n",
       "3                        0                            0   \n",
       "4                        1                            0   \n",
       "\n",
       "   Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "0                      0                     0                       0   \n",
       "1                      1                     0                       0   \n",
       "2                      0                     1                       0   \n",
       "3                      0                     1                       0   \n",
       "4                      0                     0                       0   \n",
       "\n",
       "   Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  Sign  \n",
       "0                      0                     1                     0     1  \n",
       "1                      0                     0                     0     1  \n",
       "2                      0                     0                     0     1  \n",
       "3                      0                     0                     0     1  \n",
       "4                      0                     0                     0     1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontextual_cols = ['Index - 9',\n",
    " 'Index - 8',\n",
    " 'Index - 7',\n",
    " 'Index - 6',\n",
    " 'Index - 5',\n",
    " 'Index - 4',\n",
    " 'Index - 3',\n",
    " 'Index - 2',\n",
    " 'Index - 1',\n",
    " 'Index - 0',\n",
    " 'Index Name_CVIX Index',\n",
    " 'Index Name_EURUSD Curncy',\n",
    " 'Index Name_EURUSDV1M Curncy',\n",
    " 'Index Name_MOVE Index',\n",
    " 'Index Name_SPX Index',\n",
    " 'Index Name_SRVIX Index',\n",
    " 'Index Name_SX5E Index',\n",
    " 'Index Name_V2X Index',\n",
    " 'Index Name_VIX Index']\n",
    "nb_nontextfeatures = len(nontextual_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4519\n",
       "1    4007\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = returns[\"Sign\"]\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns.drop([\"Sign\", \"Index + 1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% val, 20% test\n",
    "\n",
    "returns_, returns_test, y_, y_test = train_test_split(\n",
    "    returns, y, test_size=0.2, train_size=0.8,\n",
    "    random_state=0, stratify=y\n",
    "    )\n",
    "\n",
    "returns_train, returns_val, y_train, y_val = train_test_split(\n",
    "    returns_, y_, test_size=0.25, train_size=0.75,\n",
    "    random_state=42, stratify=y_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reattach labels to train\n",
    "returns_train = pd.concat([returns_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_train[\"index ecb\"] = returns_train[\"index ecb\"].str.split(\",\")\n",
    "returns_train[\"index fed\"] = returns_train[\"index fed\"].str.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat samples containing multiple ECB and FED announcements, so that each sample contains only one ECB and one FED announcement\n",
    "returns_train = returns_train.explode(\"index ecb\")\n",
    "returns_train = returns_train.explode(\"index fed\")\n",
    "\n",
    "returns_train = returns_train.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4645</th>\n",
       "      <td>-0.004264</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>-0.042925</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>-0.008969</td>\n",
       "      <td>0.039740</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>-0.008518</td>\n",
       "      <td>0.003416</td>\n",
       "      <td>-0.040893</td>\n",
       "      <td>-0.008025</td>\n",
       "      <td>0.069172</td>\n",
       "      <td>0.026382</td>\n",
       "      <td>-0.036458</td>\n",
       "      <td>0.010076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>0.005284</td>\n",
       "      <td>-0.000998</td>\n",
       "      <td>0.010213</td>\n",
       "      <td>-0.004101</td>\n",
       "      <td>-0.011616</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>-0.003657</td>\n",
       "      <td>-0.024653</td>\n",
       "      <td>-0.006352</td>\n",
       "      <td>0.012444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>-0.039405</td>\n",
       "      <td>-0.016078</td>\n",
       "      <td>0.018308</td>\n",
       "      <td>0.061411</td>\n",
       "      <td>-0.008718</td>\n",
       "      <td>-0.004843</td>\n",
       "      <td>-0.030184</td>\n",
       "      <td>0.039245</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6586</th>\n",
       "      <td>-0.002471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.030129</td>\n",
       "      <td>0.008506</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>-0.026157</td>\n",
       "      <td>0.004215</td>\n",
       "      <td>0.028218</td>\n",
       "      <td>-0.016704</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>0.044951</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>-0.056247</td>\n",
       "      <td>0.059887</td>\n",
       "      <td>-0.064720</td>\n",
       "      <td>-0.079639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>0.027124</td>\n",
       "      <td>0.038878</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>-0.000974</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>-0.009217</td>\n",
       "      <td>-0.003837</td>\n",
       "      <td>0.006340</td>\n",
       "      <td>-0.011104</td>\n",
       "      <td>-0.002672</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>-0.068951</td>\n",
       "      <td>-0.049201</td>\n",
       "      <td>-0.025539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>-0.009557</td>\n",
       "      <td>-0.017437</td>\n",
       "      <td>-0.048725</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.080720</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>-0.068951</td>\n",
       "      <td>-0.049201</td>\n",
       "      <td>-0.025539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>-0.009557</td>\n",
       "      <td>-0.017437</td>\n",
       "      <td>-0.048725</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.080720</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8301</th>\n",
       "      <td>-0.004144</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>-0.007618</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>-0.009423</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004799</td>\n",
       "      <td>-0.001219</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7210 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "4645  -0.004264   0.008511   0.016807   0.008299   0.000000  -0.016667   \n",
       "3230  -0.008518   0.003416  -0.040893  -0.008025   0.069172   0.026382   \n",
       "1301   0.005284  -0.000998   0.010213  -0.004101  -0.011616   0.001504   \n",
       "1088  -0.039405  -0.016078   0.018308   0.061411  -0.008718  -0.004843   \n",
       "6586  -0.002471   0.000000   0.000000  -0.030129   0.008506   0.004636   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2623   0.044951   0.004568  -0.056247   0.059887  -0.064720  -0.079639   \n",
       "4855   0.002891   0.012038  -0.000974   0.000905   0.008521  -0.009217   \n",
       "1076  -0.068951  -0.049201  -0.025539   0.000000   0.070942  -0.009557   \n",
       "1076  -0.068951  -0.049201  -0.025539   0.000000   0.070942  -0.009557   \n",
       "8301  -0.004144   0.011801   0.011421  -0.007618   0.005473  -0.009423   \n",
       "\n",
       "      Index - 3  Index - 2  Index - 1  Index - 0  ... Index Name_CVIX Index  \\\n",
       "4645  -0.042925  -0.017700  -0.008969   0.039740  ...                     0   \n",
       "3230  -0.036458   0.010076   0.000000   0.000000  ...                     0   \n",
       "1301  -0.003657  -0.024653  -0.006352   0.012444  ...                     0   \n",
       "1088  -0.030184   0.039245   0.028455   0.016807  ...                     0   \n",
       "6586  -0.026157   0.004215   0.028218  -0.016704  ...                     0   \n",
       "...         ...        ...        ...        ...  ...                   ...   \n",
       "2623   0.000000   0.029976   0.027124   0.038878  ...                     0   \n",
       "4855  -0.003837   0.006340  -0.011104  -0.002672  ...                     0   \n",
       "1076  -0.017437  -0.048725   0.008853   0.080720  ...                     0   \n",
       "1076  -0.017437  -0.048725   0.008853   0.080720  ...                     0   \n",
       "8301  -0.002443   0.000000  -0.004799  -0.001219  ...                     0   \n",
       "\n",
       "     Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "4645                        0                            1   \n",
       "3230                        0                            0   \n",
       "1301                        1                            0   \n",
       "1088                        0                            1   \n",
       "6586                        0                            0   \n",
       "...                       ...                          ...   \n",
       "2623                        0                            0   \n",
       "4855                        1                            0   \n",
       "1076                        0                            0   \n",
       "1076                        0                            0   \n",
       "8301                        0                            0   \n",
       "\n",
       "      Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "4645                      0                     0                       0   \n",
       "3230                      1                     0                       0   \n",
       "1301                      0                     0                       0   \n",
       "1088                      0                     0                       0   \n",
       "6586                      0                     0                       0   \n",
       "...                     ...                   ...                     ...   \n",
       "2623                      1                     0                       0   \n",
       "4855                      0                     0                       0   \n",
       "1076                      0                     0                       0   \n",
       "1076                      0                     0                       0   \n",
       "8301                      0                     1                       0   \n",
       "\n",
       "      Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  Sign  \n",
       "4645                      0                     0                     0     0  \n",
       "3230                      0                     0                     0     0  \n",
       "1301                      0                     0                     0     1  \n",
       "1088                      0                     0                     0     0  \n",
       "6586                      1                     0                     0     0  \n",
       "...                     ...                   ...                   ...   ...  \n",
       "2623                      0                     0                     0     1  \n",
       "4855                      0                     0                     0     1  \n",
       "1076                      0                     0                     1     1  \n",
       "1076                      0                     0                     1     1  \n",
       "8301                      0                     0                     0     0  \n",
       "\n",
       "[7210 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3810\n",
       "1    3400\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = returns_train[\"Sign\"]\n",
    "returns_train = returns_train.drop([\"Sign\"], axis=1)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    904\n",
       "1    801\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1273\n",
       "1    1113\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make another version of the validation set with only one ECB and one FED announcement per sample\n",
    "\n",
    "returns_val_t = pd.concat([returns_val, y_val], axis=1)\n",
    "\n",
    "returns_val_t[\"index ecb\"] = returns_val_t[\"index ecb\"].str.split(\",\")\n",
    "returns_val_t[\"index fed\"] = returns_val_t[\"index fed\"].str.split(\",\")\n",
    "returns_val_t = returns_val_t.explode(\"index ecb\")\n",
    "returns_val_t = returns_val_t.explode(\"index fed\")\n",
    "returns_val_t = returns_val_t.dropna()\n",
    "\n",
    "y_val_t = returns_val_t[\"Sign\"]\n",
    "returns_val_t = returns_val_t.drop([\"Sign\"], axis=1)\n",
    "y_val_t.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    904\n",
       "1    802\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textual data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_dataset import get_data_loader\n",
    "from model.framework_model import CorpusEncoder, ClassificationHead, MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_03\",\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.,\n",
    "\n",
    "    \"batch_size\": 32,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.3,\n",
    "\n",
    "    \"separate\": True,\n",
    "    \n",
    "    \"max_corpus_len\": 1\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set_t, val_loader_t, tokenizer, steps = get_data_loader(\n",
    "    returns_val_t, ecb, fed, y_val_t, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import distilbert-base-uncased\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_model import MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.to(device)\n",
    "bert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 226/226 [09:51<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use distilbert to compute the embeddings of each text\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        cls_ecb = bert(batch[\"X_ecb\"].squeeze(1).to(device), batch[\"X_ecb_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "        cls_fed = bert(batch[\"X_fed\"].squeeze(1).to(device), batch[\"X_fed_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate X_ind and cls_ecb and cls_fed\n",
    "        X_ind = batch[\"X_ind\"].to(device)\n",
    "        X = torch.cat((X_ind, cls_ecb, cls_fed), dim=1)\n",
    "\n",
    "        # Add a copy of the result to the train samples\n",
    "        train_samples.append(X.detach().cpu().numpy())\n",
    "        train_labels.append(batch[\"label\"].detach().cpu().numpy())\n",
    "\n",
    "train_samples = np.concatenate(train_samples)\n",
    "train_labels = np.concatenate(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [02:22<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# On validation set\n",
    "val_samples = []\n",
    "val_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        cls_ecb = bert(batch[\"X_ecb\"].squeeze(1).to(device), batch[\"X_ecb_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "        cls_fed = bert(batch[\"X_fed\"].squeeze(1).to(device), batch[\"X_fed_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate X_ind and cls_ecb and cls_fed\n",
    "        X_ind = batch[\"X_ind\"].to(device)\n",
    "        X = torch.cat((X_ind, cls_ecb, cls_fed), dim=1)\n",
    "\n",
    "        # Add a copy of the result to the train samples\n",
    "        val_samples.append(X.detach().cpu().numpy())\n",
    "        val_labels.append(batch[\"label\"].detach().cpu().numpy())\n",
    "\n",
    "val_samples = np.concatenate(val_samples)\n",
    "val_labels = np.concatenate(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [03:23<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use distilbert to compute the embeddings of each text for the validation set with only one ECB and one FED announcement per sample\n",
    "val_samples_t = []\n",
    "val_labels_t = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader_t):\n",
    "        cls_ecb = bert(batch[\"X_ecb\"].squeeze(1).to(device), batch[\"X_ecb_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "        cls_fed = bert(batch[\"X_fed\"].squeeze(1).to(device), batch[\"X_fed_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate X_ind and cls_ecb and cls_fed\n",
    "        X_ind = batch[\"X_ind\"].to(device)\n",
    "        X = torch.cat((X_ind, cls_ecb, cls_fed), dim=1)\n",
    "\n",
    "        # Add a copy of the result to the train samples\n",
    "        val_samples_t.append(X.detach().cpu().numpy())\n",
    "        val_labels_t.append(batch[\"label\"].detach().cpu().numpy())\n",
    "    \n",
    "val_samples_t = np.concatenate(val_samples_t)\n",
    "val_labels_t = np.concatenate(val_labels_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [02:28<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Use distilbert to compute the embeddings of each text for the test set\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        cls_ecb = bert(batch[\"X_ecb\"].squeeze(1).to(device), batch[\"X_ecb_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "        cls_fed = bert(batch[\"X_fed\"].squeeze(1).to(device), batch[\"X_fed_mask\"].squeeze(1).to(device)).last_hidden_state[:, 0, :]\n",
    "\n",
    "        # concatenate X_ind and cls_ecb and cls_fed\n",
    "        X_ind = batch[\"X_ind\"].to(device)\n",
    "        X = torch.cat((X_ind, cls_ecb, cls_fed), dim=1)\n",
    "\n",
    "        # Add a copy of the result to the train samples\n",
    "        test_samples.append(X.detach().cpu().numpy())\n",
    "        test_labels.append(batch[\"label\"].detach().cpu().numpy())\n",
    "\n",
    "test_samples = np.concatenate(test_samples)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5662368112543963"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test LGBMClassifier on the data.\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(train_samples, train_labels)\n",
    "\n",
    "lgbm.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-05 09:42:16,768]\u001b[0m A new study created in memory with name: no-name-90e7da6c-8c4e-4d14-88f0-3e51ef468e9f\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:42:25,941]\u001b[0m Trial 0 finished with value: 0.5302052785923753 and parameters: {'max_depth': 10, 'learning_rate': 0.001023277511457471, 'colsample_bytree': 0.637353750597351}. Best is trial 0 with value: 0.5302052785923753.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:42:36,062]\u001b[0m Trial 1 finished with value: 0.5513196480938416 and parameters: {'max_depth': 13, 'learning_rate': 0.016408079007132387, 'colsample_bytree': 0.8209747283017248}. Best is trial 1 with value: 0.5513196480938416.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:42:45,820]\u001b[0m Trial 2 finished with value: 0.555425219941349 and parameters: {'max_depth': 9, 'learning_rate': 0.09996910268840384, 'colsample_bytree': 0.8615577999008968}. Best is trial 2 with value: 0.555425219941349.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:42:55,465]\u001b[0m Trial 3 finished with value: 0.5595307917888563 and parameters: {'max_depth': 13, 'learning_rate': 0.0630535075347635, 'colsample_bytree': 0.7702359830616647}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:06,766]\u001b[0m Trial 4 finished with value: 0.5530791788856305 and parameters: {'max_depth': 16, 'learning_rate': 0.013961893933896482, 'colsample_bytree': 0.9131487518311685}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:12,776]\u001b[0m Trial 5 finished with value: 0.543108504398827 and parameters: {'max_depth': 5, 'learning_rate': 0.06742247576269862, 'colsample_bytree': 0.9589568527990455}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:21,852]\u001b[0m Trial 6 finished with value: 0.5272727272727272 and parameters: {'max_depth': 20, 'learning_rate': 0.0026176920015263355, 'colsample_bytree': 0.6427846658261286}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:32,351]\u001b[0m Trial 7 finished with value: 0.5278592375366569 and parameters: {'max_depth': 19, 'learning_rate': 0.0028462294787562274, 'colsample_bytree': 0.7942122812337367}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:40,853]\u001b[0m Trial 8 finished with value: 0.5519061583577712 and parameters: {'max_depth': 14, 'learning_rate': 0.016685093477355965, 'colsample_bytree': 0.6285509967130201}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:50,473]\u001b[0m Trial 9 finished with value: 0.5302052785923753 and parameters: {'max_depth': 20, 'learning_rate': 0.004531768332783942, 'colsample_bytree': 0.6243898311932117}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:43:56,910]\u001b[0m Trial 10 finished with value: 0.5348973607038123 and parameters: {'max_depth': 9, 'learning_rate': 0.04134426667962119, 'colsample_bytree': 0.5126327641519337}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:06,738]\u001b[0m Trial 11 finished with value: 0.5489736070381231 and parameters: {'max_depth': 9, 'learning_rate': 0.07857240696115296, 'colsample_bytree': 0.8578507814011191}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:11,920]\u001b[0m Trial 12 finished with value: 0.5542521994134897 and parameters: {'max_depth': 5, 'learning_rate': 0.03593207469637426, 'colsample_bytree': 0.7331365649255068}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:20,755]\u001b[0m Trial 13 finished with value: 0.5472140762463343 and parameters: {'max_depth': 11, 'learning_rate': 0.09683502384439244, 'colsample_bytree': 0.7335439115737579}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:29,175]\u001b[0m Trial 14 finished with value: 0.5466275659824047 and parameters: {'max_depth': 7, 'learning_rate': 0.03386346653511086, 'colsample_bytree': 0.8919918986353758}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:40,640]\u001b[0m Trial 15 finished with value: 0.5419354838709678 and parameters: {'max_depth': 15, 'learning_rate': 0.04114248226447609, 'colsample_bytree': 0.9900449208263666}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:50,807]\u001b[0m Trial 16 finished with value: 0.5395894428152492 and parameters: {'max_depth': 17, 'learning_rate': 0.00751303114085499, 'colsample_bytree': 0.7793440052206259}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:44:59,933]\u001b[0m Trial 17 finished with value: 0.5507331378299121 and parameters: {'max_depth': 12, 'learning_rate': 0.02536532019429633, 'colsample_bytree': 0.71948009532876}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:08,848]\u001b[0m Trial 18 finished with value: 0.535483870967742 and parameters: {'max_depth': 8, 'learning_rate': 0.0586232710309152, 'colsample_bytree': 0.8499539488226538}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:20,222]\u001b[0m Trial 19 finished with value: 0.5348973607038123 and parameters: {'max_depth': 12, 'learning_rate': 0.008091753281364944, 'colsample_bytree': 0.9246714032259463}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:26,885]\u001b[0m Trial 20 finished with value: 0.5577712609970674 and parameters: {'max_depth': 7, 'learning_rate': 0.09700170225709961, 'colsample_bytree': 0.6778151187216029}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:34,255]\u001b[0m Trial 21 finished with value: 0.5571847507331378 and parameters: {'max_depth': 7, 'learning_rate': 0.09998540266863785, 'colsample_bytree': 0.6891668589087513}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:41,649]\u001b[0m Trial 22 finished with value: 0.5536656891495602 and parameters: {'max_depth': 7, 'learning_rate': 0.05366488272686656, 'colsample_bytree': 0.6889384316936391}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:47,387]\u001b[0m Trial 23 finished with value: 0.533724340175953 and parameters: {'max_depth': 6, 'learning_rate': 0.025123974108785765, 'colsample_bytree': 0.5790514633828099}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:45:54,406]\u001b[0m Trial 24 finished with value: 0.5419354838709678 and parameters: {'max_depth': 7, 'learning_rate': 0.06000411259868715, 'colsample_bytree': 0.6874862869726936}. Best is trial 3 with value: 0.5595307917888563.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:01,630]\u001b[0m Trial 25 finished with value: 0.5607038123167155 and parameters: {'max_depth': 11, 'learning_rate': 0.09525071744941882, 'colsample_bytree': 0.5622215403878025}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:08,794]\u001b[0m Trial 26 finished with value: 0.546041055718475 and parameters: {'max_depth': 11, 'learning_rate': 0.05225717993160472, 'colsample_bytree': 0.506873349929452}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:16,452]\u001b[0m Trial 27 finished with value: 0.5413489736070382 and parameters: {'max_depth': 14, 'learning_rate': 0.02432200358081281, 'colsample_bytree': 0.5476445324011641}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:24,023]\u001b[0m Trial 28 finished with value: 0.5536656891495602 and parameters: {'max_depth': 17, 'learning_rate': 0.07654035268978754, 'colsample_bytree': 0.5820156481935392}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:34,380]\u001b[0m Trial 29 finished with value: 0.5302052785923753 and parameters: {'max_depth': 10, 'learning_rate': 0.0011246275911650765, 'colsample_bytree': 0.7759660348119538}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:42,120]\u001b[0m Trial 30 finished with value: 0.547800586510264 and parameters: {'max_depth': 11, 'learning_rate': 0.046844075243737016, 'colsample_bytree': 0.6032741966237659}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:49,682]\u001b[0m Trial 31 finished with value: 0.5548387096774193 and parameters: {'max_depth': 8, 'learning_rate': 0.09505496585286473, 'colsample_bytree': 0.67476733822891}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:46:58,091]\u001b[0m Trial 32 finished with value: 0.5419354838709678 and parameters: {'max_depth': 13, 'learning_rate': 0.07068278224972671, 'colsample_bytree': 0.6563313318422814}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:47:06,901]\u001b[0m Trial 33 finished with value: 0.5472140762463343 and parameters: {'max_depth': 10, 'learning_rate': 0.09568757876102378, 'colsample_bytree': 0.7147941269354628}. Best is trial 25 with value: 0.5607038123167155.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:47:14,594]\u001b[0m Trial 34 finished with value: 0.5700879765395894 and parameters: {'max_depth': 6, 'learning_rate': 0.07418345625454321, 'colsample_bytree': 0.80994395910725}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:47:19,741]\u001b[0m Trial 35 finished with value: 0.5372434017595308 and parameters: {'max_depth': 5, 'learning_rate': 0.028120501825976068, 'colsample_bytree': 0.8181812167689642}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:47:26,181]\u001b[0m Trial 36 finished with value: 0.5448680351906159 and parameters: {'max_depth': 6, 'learning_rate': 0.07184097245163017, 'colsample_bytree': 0.7551636981180785}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:47:37,625]\u001b[0m Trial 37 finished with value: 0.543108504398827 and parameters: {'max_depth': 14, 'learning_rate': 0.014514142870963958, 'colsample_bytree': 0.8168172668923379}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:47:43,806]\u001b[0m Trial 38 finished with value: 0.5390029325513197 and parameters: {'max_depth': 8, 'learning_rate': 0.03256580220665596, 'colsample_bytree': 0.5425214829741242}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:47:54,443]\u001b[0m Trial 39 finished with value: 0.5413489736070382 and parameters: {'max_depth': 15, 'learning_rate': 0.020770147935292884, 'colsample_bytree': 0.8366708177693281}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:48:02,582]\u001b[0m Trial 40 finished with value: 0.5507331378299121 and parameters: {'max_depth': 6, 'learning_rate': 0.010524672799164718, 'colsample_bytree': 0.8063735924011646}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:48:10,291]\u001b[0m Trial 41 finished with value: 0.5595307917888563 and parameters: {'max_depth': 7, 'learning_rate': 0.08067768529320421, 'colsample_bytree': 0.7498795631709135}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:48:21,399]\u001b[0m Trial 42 finished with value: 0.5448680351906159 and parameters: {'max_depth': 9, 'learning_rate': 0.07652964975914436, 'colsample_bytree': 0.7691492150501629}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:48:28,276]\u001b[0m Trial 43 finished with value: 0.533724340175953 and parameters: {'max_depth': 5, 'learning_rate': 0.045131532154971725, 'colsample_bytree': 0.8907076201087316}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:48:36,056]\u001b[0m Trial 44 finished with value: 0.567741935483871 and parameters: {'max_depth': 6, 'learning_rate': 0.06214195642995131, 'colsample_bytree': 0.7453574212856569}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:48:44,317]\u001b[0m Trial 45 finished with value: 0.5565982404692082 and parameters: {'max_depth': 6, 'learning_rate': 0.06560594118281202, 'colsample_bytree': 0.7954896252908307}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:48:54,117]\u001b[0m Trial 46 finished with value: 0.5483870967741935 and parameters: {'max_depth': 8, 'learning_rate': 0.05291708305856287, 'colsample_bytree': 0.7538615454460502}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:49:05,503]\u001b[0m Trial 47 finished with value: 0.555425219941349 and parameters: {'max_depth': 13, 'learning_rate': 0.03886363191911239, 'colsample_bytree': 0.884292888943676}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:49:14,583]\u001b[0m Trial 48 finished with value: 0.5548387096774193 and parameters: {'max_depth': 9, 'learning_rate': 0.07961978131757769, 'colsample_bytree': 0.7279330119923676}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:49:21,993]\u001b[0m Trial 49 finished with value: 0.5325513196480939 and parameters: {'max_depth': 5, 'learning_rate': 0.0037884984362728563, 'colsample_bytree': 0.8420921096522339}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:49:31,575]\u001b[0m Trial 50 finished with value: 0.5636363636363636 and parameters: {'max_depth': 12, 'learning_rate': 0.059051804163689485, 'colsample_bytree': 0.788523007876738}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:49:41,250]\u001b[0m Trial 51 finished with value: 0.5548387096774193 and parameters: {'max_depth': 12, 'learning_rate': 0.058429269159064684, 'colsample_bytree': 0.7878924952223119}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:49:50,109]\u001b[0m Trial 52 finished with value: 0.546041055718475 and parameters: {'max_depth': 13, 'learning_rate': 0.045994105143214226, 'colsample_bytree': 0.7096344786269247}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:49:58,693]\u001b[0m Trial 53 finished with value: 0.5530791788856305 and parameters: {'max_depth': 15, 'learning_rate': 0.08412606295744483, 'colsample_bytree': 0.7430344763830203}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:50:07,635]\u001b[0m Trial 54 finished with value: 0.5618768328445748 and parameters: {'max_depth': 11, 'learning_rate': 0.06236623881883258, 'colsample_bytree': 0.7613357039413836}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:50:16,656]\u001b[0m Trial 55 finished with value: 0.5519061583577712 and parameters: {'max_depth': 11, 'learning_rate': 0.06474289240096612, 'colsample_bytree': 0.7612736925728371}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:50:27,314]\u001b[0m Trial 56 finished with value: 0.5436950146627566 and parameters: {'max_depth': 12, 'learning_rate': 0.03675078921502013, 'colsample_bytree': 0.8751998185108145}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:50:38,617]\u001b[0m Trial 57 finished with value: 0.5302052785923753 and parameters: {'max_depth': 10, 'learning_rate': 0.001671133398947388, 'colsample_bytree': 0.9235704210270135}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:50:48,381]\u001b[0m Trial 58 finished with value: 0.5372434017595308 and parameters: {'max_depth': 14, 'learning_rate': 0.04956855315004897, 'colsample_bytree': 0.8339626044119003}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:50:58,136]\u001b[0m Trial 59 finished with value: 0.5348973607038123 and parameters: {'max_depth': 16, 'learning_rate': 0.032004596796070026, 'colsample_bytree': 0.8002736038501395}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:51:05,727]\u001b[0m Trial 60 finished with value: 0.5519061583577712 and parameters: {'max_depth': 12, 'learning_rate': 0.06021707449864504, 'colsample_bytree': 0.6441557220035192}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:51:14,073]\u001b[0m Trial 61 finished with value: 0.555425219941349 and parameters: {'max_depth': 10, 'learning_rate': 0.08392466019295386, 'colsample_bytree': 0.7782843413362565}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:51:22,607]\u001b[0m Trial 62 finished with value: 0.5618768328445748 and parameters: {'max_depth': 11, 'learning_rate': 0.06696722279644099, 'colsample_bytree': 0.7040149183803239}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:51:32,533]\u001b[0m Trial 63 finished with value: 0.5378299120234604 and parameters: {'max_depth': 11, 'learning_rate': 0.04415970815872899, 'colsample_bytree': 0.7323713006783317}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:51:42,017]\u001b[0m Trial 64 finished with value: 0.5577712609970674 and parameters: {'max_depth': 13, 'learning_rate': 0.06355360842416641, 'colsample_bytree': 0.8114282211217746}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:51:49,333]\u001b[0m Trial 65 finished with value: 0.5524926686217009 and parameters: {'max_depth': 9, 'learning_rate': 0.08902250856608349, 'colsample_bytree': 0.7109845892127993}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:51:58,271]\u001b[0m Trial 66 finished with value: 0.5501466275659824 and parameters: {'max_depth': 11, 'learning_rate': 0.05587824021634325, 'colsample_bytree': 0.701591291244936}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:52:05,141]\u001b[0m Trial 67 finished with value: 0.5583577712609971 and parameters: {'max_depth': 6, 'learning_rate': 0.07157112733527374, 'colsample_bytree': 0.7470627773407471}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:52:12,999]\u001b[0m Trial 68 finished with value: 0.5436950146627566 and parameters: {'max_depth': 10, 'learning_rate': 0.082679361685723, 'colsample_bytree': 0.6138662347846571}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:52:22,632]\u001b[0m Trial 69 finished with value: 0.5366568914956011 and parameters: {'max_depth': 12, 'learning_rate': 0.005667063200621356, 'colsample_bytree': 0.789382135400518}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:52:32,398]\u001b[0m Trial 70 finished with value: 0.5589442815249267 and parameters: {'max_depth': 19, 'learning_rate': 0.04083814503208124, 'colsample_bytree': 0.8622162682525313}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:52:39,646]\u001b[0m Trial 71 finished with value: 0.5648093841642229 and parameters: {'max_depth': 7, 'learning_rate': 0.06967425301599633, 'colsample_bytree': 0.7675884314094125}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:52:48,517]\u001b[0m Trial 72 finished with value: 0.5571847507331378 and parameters: {'max_depth': 14, 'learning_rate': 0.06987086583136441, 'colsample_bytree': 0.7657311047324625}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:52:56,297]\u001b[0m Trial 73 finished with value: 0.5454545454545454 and parameters: {'max_depth': 12, 'learning_rate': 0.05374620163356838, 'colsample_bytree': 0.6695339807250204}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:03,770]\u001b[0m Trial 74 finished with value: 0.5542521994134897 and parameters: {'max_depth': 7, 'learning_rate': 0.0949764123489639, 'colsample_bytree': 0.8260112229246823}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:12,310]\u001b[0m Trial 75 finished with value: 0.5413489736070382 and parameters: {'max_depth': 11, 'learning_rate': 0.06638774258941, 'colsample_bytree': 0.7810186508298693}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:19,756]\u001b[0m Trial 76 finished with value: 0.5583577712609971 and parameters: {'max_depth': 8, 'learning_rate': 0.0811584874239853, 'colsample_bytree': 0.7426905490225943}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:26,842]\u001b[0m Trial 77 finished with value: 0.5442815249266862 and parameters: {'max_depth': 13, 'learning_rate': 0.019237793260433223, 'colsample_bytree': 0.5538593140919739}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:30,735]\u001b[0m Trial 78 finished with value: 0.530791788856305 and parameters: {'max_depth': 5, 'learning_rate': 0.028289612741209127, 'colsample_bytree': 0.5232921843662358}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:37,450]\u001b[0m Trial 79 finished with value: 0.5583577712609971 and parameters: {'max_depth': 7, 'learning_rate': 0.049519898973914775, 'colsample_bytree': 0.6937753434109738}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:44,606]\u001b[0m Trial 80 finished with value: 0.5601173020527859 and parameters: {'max_depth': 6, 'learning_rate': 0.06041584139330334, 'colsample_bytree': 0.9979300489970506}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:51,918]\u001b[0m Trial 81 finished with value: 0.5425219941348973 and parameters: {'max_depth': 6, 'learning_rate': 0.099924953637755, 'colsample_bytree': 0.993958075121245}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:53:55,785]\u001b[0m Trial 82 finished with value: 0.5378299120234604 and parameters: {'max_depth': 5, 'learning_rate': 0.059261036219174196, 'colsample_bytree': 0.5809956272787115}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:01,601]\u001b[0m Trial 83 finished with value: 0.5612903225806452 and parameters: {'max_depth': 6, 'learning_rate': 0.07400744112847545, 'colsample_bytree': 0.7269143176591607}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:07,399]\u001b[0m Trial 84 finished with value: 0.5595307917888563 and parameters: {'max_depth': 6, 'learning_rate': 0.07162575990107194, 'colsample_bytree': 0.7293048898656799}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:14,309]\u001b[0m Trial 85 finished with value: 0.5577712609970674 and parameters: {'max_depth': 6, 'learning_rate': 0.08851648921771424, 'colsample_bytree': 0.9696936922580135}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:23,217]\u001b[0m Trial 86 finished with value: 0.547800586510264 and parameters: {'max_depth': 8, 'learning_rate': 0.07328593698757305, 'colsample_bytree': 0.9440796165139714}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:30,973]\u001b[0m Trial 87 finished with value: 0.5442815249266862 and parameters: {'max_depth': 7, 'learning_rate': 0.04280304724838247, 'colsample_bytree': 0.9014807746312838}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:35,645]\u001b[0m Trial 88 finished with value: 0.533724340175953 and parameters: {'max_depth': 5, 'learning_rate': 0.05195336910520142, 'colsample_bytree': 0.7209254762252846}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:42,862]\u001b[0m Trial 89 finished with value: 0.5495601173020528 and parameters: {'max_depth': 7, 'learning_rate': 0.06280212547058897, 'colsample_bytree': 0.7664091858312339}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:50,257]\u001b[0m Trial 90 finished with value: 0.533724340175953 and parameters: {'max_depth': 6, 'learning_rate': 0.03613991049703616, 'colsample_bytree': 0.9795065934221441}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:54:56,222]\u001b[0m Trial 91 finished with value: 0.5472140762463343 and parameters: {'max_depth': 6, 'learning_rate': 0.07075048423564084, 'colsample_bytree': 0.7329622415824311}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:55:05,120]\u001b[0m Trial 92 finished with value: 0.5583577712609971 and parameters: {'max_depth': 11, 'learning_rate': 0.05544792383333623, 'colsample_bytree': 0.8033992412517135}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:55:12,855]\u001b[0m Trial 93 finished with value: 0.5607038123167155 and parameters: {'max_depth': 9, 'learning_rate': 0.07693220426152443, 'colsample_bytree': 0.7538573773890008}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:55:21,122]\u001b[0m Trial 94 finished with value: 0.5624633431085044 and parameters: {'max_depth': 10, 'learning_rate': 0.09068904409237516, 'colsample_bytree': 0.7619642453633092}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:55:29,049]\u001b[0m Trial 95 finished with value: 0.5495601173020528 and parameters: {'max_depth': 10, 'learning_rate': 0.08802190504989821, 'colsample_bytree': 0.7565127308083048}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:55:37,374]\u001b[0m Trial 96 finished with value: 0.5425219941348973 and parameters: {'max_depth': 9, 'learning_rate': 0.07866428010327076, 'colsample_bytree': 0.7781940661748386}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:55:45,562]\u001b[0m Trial 97 finished with value: 0.5607038123167155 and parameters: {'max_depth': 9, 'learning_rate': 0.0892880384600828, 'colsample_bytree': 0.7932804120690772}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:55:53,756]\u001b[0m Trial 98 finished with value: 0.5548387096774193 and parameters: {'max_depth': 9, 'learning_rate': 0.048296945628859805, 'colsample_bytree': 0.742023008905767}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 09:56:02,076]\u001b[0m Trial 99 finished with value: 0.5612903225806452 and parameters: {'max_depth': 10, 'learning_rate': 0.09142374818141522, 'colsample_bytree': 0.7968669783785365}. Best is trial 34 with value: 0.5700879765395894.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Use optuna to find the best parameters for LGBMClassifier\n",
    "\n",
    "def objective(trial):\n",
    "    lgbm = LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 5, 20),\n",
    "        learning_rate=trial.suggest_loguniform(\"learning_rate\", 1e-3, 1e-1),\n",
    "        colsample_bytree=trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        # reg_lambda=trial.suggest_loguniform(\"reg_lambda\", 1e-3, 1e3),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    lgbm.fit(train_samples, train_labels)\n",
    "\n",
    "    return lgbm.score(val_samples, val_labels)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5609613130128956"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best parameters\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=study.best_params[\"max_depth\"],\n",
    "    learning_rate=study.best_params[\"learning_rate\"],\n",
    "    colsample_bytree=study.best_params[\"colsample_bytree\"],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(train_samples, train_labels)\n",
    "\n",
    "lgbm.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concate the train and validation set\n",
    "train_samples_ = np.concatenate((train_samples, val_samples_t))\n",
    "train_labels_ = np.concatenate((train_labels, val_labels_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5515826494724502"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.fit(train_samples_, train_labels_)\n",
    "lgbm.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5762016412661196"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(train_samples_, train_labels_)\n",
    "lgbm.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47156726768377255"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_labels == 1.0).sum() / len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5773739742086753"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test XGBClassifier on the data.\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(train_samples, train_labels)\n",
    "\n",
    "xgb.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5844079718640094"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(train_samples_, train_labels_)\n",
    "xgb.score(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-05 10:05:53,406]\u001b[0m A new study created in memory with name: no-name-11609199-be8e-4902-852a-d656a4cfa7b0\u001b[0m\n",
      "\u001b[32m[I 2023-04-05 10:06:35,183]\u001b[0m Trial 0 finished with value: 0.5648093841642229 and parameters: {'max_depth': 15, 'learning_rate': 0.4239917349291033, 'colsample_bytree': 0.8504270294941401}. Best is trial 0 with value: 0.5648093841642229.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Use optuna to find the best parameters for the XGBClassifier\n",
    "\n",
    "def objective(trial):\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.5),\n",
    "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        # reg_lambda=trial.suggest_float(\"reg_lambda\", 0.01, 1.0),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(train_samples, train_labels)\n",
    "    return xgb.score(val_samples, val_labels)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5916201117318436"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best parameters\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=study.best_params[\"max_depth\"],\n",
    "    learning_rate=study.best_params[\"learning_rate\"],\n",
    "    colsample_bytree=study.best_params[\"colsample_bytree\"],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb.fit(train_samples, train_labels)\n",
    "\n",
    "xgb.score(test_samples, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_01\",\n",
    "\n",
    "    \"learning_rate\": 0.01,\n",
    "\n",
    "    \"weight_decay\": 0,\n",
    "\n",
    "    \"batch_size\": 2,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.5,\n",
    "\n",
    "    \"separate\": True,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 0,  ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sample = next(iter(train_loader))\n",
    "# first_sample['X_ecb_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(first_sample['X_ecb_mask'].sum(dim=-1) > 2).any(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    X_att = batch[\"X_ecb_mask\"]\n",
    "    condition = (first_sample['X_ecb_mask'].sum(dim=-1) > 2).any(dim=-1)\n",
    "    if not condition.all():\n",
    "        print(idx, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model3 = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, evaluate, train_with_accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2684/2684 [1:03:07<00:00,  1.41s/batch, accuracy=54, loss=1.11]  \n",
      "Evaluation:   0%|          | 0/895 [00:01<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6068\\3208734144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train(model=model3, train_loader=train_loader, val_loader=val_loader, config=config, device=device, \n\u001b[0m\u001b[0;32m      2\u001b[0m             max_epochs=1, eval_every=1, name=\"model_03_test\")\n",
      "\u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, config, device, max_epochs, eval_every, name)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m# Evaluation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             eval_loss, eval_accu, eval_f1 = evaluate(\n\u001b[0m\u001b[0;32m    137\u001b[0m                 model, val_loader, config, device)\n\u001b[0;32m    138\u001b[0m             \u001b[0meval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, val_loader, config, device)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;31m# Computing predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3093\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3095\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "train(model=model3, train_loader=train_loader, val_loader=val_loader, config=config, device=device, \n",
    "            max_epochs=2, eval_every=1, name=\"model_03_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4c350da27618d5732fc58ebcab8d2c0381c51b7361f332741f21e30512bbdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
