{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# from preprocessing.preprocessing import ecb_pipeline_en, fast_detect\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"data/train_series.csv\"\n",
    "FILENAME_ECB = \"data/ecb_data_preprocessed.csv\"\n",
    "FILENAME_FED = \"data/fed_data_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv(FILENAME, index_col=0)\n",
    "ecb = pd.read_csv(FILENAME_ECB, index_col=0)\n",
    "fed = pd.read_csv(FILENAME_FED, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.get_dummies(returns, columns=[\"Index Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[\"Sign\"] = (returns[\"Index + 1\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = returns[\"Sign\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4930\n",
       "1    4016\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns.drop([\"Sign\", \"Index + 1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontextual_cols = ['Index - 9',\n",
    " 'Index - 8',\n",
    " 'Index - 7',\n",
    " 'Index - 6',\n",
    " 'Index - 5',\n",
    " 'Index - 4',\n",
    " 'Index - 3',\n",
    " 'Index - 2',\n",
    " 'Index - 1',\n",
    " 'Index - 0',\n",
    " 'Index Name_CVIX Index',\n",
    " 'Index Name_EURUSD Curncy',\n",
    " 'Index Name_EURUSDV1M Curncy',\n",
    " 'Index Name_MOVE Index',\n",
    " 'Index Name_SPX Index',\n",
    " 'Index Name_SRVIX Index',\n",
    " 'Index Name_SX5E Index',\n",
    " 'Index Name_V2X Index',\n",
    " 'Index Name_VIX Index']\n",
    "nb_nontextfeatures = len(nontextual_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% val, 20% test\n",
    "\n",
    "returns_, returns_test, y_, y_test = train_test_split(\n",
    "    returns, y, test_size=0.2, train_size=0.8,\n",
    "    random_state=0, stratify=y\n",
    "    )\n",
    "\n",
    "returns_train, returns_val, y_train, y_val = train_test_split(\n",
    "    returns_, y_, test_size=0.25, train_size=0.75,\n",
    "    random_state=42, stratify=y_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del returns, y\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textual data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_dataset import get_data_loader\n",
    "from model.framework_model import MyModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": None,\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.001,\n",
    "\n",
    "    \"batch_size\": 64,\n",
    "\n",
    "    \"layers\": 5,\n",
    "\n",
    "    \"dropout\": 0.5,\n",
    "\n",
    "    \"separate\": False,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(config[\"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (nontext_network): NontextualNetwork()\n",
       "  (corpus_encoder): CorpusEncoder()\n",
       "  (classifier): ClassificationHead(\n",
       "    (mlp): GPTMLP(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=19, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): Dropout(p=0.5, inplace=False)\n",
       "        (8): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (9): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 2.3337e-02,  0.0000e+00, -1.3661e-02,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-1.9520e-01, -1.0922e-03,  5.8503e-02,  ...,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00],\n",
       "         ...,\n",
       "         [-4.5488e-03,  6.0890e-03, -3.0827e-03,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 4.9453e-02,  3.0743e-02, -9.1785e-04,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-2.8000e-02,  4.5174e-03, -6.7419e-03,  ...,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00]], dtype=torch.float64),\n",
       " tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 3.6228e-03,  6.5330e-03,  6.0443e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-5.9292e-03, -4.0927e-02, -2.0905e-02,  ...,  1.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-3.3140e-03, -1.9744e-02,  2.2270e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 1.3176e-03,  7.1796e-04,  1.0471e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 7.8910e-03, -1.3175e-02, -8.4360e-03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-2.7449e-02, -1.6232e-02,  9.9189e-02,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], dtype=torch.float64), tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0])]\n",
      "128.0\n"
     ]
    }
   ],
   "source": [
    "# Test output\n",
    "batch = next(iter(train_loader))\n",
    "print(batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_ind, y = batch\n",
    "    my_model_output = my_model(None, None, X_ind.float().to(device))\n",
    "\n",
    "print(my_model_output.size(0)/64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (nontext_network): NontextualNetwork()\n",
       "  (corpus_encoder): CorpusEncoder()\n",
       "  (classifier): ClassificationHead(\n",
       "    (mlp): GPTMLP(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=19, out_features=128, bias=True)\n",
       "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "        (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (6): ReLU()\n",
       "        (7): Dropout(p=0.5, inplace=False)\n",
       "        (8): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (9): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/84 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([64])) must be the same as input size (torch.Size([8192]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(my_model, train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader,config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m      2\u001b[0m             device\u001b[39m=\u001b[39;49mdevice, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, eval_every\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mno_nlp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py:149\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, config, device, max_epochs, eval_every, name, train_loss_history, starting_epoch)\u001b[0m\n\u001b[0;32m    147\u001b[0m output \u001b[39m=\u001b[39m model(X_text, X_mask, X_ind)\n\u001b[0;32m    148\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39msize())\n\u001b[1;32m--> 149\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y)\n\u001b[0;32m    150\u001b[0m \u001b[39m# print(loss.grad_fn)\u001b[39;00m\n\u001b[0;32m    152\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[0;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[0;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\torch\\nn\\functional.py:3160\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3157\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m-> 3160\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[0;32m   3162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([64])) must be the same as input size (torch.Size([8192]))"
     ]
    }
   ],
   "source": [
    "train(my_model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
    "            device=device, max_epochs=5, eval_every=2, name=\"no_nlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-08 22:50:20,964]\u001b[0m A new study created in memory with name: no-name-7219f6bd-89ad-4c1b-9c4c-27298a09bd60\u001b[0m\n",
      "c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\study.py:393: FutureWarning: `n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/336 [00:00<?, ?batch/s]\n",
      "Epoch 1:   0%|          | 0/336 [00:00<?, ?batch/s]\n",
      "Epoch 1:   0%|          | 1/336 [00:00<02:34,  2.17batch/s, accuracy=56.2, loss=0.694]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   1%|          | 2/336 [00:01<02:52,  1.93batch/s, accuracy=56.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   1%|          | 3/336 [00:01<03:09,  1.76batch/s, accuracy=56.2, loss=0.693]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   1%|          | 4/336 [00:02<02:49,  1.96batch/s, accuracy=59.4, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   2%|▏         | 6/336 [00:02<02:33,  2.14batch/s, accuracy=62.5, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   2%|▏         | 6/336 [00:03<02:33,  2.14batch/s, accuracy=62.5, loss=0.691]\n",
      "Epoch 1:   2%|▏         | 7/336 [00:03<03:04,  1.79batch/s, accuracy=62.5, loss=0.691]\n",
      "Epoch 1:   2%|▏         | 8/336 [00:04<02:29,  2.19batch/s, accuracy=60.9, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   3%|▎         | 9/336 [00:04<02:37,  2.08batch/s, accuracy=61.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   3%|▎         | 10/336 [00:05<02:42,  2.00batch/s, accuracy=60.6, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   3%|▎         | 11/336 [00:05<02:44,  1.98batch/s, accuracy=59.7, loss=0.692]\n",
      "Epoch 1:   3%|▎         | 11/336 [00:06<02:44,  1.98batch/s, accuracy=59.4, loss=0.692]\n",
      "Epoch 1:   4%|▎         | 12/336 [00:06<02:49,  1.91batch/s, accuracy=59.4, loss=0.692]\n",
      "Epoch 1:   4%|▍         | 13/336 [00:06<02:39,  2.03batch/s, accuracy=59.1, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   4%|▍         | 14/336 [00:07<02:43,  1.97batch/s, accuracy=58.9, loss=0.692]\n",
      "\u001b[A\n",
      "Epoch 1:   4%|▍         | 14/336 [00:07<02:43,  1.97batch/s, accuracy=59.2, loss=0.691]\n",
      "Epoch 1:   4%|▍         | 15/336 [00:07<02:28,  2.17batch/s, accuracy=59.2, loss=0.691]\n",
      "Epoch 1:   4%|▍         | 15/336 [00:07<02:28,  2.17batch/s, accuracy=59.4, loss=0.691]\n",
      "Epoch 1:   5%|▍         | 16/336 [00:07<02:29,  2.15batch/s, accuracy=59.4, loss=0.691]\n",
      "Epoch 1:   5%|▌         | 17/336 [00:08<02:21,  2.25batch/s, accuracy=58.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   5%|▌         | 18/336 [00:08<02:32,  2.08batch/s, accuracy=59.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   6%|▌         | 19/336 [00:09<02:36,  2.02batch/s, accuracy=61.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   6%|▌         | 20/336 [00:09<02:28,  2.13batch/s, accuracy=60.9, loss=0.69] \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   6%|▋         | 21/336 [00:10<02:36,  2.01batch/s, accuracy=61.3, loss=0.69]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   7%|▋         | 23/336 [00:11<02:23,  2.18batch/s, accuracy=60.6, loss=0.69] \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   7%|▋         | 24/336 [00:11<02:32,  2.05batch/s, accuracy=61.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   7%|▋         | 25/336 [00:12<02:39,  1.95batch/s, accuracy=60, loss=0.691]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   8%|▊         | 26/336 [00:12<02:41,  1.92batch/s, accuracy=61.1, loss=0.69]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   8%|▊         | 27/336 [00:13<02:28,  2.08batch/s, accuracy=61.3, loss=0.69]\n",
      "\u001b[A\n",
      "Epoch 1:   8%|▊         | 28/336 [00:13<02:31,  2.04batch/s, accuracy=61.2, loss=0.69]\n",
      "Epoch 1:   9%|▊         | 29/336 [00:14<02:21,  2.16batch/s, accuracy=60.3, loss=0.69]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   9%|▉         | 30/336 [00:14<02:25,  2.10batch/s, accuracy=60, loss=0.69]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:   9%|▉         | 31/336 [00:15<02:33,  1.99batch/s, accuracy=59.7, loss=0.69]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  10%|▉         | 32/336 [00:15<02:23,  2.12batch/s, accuracy=59.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  10%|▉         | 33/336 [00:16<02:32,  1.98batch/s, accuracy=59.8, loss=0.69] \n",
      "\u001b[A\n",
      "Epoch 1:  10%|█         | 34/336 [00:16<02:34,  1.95batch/s, accuracy=60.1, loss=0.69]\n",
      "Epoch 1:  10%|█         | 35/336 [00:17<02:32,  1.98batch/s, accuracy=60.4, loss=0.69]\n",
      "Epoch 1:  10%|█         | 35/336 [00:17<02:32,  1.98batch/s, accuracy=60.4, loss=0.69]\n",
      "\u001b[A\n",
      "Epoch 1:  11%|█         | 36/336 [00:17<02:20,  2.13batch/s, accuracy=60.4, loss=0.69]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  11%|█         | 37/336 [00:18<02:30,  1.99batch/s, accuracy=60.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  11%|█▏        | 38/336 [00:18<02:32,  1.95batch/s, accuracy=59.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  12%|█▏        | 39/336 [00:19<02:24,  2.05batch/s, accuracy=59.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  12%|█▏        | 40/336 [00:19<02:30,  1.97batch/s, accuracy=59.5, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  12%|█▏        | 41/336 [00:20<02:30,  1.97batch/s, accuracy=59.6, loss=0.691]\n",
      "Epoch 1:  12%|█▎        | 42/336 [00:20<02:18,  2.13batch/s, accuracy=59.5, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  12%|█▎        | 42/336 [00:20<02:18,  2.13batch/s, accuracy=59.5, loss=0.691]\n",
      "Epoch 1:  13%|█▎        | 43/336 [00:21<02:22,  2.05batch/s, accuracy=59.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  13%|█▎        | 44/336 [00:21<02:31,  1.93batch/s, accuracy=59.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  13%|█▎        | 45/336 [00:22<02:30,  1.93batch/s, accuracy=59.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  14%|█▎        | 46/336 [00:23<02:33,  1.90batch/s, accuracy=58.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  14%|█▍        | 47/336 [00:23<02:21,  2.04batch/s, accuracy=58.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  14%|█▍        | 48/336 [00:23<02:23,  2.00batch/s, accuracy=58.7, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  15%|█▍        | 49/336 [00:24<02:29,  1.92batch/s, accuracy=58.8, loss=0.691]\n",
      "Epoch 1:  15%|█▍        | 50/336 [00:24<02:16,  2.09batch/s, accuracy=59, loss=0.691]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  15%|█▌        | 51/336 [00:25<02:25,  1.97batch/s, accuracy=59.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  15%|█▌        | 52/336 [00:25<02:26,  1.94batch/s, accuracy=58.9, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  16%|█▌        | 53/336 [00:26<02:25,  1.95batch/s, accuracy=58.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  16%|█▌        | 54/336 [00:26<02:14,  2.09batch/s, accuracy=58.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  17%|█▋        | 56/336 [00:27<02:08,  2.18batch/s, accuracy=58.9, loss=0.691]\n",
      "Epoch 1:  17%|█▋        | 56/336 [00:27<02:08,  2.18batch/s, accuracy=58.9, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  17%|█▋        | 57/336 [00:28<02:18,  2.01batch/s, accuracy=58.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  17%|█▋        | 58/336 [00:28<02:22,  1.95batch/s, accuracy=58.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  18%|█▊        | 59/336 [00:29<02:16,  2.03batch/s, accuracy=58.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  18%|█▊        | 60/336 [00:29<02:22,  1.94batch/s, accuracy=58.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  18%|█▊        | 61/336 [00:30<02:25,  1.90batch/s, accuracy=58, loss=0.691]  \n",
      "Epoch 1:  18%|█▊        | 61/336 [00:30<02:25,  1.90batch/s, accuracy=58.1, loss=0.691]\n",
      "Epoch 1:  18%|█▊        | 62/336 [00:30<02:26,  1.87batch/s, accuracy=58.1, loss=0.691]\n",
      "Epoch 1:  19%|█▉        | 63/336 [00:31<02:21,  1.93batch/s, accuracy=58.1, loss=0.691]\n",
      "Epoch 1:  19%|█▉        | 63/336 [00:31<02:21,  1.93batch/s, accuracy=58.1, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  19%|█▉        | 64/336 [00:32<02:29,  1.82batch/s, accuracy=58.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  19%|█▉        | 65/336 [00:32<02:27,  1.84batch/s, accuracy=58.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  20%|█▉        | 66/336 [00:33<02:26,  1.84batch/s, accuracy=58.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  20%|█▉        | 67/336 [00:33<02:18,  1.95batch/s, accuracy=58.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  20%|██        | 68/336 [00:34<02:24,  1.86batch/s, accuracy=58.6, loss=0.691]\n",
      "Epoch 1:  20%|██        | 68/336 [00:34<02:24,  1.86batch/s, accuracy=58.8, loss=0.691]\n",
      "Epoch 1:  21%|██        | 69/336 [00:34<02:28,  1.80batch/s, accuracy=58.8, loss=0.691]\n",
      "Epoch 1:  21%|██        | 70/336 [00:35<02:17,  1.94batch/s, accuracy=58.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  21%|██        | 71/336 [00:35<02:20,  1.89batch/s, accuracy=58.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  21%|██▏       | 72/336 [00:36<02:07,  2.07batch/s, accuracy=58.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  22%|██▏       | 73/336 [00:36<02:08,  2.04batch/s, accuracy=58.4, loss=0.691]\n",
      "Epoch 1:  22%|██▏       | 73/336 [00:37<02:08,  2.04batch/s, accuracy=58.2, loss=0.691]\n",
      "Epoch 1:  22%|██▏       | 74/336 [00:37<02:12,  1.97batch/s, accuracy=58.2, loss=0.691]\n",
      "Epoch 1:  22%|██▏       | 75/336 [00:37<02:04,  2.09batch/s, accuracy=58.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  23%|██▎       | 76/336 [00:38<02:12,  1.97batch/s, accuracy=58, loss=0.691]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  23%|██▎       | 77/336 [00:38<02:14,  1.92batch/s, accuracy=58, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  23%|██▎       | 78/336 [00:39<02:13,  1.94batch/s, accuracy=57.9, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  24%|██▎       | 79/336 [00:39<02:03,  2.08batch/s, accuracy=58.1, loss=0.691]\n",
      "Epoch 1:  24%|██▎       | 79/336 [00:39<02:03,  2.08batch/s, accuracy=58.1, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  24%|██▎       | 79/336 [00:40<02:03,  2.08batch/s, accuracy=58, loss=0.691]  \n",
      "Epoch 1:  24%|██▍       | 80/336 [00:40<02:09,  1.97batch/s, accuracy=58, loss=0.691]\n",
      "Epoch 1:  24%|██▍       | 80/336 [00:40<02:09,  1.97batch/s, accuracy=58, loss=0.691]\n",
      "Epoch 1:  24%|██▍       | 81/336 [00:40<02:14,  1.89batch/s, accuracy=58, loss=0.691]\n",
      "Epoch 1:  24%|██▍       | 81/336 [00:40<02:14,  1.89batch/s, accuracy=58, loss=0.691]\n",
      "Epoch 1:  24%|██▍       | 81/336 [00:41<02:14,  1.89batch/s, accuracy=58.1, loss=0.691]\n",
      "Epoch 1:  24%|██▍       | 82/336 [00:41<02:17,  1.85batch/s, accuracy=58.1, loss=0.691]\n",
      "Epoch 1:  25%|██▍       | 83/336 [00:41<02:04,  2.03batch/s, accuracy=58.1, loss=0.691]\n",
      "Epoch 1:  25%|██▍       | 83/336 [00:41<02:04,  2.03batch/s, accuracy=58.1, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  25%|██▌       | 84/336 [00:42<02:06,  2.00batch/s, accuracy=57.9, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  25%|██▌       | 85/336 [00:42<02:07,  1.97batch/s, accuracy=57.9, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  26%|██▌       | 86/336 [00:43<01:56,  2.14batch/s, accuracy=57.8, loss=0.691]\n",
      "Epoch 1:  26%|██▌       | 87/336 [00:43<01:50,  2.25batch/s, accuracy=57.9, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  26%|██▌       | 87/336 [00:43<01:50,  2.25batch/s, accuracy=57.9, loss=0.691]\n",
      "Epoch 1:  26%|██▌       | 88/336 [00:44<02:17,  1.80batch/s, accuracy=57.7, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  26%|██▌       | 88/336 [00:44<02:17,  1.80batch/s, accuracy=57.7, loss=0.691]\n",
      "Epoch 1:  26%|██▋       | 89/336 [00:44<02:11,  1.88batch/s, accuracy=57.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  27%|██▋       | 90/336 [00:45<02:11,  1.87batch/s, accuracy=57.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  27%|██▋       | 91/336 [00:45<02:15,  1.81batch/s, accuracy=57.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  27%|██▋       | 92/336 [00:46<02:18,  1.77batch/s, accuracy=57.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  28%|██▊       | 93/336 [00:47<02:17,  1.76batch/s, accuracy=57.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  28%|██▊       | 94/336 [00:47<02:08,  1.88batch/s, accuracy=57.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  28%|██▊       | 95/336 [00:48<02:05,  1.92batch/s, accuracy=57.3, loss=0.691]\n",
      "Epoch 1:  28%|██▊       | 95/336 [00:48<02:05,  1.92batch/s, accuracy=57.4, loss=0.691]\n",
      "Epoch 1:  29%|██▊       | 96/336 [00:48<02:01,  1.97batch/s, accuracy=57.4, loss=0.691]\n",
      "Epoch 1:  29%|██▊       | 96/336 [00:48<02:01,  1.97batch/s, accuracy=57.4, loss=0.691]\n",
      "Epoch 1:  29%|██▊       | 96/336 [00:49<02:01,  1.97batch/s, accuracy=57.3, loss=0.691]\n",
      "Epoch 1:  29%|██▉       | 97/336 [00:49<02:06,  1.88batch/s, accuracy=57.3, loss=0.691]\n",
      "Epoch 1:  29%|██▉       | 97/336 [00:49<02:06,  1.88batch/s, accuracy=57.3, loss=0.691]\n",
      "Epoch 1:  29%|██▉       | 97/336 [00:49<02:06,  1.88batch/s, accuracy=57.5, loss=0.691]\n",
      "Epoch 1:  29%|██▉       | 98/336 [00:49<02:05,  1.89batch/s, accuracy=57.5, loss=0.691]\n",
      "Epoch 1:  29%|██▉       | 99/336 [00:50<02:02,  1.94batch/s, accuracy=57.5, loss=0.691]\n",
      "Epoch 1:  29%|██▉       | 99/336 [00:50<02:02,  1.94batch/s, accuracy=57.5, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  30%|██▉       | 100/336 [00:50<02:08,  1.84batch/s, accuracy=57.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  30%|███       | 101/336 [00:51<02:08,  1.83batch/s, accuracy=57.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  31%|███       | 103/336 [00:51<01:41,  2.29batch/s, accuracy=57.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  31%|███       | 104/336 [00:52<01:47,  2.16batch/s, accuracy=57.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  31%|███▏      | 105/336 [00:53<01:59,  1.93batch/s, accuracy=57.4, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  32%|███▏      | 106/336 [00:53<02:03,  1.86batch/s, accuracy=57.3, loss=0.691]\n",
      "Epoch 1:  32%|███▏      | 107/336 [00:54<01:53,  2.01batch/s, accuracy=56.9, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  32%|███▏      | 107/336 [00:54<01:53,  2.01batch/s, accuracy=56.9, loss=0.691]\n",
      "Epoch 1:  32%|███▏      | 108/336 [00:54<01:56,  1.95batch/s, accuracy=56.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  32%|███▏      | 109/336 [00:55<01:58,  1.92batch/s, accuracy=56.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  33%|███▎      | 110/336 [00:55<01:56,  1.94batch/s, accuracy=56.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  33%|███▎      | 111/336 [00:56<02:05,  1.80batch/s, accuracy=56.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  33%|███▎      | 112/336 [00:56<01:56,  1.93batch/s, accuracy=56.6, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  34%|███▎      | 113/336 [00:57<01:58,  1.88batch/s, accuracy=56.7, loss=0.691]\n",
      "Epoch 1:  34%|███▍      | 114/336 [00:57<01:47,  2.07batch/s, accuracy=56.7, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  34%|███▍      | 114/336 [00:57<01:47,  2.07batch/s, accuracy=56.7, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  34%|███▍      | 114/336 [00:58<01:47,  2.07batch/s, accuracy=56.8, loss=0.691]\n",
      "Epoch 1:  34%|███▍      | 115/336 [00:58<02:03,  1.79batch/s, accuracy=56.8, loss=0.691]\n",
      "Epoch 1:  35%|███▍      | 116/336 [00:58<01:52,  1.96batch/s, accuracy=56.8, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  35%|███▍      | 116/336 [00:59<01:52,  1.96batch/s, accuracy=56.8, loss=0.691]\n",
      "Epoch 1:  35%|███▍      | 117/336 [00:59<02:01,  1.80batch/s, accuracy=56.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  35%|███▌      | 118/336 [01:00<02:03,  1.76batch/s, accuracy=56.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  35%|███▌      | 119/336 [01:00<01:58,  1.83batch/s, accuracy=57, loss=0.691]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  36%|███▌      | 120/336 [01:01<02:00,  1.80batch/s, accuracy=56.9, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  36%|███▌      | 121/336 [01:01<01:50,  1.95batch/s, accuracy=57.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  36%|███▋      | 122/336 [01:02<01:55,  1.85batch/s, accuracy=57.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  37%|███▋      | 123/336 [01:02<01:57,  1.81batch/s, accuracy=57.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  37%|███▋      | 123/336 [01:03<01:57,  1.81batch/s, accuracy=57, loss=0.691]  \n",
      "Epoch 1:  37%|███▋      | 125/336 [01:03<01:44,  2.02batch/s, accuracy=57, loss=0.691]\n",
      "Epoch 1:  37%|███▋      | 125/336 [01:03<01:44,  2.02batch/s, accuracy=57, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  38%|███▊      | 126/336 [01:04<01:48,  1.93batch/s, accuracy=56.9, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  38%|███▊      | 127/336 [01:04<01:38,  2.11batch/s, accuracy=56.9, loss=0.691]\n",
      "Epoch 1:  38%|███▊      | 127/336 [01:05<01:38,  2.11batch/s, accuracy=56.9, loss=0.691]\n",
      "Epoch 1:  38%|███▊      | 128/336 [01:05<01:37,  2.13batch/s, accuracy=56.9, loss=0.691]\n",
      "Epoch 1:  38%|███▊      | 129/336 [01:05<01:32,  2.25batch/s, accuracy=57, loss=0.691]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  39%|███▊      | 130/336 [01:06<01:40,  2.06batch/s, accuracy=57, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  39%|███▉      | 131/336 [01:06<01:41,  2.02batch/s, accuracy=56.9, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  39%|███▉      | 132/336 [01:07<01:44,  1.95batch/s, accuracy=56.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  40%|███▉      | 133/336 [01:07<01:32,  2.18batch/s, accuracy=56.8, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  40%|███▉      | 134/336 [01:07<01:32,  2.17batch/s, accuracy=56.8, loss=0.691]\n",
      "Epoch 1:  40%|████      | 135/336 [01:08<01:28,  2.27batch/s, accuracy=56.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  40%|████      | 136/336 [01:08<01:34,  2.12batch/s, accuracy=56.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  41%|████      | 137/336 [01:09<01:37,  2.03batch/s, accuracy=56.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  41%|████      | 138/336 [01:09<01:40,  1.97batch/s, accuracy=56.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  42%|████▏     | 140/336 [01:10<01:23,  2.34batch/s, accuracy=56.6, loss=0.691]\n",
      "Epoch 1:  42%|████▏     | 140/336 [01:10<01:23,  2.34batch/s, accuracy=56.6, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  42%|████▏     | 141/336 [01:11<01:27,  2.22batch/s, accuracy=56.4, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  42%|████▏     | 141/336 [01:11<01:27,  2.22batch/s, accuracy=56.4, loss=0.691]\n",
      "Epoch 1:  42%|████▏     | 142/336 [01:11<01:32,  2.10batch/s, accuracy=56.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  43%|████▎     | 143/336 [01:12<01:37,  1.97batch/s, accuracy=56.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  43%|████▎     | 144/336 [01:12<01:40,  1.91batch/s, accuracy=56.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  43%|████▎     | 145/336 [01:13<01:39,  1.93batch/s, accuracy=56.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  43%|████▎     | 146/336 [01:13<01:43,  1.84batch/s, accuracy=56.6, loss=0.691]\n",
      "Epoch 1:  43%|████▎     | 146/336 [01:14<01:43,  1.84batch/s, accuracy=56.5, loss=0.691]\n",
      "Epoch 1:  44%|████▍     | 147/336 [01:14<01:46,  1.78batch/s, accuracy=56.5, loss=0.691]\n",
      "Epoch 1:  44%|████▍     | 148/336 [01:15<01:40,  1.87batch/s, accuracy=56.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  44%|████▍     | 149/336 [01:15<01:42,  1.83batch/s, accuracy=56.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  45%|████▍     | 150/336 [01:16<01:38,  1.89batch/s, accuracy=56.3, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  45%|████▍     | 151/336 [01:16<01:47,  1.71batch/s, accuracy=56.2, loss=0.691]\n",
      "Epoch 1:  45%|████▌     | 152/336 [01:17<01:46,  1.73batch/s, accuracy=56.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  46%|████▌     | 153/336 [01:18<01:53,  1.61batch/s, accuracy=56.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  46%|████▌     | 154/336 [01:18<01:51,  1.64batch/s, accuracy=56.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  46%|████▌     | 155/336 [01:19<01:53,  1.59batch/s, accuracy=56.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  46%|████▋     | 156/336 [01:19<01:47,  1.67batch/s, accuracy=56.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  47%|████▋     | 157/336 [01:20<01:38,  1.82batch/s, accuracy=56.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  47%|████▋     | 158/336 [01:20<01:35,  1.86batch/s, accuracy=56, loss=0.692]  \n",
      "Epoch 1:  47%|████▋     | 158/336 [01:21<01:35,  1.86batch/s, accuracy=56, loss=0.692]\n",
      "Epoch 1:  47%|████▋     | 159/336 [01:21<01:35,  1.85batch/s, accuracy=56, loss=0.692]\n",
      "Epoch 1:  48%|████▊     | 160/336 [01:21<01:29,  1.97batch/s, accuracy=56, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  48%|████▊     | 161/336 [01:22<01:33,  1.88batch/s, accuracy=55.9, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  48%|████▊     | 162/336 [01:22<01:23,  2.10batch/s, accuracy=55.9, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  49%|████▉     | 164/336 [01:23<01:17,  2.21batch/s, accuracy=55.9, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  49%|████▉     | 165/336 [01:24<01:12,  2.35batch/s, accuracy=55.8, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  49%|████▉     | 166/336 [01:24<01:19,  2.13batch/s, accuracy=55.8, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  50%|████▉     | 167/336 [01:25<01:25,  1.98batch/s, accuracy=55.9, loss=0.692]\n",
      "\u001b[A\n",
      "Epoch 1:  50%|█████     | 168/336 [01:25<01:24,  1.99batch/s, accuracy=56, loss=0.692]  \n",
      "Epoch 1:  50%|█████     | 168/336 [01:26<01:24,  1.99batch/s, accuracy=55.8, loss=0.692]\n",
      "Epoch 1:  50%|█████     | 169/336 [01:26<01:27,  1.92batch/s, accuracy=55.8, loss=0.692]\n",
      "Epoch 1:  50%|█████     | 169/336 [01:26<01:27,  1.92batch/s, accuracy=55.8, loss=0.692]\n",
      "Epoch 1:  51%|█████     | 170/336 [01:26<01:20,  2.07batch/s, accuracy=55.8, loss=0.692]\n",
      "Epoch 1:  51%|█████     | 170/336 [01:26<01:20,  2.07batch/s, accuracy=55.8, loss=0.692]\n",
      "\u001b[A\n",
      "Epoch 1:  51%|█████     | 171/336 [01:27<01:22,  1.99batch/s, accuracy=55.7, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  51%|█████     | 172/336 [01:27<01:24,  1.94batch/s, accuracy=55.7, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  51%|█████▏    | 173/336 [01:28<01:28,  1.85batch/s, accuracy=55.7, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 174/336 [01:28<01:26,  1.88batch/s, accuracy=55.7, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 175/336 [01:29<01:28,  1.81batch/s, accuracy=55.8, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  52%|█████▏    | 176/336 [01:29<01:25,  1.88batch/s, accuracy=55.8, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 177/336 [01:30<01:19,  1.99batch/s, accuracy=55.9, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 178/336 [01:30<01:20,  1.96batch/s, accuracy=55.9, loss=0.692]\n",
      "\u001b[A\n",
      "Epoch 1:  53%|█████▎    | 179/336 [01:31<01:25,  1.83batch/s, accuracy=56, loss=0.691]  \n",
      "Epoch 1:  53%|█████▎    | 179/336 [01:31<01:25,  1.83batch/s, accuracy=55.9, loss=0.692]\n",
      "Epoch 1:  54%|█████▎    | 180/336 [01:31<01:22,  1.89batch/s, accuracy=55.9, loss=0.692]\n",
      "\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 181/336 [01:32<01:17,  2.01batch/s, accuracy=55.9, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 181/336 [01:32<01:17,  2.01batch/s, accuracy=55.9, loss=0.691]\n",
      "Epoch 1:  54%|█████▍    | 182/336 [01:32<01:17,  1.99batch/s, accuracy=55.9, loss=0.692]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  54%|█████▍    | 183/336 [01:33<01:19,  1.92batch/s, accuracy=55.9, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  55%|█████▍    | 184/336 [01:34<01:21,  1.88batch/s, accuracy=56, loss=0.691]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  55%|█████▌    | 185/336 [01:34<01:14,  2.04batch/s, accuracy=56, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 187/336 [01:35<01:09,  2.15batch/s, accuracy=55.8, loss=0.691]\n",
      "Epoch 1:  56%|█████▌    | 187/336 [01:35<01:09,  2.15batch/s, accuracy=55.8, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  56%|█████▌    | 188/336 [01:35<01:14,  2.00batch/s, accuracy=56, loss=0.691]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  56%|█████▋    | 189/336 [01:36<01:10,  2.08batch/s, accuracy=55.9, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 190/336 [01:37<01:17,  1.88batch/s, accuracy=56, loss=0.691]  \n",
      "\u001b[A\n",
      "Epoch 1:  57%|█████▋    | 191/336 [01:37<01:19,  1.83batch/s, accuracy=55.9, loss=0.691]\n",
      "Epoch 1:  57%|█████▋    | 191/336 [01:37<01:19,  1.83batch/s, accuracy=55.9, loss=0.691]\n",
      "Epoch 1:  57%|█████▋    | 192/336 [01:38<01:17,  1.87batch/s, accuracy=55.9, loss=0.691]\n",
      "Epoch 1:  57%|█████▋    | 192/336 [01:38<01:17,  1.87batch/s, accuracy=55.9, loss=0.691]\n",
      "Epoch 1:  57%|█████▋    | 193/336 [01:38<01:03,  2.24batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  58%|█████▊    | 194/336 [01:38<01:05,  2.16batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  58%|█████▊    | 194/336 [01:38<01:05,  2.16batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  58%|█████▊    | 194/336 [01:39<01:05,  2.16batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  58%|█████▊    | 195/336 [01:39<01:09,  2.04batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  58%|█████▊    | 195/336 [01:39<01:09,  2.04batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  58%|█████▊    | 196/336 [01:39<01:06,  2.11batch/s, accuracy=55.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  59%|█████▊    | 197/336 [01:40<01:12,  1.92batch/s, accuracy=55.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 198/336 [01:41<01:13,  1.87batch/s, accuracy=55.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  59%|█████▉    | 199/336 [01:41<01:07,  2.03batch/s, accuracy=55.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  60%|█████▉    | 200/336 [01:41<01:09,  1.96batch/s, accuracy=55.8, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  60%|█████▉    | 201/336 [01:42<01:10,  1.92batch/s, accuracy=55.8, loss=0.691]\n",
      "Epoch 1:  60%|██████    | 202/336 [01:42<01:02,  2.13batch/s, accuracy=55.8, loss=0.691]\n",
      "Epoch 1:  60%|██████    | 202/336 [01:43<01:02,  2.13batch/s, accuracy=55.8, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  60%|██████    | 203/336 [01:43<01:14,  1.79batch/s, accuracy=55.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  61%|██████    | 204/336 [01:44<01:19,  1.66batch/s, accuracy=55.7, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  61%|██████    | 205/336 [01:44<01:10,  1.86batch/s, accuracy=55.7, loss=0.691]\n",
      "Epoch 1:  61%|██████    | 205/336 [01:45<01:10,  1.86batch/s, accuracy=55.7, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  61%|██████▏   | 206/336 [01:45<01:12,  1.80batch/s, accuracy=55.8, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 207/336 [01:45<01:14,  1.72batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  62%|██████▏   | 208/336 [01:46<01:16,  1.67batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  62%|██████▏   | 208/336 [01:46<01:16,  1.67batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  62%|██████▏   | 208/336 [01:47<01:16,  1.67batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  62%|██████▏   | 209/336 [01:47<01:15,  1.68batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  62%|██████▎   | 210/336 [01:47<01:07,  1.86batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  62%|██████▎   | 210/336 [01:47<01:07,  1.86batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 211/336 [01:48<01:10,  1.78batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 212/336 [01:48<01:08,  1.82batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  63%|██████▎   | 213/336 [01:49<01:03,  1.93batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 215/336 [01:50<01:02,  1.93batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  64%|██████▍   | 215/336 [01:50<01:02,  1.93batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  64%|██████▍   | 216/336 [01:50<01:07,  1.78batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 217/336 [01:51<01:05,  1.83batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  65%|██████▍   | 218/336 [01:51<01:00,  1.94batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  65%|██████▍   | 218/336 [01:52<01:00,  1.94batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  65%|██████▌   | 219/336 [01:52<01:03,  1.84batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  65%|██████▌   | 219/336 [01:52<01:03,  1.84batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  65%|██████▌   | 220/336 [01:52<01:01,  1.89batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  65%|██████▌   | 220/336 [01:53<01:01,  1.89batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  65%|██████▌   | 220/336 [01:53<01:01,  1.89batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  66%|██████▌   | 221/336 [01:53<01:12,  1.60batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  66%|██████▌   | 222/336 [01:54<01:05,  1.75batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  66%|██████▌   | 222/336 [01:54<01:05,  1.75batch/s, accuracy=55.5, loss=0.691]\n",
      "Epoch 1:  66%|██████▋   | 223/336 [01:54<01:02,  1.81batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 224/336 [01:55<01:06,  1.69batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 225/336 [01:56<01:06,  1.66batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  67%|██████▋   | 226/336 [01:56<00:59,  1.86batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 227/336 [01:57<01:02,  1.73batch/s, accuracy=55.6, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 228/336 [01:57<01:00,  1.78batch/s, accuracy=55.6, loss=0.691]\n",
      "Epoch 1:  68%|██████▊   | 229/336 [01:58<00:56,  1.90batch/s, accuracy=55.5, loss=0.691]\n",
      "Epoch 1:  68%|██████▊   | 229/336 [01:58<00:56,  1.90batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  68%|██████▊   | 230/336 [01:58<00:54,  1.94batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 231/336 [01:59<00:59,  1.78batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 232/336 [01:59<00:58,  1.79batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  69%|██████▉   | 233/336 [02:00<00:57,  1.80batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 234/336 [02:01<00:55,  1.85batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  70%|██████▉   | 235/336 [02:01<01:02,  1.61batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  70%|███████   | 236/336 [02:02<00:55,  1.79batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  71%|███████   | 237/336 [02:02<00:58,  1.69batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  71%|███████   | 238/336 [02:03<00:58,  1.67batch/s, accuracy=55.5, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  71%|███████   | 239/336 [02:04<01:02,  1.55batch/s, accuracy=55.4, loss=0.691]\n",
      "Epoch 1:  71%|███████   | 239/336 [02:04<01:02,  1.55batch/s, accuracy=55.5, loss=0.691]\n",
      "Epoch 1:  71%|███████▏  | 240/336 [02:04<00:59,  1.60batch/s, accuracy=55.5, loss=0.691]\n",
      "Epoch 1:  71%|███████▏  | 240/336 [02:04<00:59,  1.60batch/s, accuracy=55.5, loss=0.691]\n",
      "Epoch 1:  71%|███████▏  | 240/336 [02:05<00:59,  1.60batch/s, accuracy=55.4, loss=0.691]\n",
      "Epoch 1:  72%|███████▏  | 241/336 [02:05<01:03,  1.51batch/s, accuracy=55.4, loss=0.691]\n",
      "Epoch 1:  72%|███████▏  | 241/336 [02:05<01:03,  1.51batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 242/336 [02:06<01:08,  1.38batch/s, accuracy=55.4, loss=0.691]\n",
      "Epoch 1:  72%|███████▏  | 243/336 [02:06<01:00,  1.55batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  72%|███████▏  | 243/336 [02:07<01:00,  1.55batch/s, accuracy=55.4, loss=0.691]\n",
      "Epoch 1:  73%|███████▎  | 244/336 [02:07<00:59,  1.54batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 245/336 [02:08<00:56,  1.61batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  73%|███████▎  | 246/336 [02:08<00:56,  1.60batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  73%|███████▎  | 246/336 [02:09<00:56,  1.60batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  74%|███████▎  | 247/336 [02:09<00:55,  1.62batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  74%|███████▍  | 248/336 [02:09<00:52,  1.69batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 249/336 [02:10<00:51,  1.70batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  74%|███████▍  | 250/336 [02:10<00:49,  1.75batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  75%|███████▍  | 251/336 [02:11<00:43,  1.94batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 252/336 [02:12<00:45,  1.85batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  75%|███████▌  | 253/336 [02:12<00:45,  1.82batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  75%|███████▌  | 253/336 [02:12<00:45,  1.82batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  76%|███████▌  | 254/336 [02:13<00:43,  1.87batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  76%|███████▌  | 254/336 [02:13<00:43,  1.87batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  76%|███████▌  | 255/336 [02:13<00:45,  1.77batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  76%|███████▌  | 256/336 [02:14<00:46,  1.73batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  76%|███████▋  | 257/336 [02:14<00:48,  1.63batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 258/336 [02:15<00:44,  1.76batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 259/336 [02:16<00:48,  1.59batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  77%|███████▋  | 260/336 [02:16<00:44,  1.71batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 261/336 [02:17<00:43,  1.74batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  78%|███████▊  | 262/336 [02:17<00:42,  1.73batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  78%|███████▊  | 263/336 [02:18<00:38,  1.91batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  79%|███████▊  | 264/336 [02:18<00:40,  1.78batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 265/336 [02:19<00:36,  1.95batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 266/336 [02:19<00:38,  1.81batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  79%|███████▉  | 266/336 [02:19<00:38,  1.81batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  79%|███████▉  | 266/336 [02:20<00:38,  1.81batch/s, accuracy=55.4, loss=0.691]\n",
      "Epoch 1:  80%|███████▉  | 268/336 [02:20<00:34,  1.99batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  80%|████████  | 269/336 [02:21<00:35,  1.87batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  80%|████████  | 270/336 [02:21<00:31,  2.09batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  81%|████████  | 271/336 [02:22<00:33,  1.96batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  81%|████████▏ | 273/336 [02:23<00:27,  2.28batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 274/336 [02:23<00:29,  2.11batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 276/336 [02:24<00:27,  2.21batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  82%|████████▏ | 277/336 [02:25<00:27,  2.11batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 278/336 [02:25<00:27,  2.11batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 279/336 [02:26<00:29,  1.96batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  83%|████████▎ | 280/336 [02:26<00:29,  1.92batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  84%|████████▎ | 281/336 [02:27<00:26,  2.10batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 282/336 [02:27<00:26,  2.05batch/s, accuracy=55.4, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 283/336 [02:28<00:26,  1.99batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 284/336 [02:28<00:27,  1.92batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  85%|████████▍ | 285/336 [02:29<00:26,  1.94batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 286/336 [02:29<00:25,  1.94batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 287/336 [02:30<00:26,  1.82batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  86%|████████▌ | 288/336 [02:30<00:26,  1.80batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  86%|████████▌ | 288/336 [02:30<00:26,  1.80batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  86%|████████▌ | 288/336 [02:31<00:26,  1.80batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  86%|████████▌ | 289/336 [02:31<00:25,  1.85batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  86%|████████▋ | 290/336 [02:31<00:23,  1.96batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  86%|████████▋ | 290/336 [02:32<00:23,  1.96batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 291/336 [02:32<00:24,  1.84batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 292/336 [02:33<00:23,  1.86batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  87%|████████▋ | 293/336 [02:33<00:23,  1.87batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 294/336 [02:33<00:20,  2.00batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 295/336 [02:34<00:21,  1.95batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  88%|████████▊ | 295/336 [02:35<00:21,  1.95batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  88%|████████▊ | 296/336 [02:35<00:21,  1.89batch/s, accuracy=55.3, loss=0.691]\n",
      "Epoch 1:  88%|████████▊ | 297/336 [02:35<00:19,  1.96batch/s, accuracy=55.3, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  89%|████████▊ | 298/336 [02:36<00:19,  1.91batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 299/336 [02:36<00:20,  1.84batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  89%|████████▉ | 300/336 [02:37<00:18,  1.98batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  90%|████████▉ | 302/336 [02:38<00:17,  1.99batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  90%|████████▉ | 302/336 [02:38<00:17,  1.99batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  90%|█████████ | 303/336 [02:38<00:18,  1.82batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  90%|█████████ | 304/336 [02:39<00:18,  1.70batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  91%|█████████ | 305/336 [02:40<00:17,  1.76batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  91%|█████████ | 306/336 [02:40<00:15,  1.92batch/s, accuracy=55.1, loss=0.691]\n",
      "Epoch 1:  91%|█████████ | 306/336 [02:40<00:15,  1.92batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  91%|█████████▏| 307/336 [02:40<00:14,  1.96batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  91%|█████████▏| 307/336 [02:41<00:14,  1.96batch/s, accuracy=55.1, loss=0.691]\n",
      "Epoch 1:  92%|█████████▏| 308/336 [02:41<00:14,  1.89batch/s, accuracy=55.1, loss=0.691]\n",
      "Epoch 1:  92%|█████████▏| 308/336 [02:41<00:14,  1.89batch/s, accuracy=55.1, loss=0.691]\n",
      "Epoch 1:  92%|█████████▏| 309/336 [02:42<00:14,  1.88batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 310/336 [02:42<00:14,  1.78batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 311/336 [02:43<00:13,  1.79batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 312/336 [02:43<00:14,  1.70batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 313/336 [02:44<00:15,  1.51batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  93%|█████████▎| 314/336 [02:45<00:15,  1.38batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 315/336 [02:46<00:14,  1.47batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 316/336 [02:46<00:11,  1.69batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  94%|█████████▍| 317/336 [02:47<00:11,  1.72batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 318/336 [02:47<00:10,  1.77batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  95%|█████████▍| 319/336 [02:47<00:08,  1.93batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  95%|█████████▍| 319/336 [02:48<00:08,  1.93batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  95%|█████████▌| 320/336 [02:48<00:08,  1.88batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  95%|█████████▌| 320/336 [02:48<00:08,  1.88batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 321/336 [02:49<00:08,  1.83batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 322/336 [02:49<00:07,  1.84batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  96%|█████████▌| 323/336 [02:50<00:06,  2.05batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  96%|█████████▋| 324/336 [02:50<00:06,  1.78batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 325/336 [02:51<00:05,  1.96batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 326/336 [02:51<00:05,  1.88batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  97%|█████████▋| 327/336 [02:52<00:04,  2.08batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 327/336 [02:52<00:04,  2.08batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  98%|█████████▊| 328/336 [02:52<00:04,  1.97batch/s, accuracy=55.2, loss=0.691]\n",
      "Epoch 1:  98%|█████████▊| 328/336 [02:52<00:04,  1.97batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 329/336 [02:53<00:03,  1.91batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  98%|█████████▊| 330/336 [02:53<00:03,  1.90batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  99%|█████████▊| 331/336 [02:54<00:02,  2.03batch/s, accuracy=55.2, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 333/336 [02:55<00:01,  2.08batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 334/336 [02:55<00:00,  2.03batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 335/336 [02:56<00:00,  2.18batch/s, accuracy=55.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1: 100%|██████████| 336/336 [02:56<00:00,  1.90batch/s, accuracy=55.1, loss=0.691]\n",
      "Evaluation:   2%|▏         | 2/112 [00:00<00:31,  3.44batch/s, accuracy=34.4, loss=0.702]\n",
      "\u001b[A\n",
      "Evaluation:   3%|▎         | 3/112 [00:00<00:34,  3.13batch/s, accuracy=37.5, loss=0.701]\n",
      "Evaluation:   4%|▎         | 4/112 [00:01<00:36,  2.95batch/s, accuracy=40.6, loss=0.698]\n",
      "Evaluation:   4%|▎         | 4/112 [00:01<00:36,  2.95batch/s, accuracy=40, loss=0.698]  \n",
      "Evaluation:   4%|▍         | 5/112 [00:01<00:35,  3.03batch/s, accuracy=40, loss=0.698]\n",
      "Evaluation:   6%|▋         | 7/112 [00:02<00:31,  3.31batch/s, accuracy=45.5, loss=0.695]\n",
      "Evaluation:   6%|▋         | 7/112 [00:02<00:31,  3.31batch/s, accuracy=47.7, loss=0.694]\n",
      "Evaluation:   7%|▋         | 8/112 [00:02<00:32,  3.24batch/s, accuracy=47.7, loss=0.694]\n",
      "Evaluation:   9%|▉         | 10/112 [00:03<00:28,  3.53batch/s, accuracy=45.6, loss=0.695]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation:  12%|█▏        | 13/112 [00:04<00:28,  3.43batch/s, accuracy=49.5, loss=0.693]\n",
      "\u001b[A\n",
      "Evaluation:  12%|█▎        | 14/112 [00:04<00:30,  3.17batch/s, accuracy=49.6, loss=0.692]\n",
      "Evaluation:  14%|█▍        | 16/112 [00:04<00:26,  3.65batch/s, accuracy=52.3, loss=0.691]\n",
      "\u001b[A\n",
      "Evaluation:  15%|█▌        | 17/112 [00:05<00:28,  3.34batch/s, accuracy=51.1, loss=0.692]\n",
      "Evaluation:  16%|█▌        | 18/112 [00:05<00:28,  3.25batch/s, accuracy=51, loss=0.692]  \n",
      "\u001b[A\n",
      "Evaluation:  17%|█▋        | 19/112 [00:05<00:30,  3.10batch/s, accuracy=52, loss=0.692]\n",
      "Evaluation:  18%|█▊        | 20/112 [00:06<00:26,  3.45batch/s, accuracy=51.9, loss=0.692]\n",
      "Evaluation:  18%|█▊        | 20/112 [00:06<00:26,  3.45batch/s, accuracy=52.7, loss=0.691]\n",
      "Evaluation:  19%|█▉        | 21/112 [00:06<00:31,  2.88batch/s, accuracy=52.7, loss=0.691]\n",
      "Evaluation:  21%|██        | 23/112 [00:07<00:29,  3.00batch/s, accuracy=54.1, loss=0.691]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation:  22%|██▏       | 25/112 [00:08<00:30,  2.90batch/s, accuracy=54, loss=0.69]   \n",
      "\u001b[A\n",
      "Evaluation:  23%|██▎       | 26/112 [00:08<00:30,  2.79batch/s, accuracy=54.1, loss=0.69]\n",
      "Evaluation:  25%|██▌       | 28/112 [00:08<00:26,  3.12batch/s, accuracy=54.5, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  26%|██▌       | 29/112 [00:09<00:26,  3.10batch/s, accuracy=54.3, loss=0.69]\n",
      "Evaluation:  28%|██▊       | 31/112 [00:09<00:25,  3.19batch/s, accuracy=54.6, loss=0.69]\n",
      "Evaluation:  28%|██▊       | 31/112 [00:10<00:25,  3.19batch/s, accuracy=55.5, loss=0.69]\n",
      "Evaluation:  29%|██▊       | 32/112 [00:10<00:25,  3.19batch/s, accuracy=55.5, loss=0.69]\n",
      "Evaluation:  30%|███       | 34/112 [00:10<00:23,  3.29batch/s, accuracy=55.7, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  31%|███▏      | 35/112 [00:11<00:24,  3.12batch/s, accuracy=55.9, loss=0.69]\n",
      "Evaluation:  33%|███▎      | 37/112 [00:11<00:21,  3.44batch/s, accuracy=55.4, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  34%|███▍      | 38/112 [00:11<00:23,  3.16batch/s, accuracy=56.1, loss=0.69]\n",
      "Evaluation:  36%|███▌      | 40/112 [00:12<00:21,  3.39batch/s, accuracy=55.5, loss=0.69]\n",
      "Evaluation:  36%|███▌      | 40/112 [00:12<00:21,  3.39batch/s, accuracy=55.2, loss=0.69]\n",
      "Evaluation:  37%|███▋      | 41/112 [00:12<00:21,  3.31batch/s, accuracy=55.2, loss=0.69]\n",
      "Evaluation:  38%|███▊      | 43/112 [00:13<00:20,  3.33batch/s, accuracy=55.7, loss=0.69]\n",
      "Evaluation:  38%|███▊      | 43/112 [00:13<00:20,  3.33batch/s, accuracy=56.2, loss=0.69]\n",
      "Evaluation:  39%|███▉      | 44/112 [00:13<00:20,  3.24batch/s, accuracy=56.2, loss=0.69]\n",
      "Evaluation:  41%|████      | 46/112 [00:14<00:18,  3.50batch/s, accuracy=56.5, loss=0.689]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation:  43%|████▎     | 48/112 [00:14<00:19,  3.29batch/s, accuracy=56.8, loss=0.689]\n",
      "Evaluation:  43%|████▎     | 48/112 [00:15<00:19,  3.29batch/s, accuracy=56.9, loss=0.689]\n",
      "Evaluation:  44%|████▍     | 49/112 [00:15<00:20,  3.15batch/s, accuracy=56.9, loss=0.689]\n",
      "Evaluation:  46%|████▌     | 51/112 [00:15<00:18,  3.25batch/s, accuracy=57.2, loss=0.689]\n",
      "Evaluation:  46%|████▌     | 51/112 [00:16<00:18,  3.25batch/s, accuracy=56.9, loss=0.689]\n",
      "Evaluation:  46%|████▋     | 52/112 [00:16<00:18,  3.17batch/s, accuracy=57.1, loss=0.689]\n",
      "Evaluation:  48%|████▊     | 54/112 [00:16<00:18,  3.07batch/s, accuracy=56.8, loss=0.689]\n",
      "Evaluation:  48%|████▊     | 54/112 [00:17<00:18,  3.07batch/s, accuracy=56.7, loss=0.689]\n",
      "Evaluation:  49%|████▉     | 55/112 [00:17<00:19,  2.98batch/s, accuracy=56.7, loss=0.689]\n",
      "Evaluation:  51%|█████     | 57/112 [00:17<00:17,  3.15batch/s, accuracy=56.6, loss=0.689]\n",
      "\u001b[A\n",
      "Evaluation:  52%|█████▏    | 58/112 [00:18<00:17,  3.04batch/s, accuracy=56.8, loss=0.689]\n",
      "Evaluation:  54%|█████▎    | 60/112 [00:18<00:15,  3.41batch/s, accuracy=56.8, loss=0.689]\n",
      "Evaluation:  54%|█████▎    | 60/112 [00:18<00:15,  3.41batch/s, accuracy=56.8, loss=0.689]\n",
      "\u001b[A\n",
      "Evaluation:  56%|█████▋    | 63/112 [00:19<00:16,  3.00batch/s, accuracy=56.7, loss=0.689]\n",
      "\u001b[A\n",
      "Evaluation:  56%|█████▋    | 63/112 [00:20<00:16,  3.00batch/s, accuracy=56.6, loss=0.69] \n",
      "Evaluation:  59%|█████▉    | 66/112 [00:20<00:14,  3.09batch/s, accuracy=56.9, loss=0.689]\n",
      "\u001b[A\n",
      "Evaluation:  60%|█████▉    | 67/112 [00:21<00:14,  3.05batch/s, accuracy=56.9, loss=0.689]\n",
      "Evaluation:  61%|██████    | 68/112 [00:21<00:15,  2.88batch/s, accuracy=56.4, loss=0.69] \n",
      "Evaluation:  61%|██████    | 68/112 [00:21<00:15,  2.88batch/s, accuracy=56.4, loss=0.69]\n",
      "Evaluation:  62%|██████▏   | 69/112 [00:21<00:14,  2.96batch/s, accuracy=56.4, loss=0.69]\n",
      "Evaluation:  63%|██████▎   | 71/112 [00:22<00:13,  3.03batch/s, accuracy=56.3, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  64%|██████▍   | 72/112 [00:22<00:14,  2.73batch/s, accuracy=56.2, loss=0.69]\n",
      "Evaluation:  66%|██████▌   | 74/112 [00:23<00:12,  3.02batch/s, accuracy=56.2, loss=0.69]\n",
      "Evaluation:  66%|██████▌   | 74/112 [00:23<00:12,  3.02batch/s, accuracy=56, loss=0.69]  \n",
      "Evaluation:  67%|██████▋   | 75/112 [00:23<00:12,  2.87batch/s, accuracy=56, loss=0.69]\n",
      "Evaluation:  69%|██████▉   | 77/112 [00:24<00:11,  3.17batch/s, accuracy=56.2, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  70%|██████▉   | 78/112 [00:24<00:11,  2.99batch/s, accuracy=56.1, loss=0.69]\n",
      "Evaluation:  71%|███████▏  | 80/112 [00:25<00:09,  3.32batch/s, accuracy=56.1, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  72%|███████▏  | 81/112 [00:25<00:10,  3.05batch/s, accuracy=56.1, loss=0.69]\n",
      "Evaluation:  74%|███████▍  | 83/112 [00:26<00:08,  3.26batch/s, accuracy=56.1, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  75%|███████▌  | 84/112 [00:26<00:08,  3.16batch/s, accuracy=55.9, loss=0.69]\n",
      "Evaluation:  77%|███████▋  | 86/112 [00:27<00:07,  3.48batch/s, accuracy=56, loss=0.69]  \n",
      "\u001b[A\n",
      "Evaluation:  77%|███████▋  | 86/112 [00:27<00:07,  3.48batch/s, accuracy=56, loss=0.69]\n",
      "Evaluation:  79%|███████▊  | 88/112 [00:27<00:07,  3.03batch/s, accuracy=55.9, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  79%|███████▉  | 89/112 [00:28<00:07,  3.06batch/s, accuracy=56, loss=0.69]  \n",
      "Evaluation:  81%|████████▏ | 91/112 [00:28<00:06,  3.39batch/s, accuracy=55.8, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  81%|████████▏ | 91/112 [00:29<00:06,  3.39batch/s, accuracy=55.8, loss=0.69]\n",
      "Evaluation:  83%|████████▎ | 93/112 [00:29<00:06,  3.14batch/s, accuracy=55.9, loss=0.69]\n",
      "Evaluation:  84%|████████▍ | 94/112 [00:29<00:05,  3.19batch/s, accuracy=55.9, loss=0.69]\n",
      "Evaluation:  84%|████████▍ | 94/112 [00:29<00:05,  3.19batch/s, accuracy=55.9, loss=0.69]\n",
      "Evaluation:  86%|████████▌ | 96/112 [00:30<00:04,  3.28batch/s, accuracy=55.8, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  87%|████████▋ | 97/112 [00:30<00:04,  3.10batch/s, accuracy=55.9, loss=0.69]\n",
      "Evaluation:  88%|████████▊ | 98/112 [00:31<00:04,  2.93batch/s, accuracy=55.6, loss=0.69]\n",
      "Evaluation:  88%|████████▊ | 99/112 [00:31<00:04,  2.77batch/s, accuracy=55.6, loss=0.69]\n",
      "Evaluation:  88%|████████▊ | 99/112 [00:31<00:04,  2.77batch/s, accuracy=55.6, loss=0.69]\n",
      "Evaluation:  90%|█████████ | 101/112 [00:32<00:03,  2.99batch/s, accuracy=55.8, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  91%|█████████ | 102/112 [00:32<00:03,  2.94batch/s, accuracy=55.8, loss=0.69]\n",
      "Evaluation:  93%|█████████▎| 104/112 [00:33<00:02,  3.25batch/s, accuracy=55.6, loss=0.69]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluation:  96%|█████████▌| 107/112 [00:33<00:01,  3.69batch/s, accuracy=55.3, loss=0.69]\n",
      "Evaluation:  96%|█████████▌| 107/112 [00:34<00:01,  3.69batch/s, accuracy=55.3, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  98%|█████████▊| 110/112 [00:34<00:00,  3.81batch/s, accuracy=55.1, loss=0.69]\n",
      "\u001b[A\n",
      "Evaluation:  98%|█████████▊| 110/112 [00:34<00:00,  3.81batch/s, accuracy=55.1, loss=0.69]\n",
      "Evaluation: 100%|██████████| 112/112 [00:35<00:00,  3.16batch/s, accuracy=55.1, loss=0.69]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "Epoch 1:  97%|█████████▋| 325/336 [03:32<00:08,  1.24batch/s, accuracy=49.4, loss=0.694]\u001b[33m[W 2023-03-08 22:53:53,337]\u001b[0m Trial 0 failed because of the following error: TypeError('Object of type float32 is not JSON serializable')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\huuta\\AppData\\Local\\Temp\\ipykernel_4388\\1987068146.py\", line 46, in objective\n",
      "    _, _, eval_f1s = train(model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\", line 173, in train\n",
      "    eval_loss, eval_accu, eval_f1 = evaluate(\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\", line 89, in evaluate\n",
      "    save_results(output_proba, preds, labels, eval_loss, name, epoch)\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\utils.py\", line 26, in save_results\n",
      "    json.dump({\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 405, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 325, in _iterencode_list\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 438, in _iterencode\n",
      "    o = _default(o)\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type float32 is not JSON serializable\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Epoch 1: 100%|██████████| 336/336 [03:36<00:00,  1.55batch/s, accuracy=54.7, loss=0.687]\n",
      "Evaluation:   0%|          | 0/112 [00:00<?, ?batch/s]\n",
      "Evaluation:   0%|          | 0/112 [00:00<?, ?batch/s, accuracy=81.2, loss=0.635]\n",
      "Evaluation:   1%|          | 1/112 [00:00<00:41,  2.67batch/s, accuracy=81.2, loss=0.635]\n",
      "Evaluation:   3%|▎         | 3/112 [00:00<00:23,  4.56batch/s, accuracy=56.2, loss=0.677]\n",
      "\u001b[A\n",
      "Epoch 1: 100%|██████████| 336/336 [03:37<00:00,  1.55batch/s, accuracy=49.5, loss=0.694]]\n",
      "Evaluation: 100%|██████████| 112/112 [00:35<00:00,  3.13batch/s, accuracy=55.1, loss=0.684]\n",
      "\n",
      "\u001b[33m[W 2023-03-08 22:54:33,533]\u001b[0m Trial 1 failed because of the following error: TypeError('Object of type float32 is not JSON serializable')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\huuta\\AppData\\Local\\Temp\\ipykernel_4388\\1987068146.py\", line 46, in objective\n",
      "    _, _, eval_f1s = train(model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\", line 173, in train\n",
      "    eval_loss, eval_accu, eval_f1 = evaluate(\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\", line 89, in evaluate\n",
      "    save_results(output_proba, preds, labels, eval_loss, name, epoch)\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\utils.py\", line 26, in save_results\n",
      "    json.dump({\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 405, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 325, in _iterencode_list\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 438, in _iterencode\n",
      "    o = _default(o)\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "Evaluation: 100%|██████████| 112/112 [00:39<00:00,  2.82batch/s, accuracy=46.6, loss=0.693]\n",
      "\u001b[33m[W 2023-03-08 22:54:38,460]\u001b[0m Trial 2 failed because of the following error: TypeError('Object of type float32 is not JSON serializable')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\huuta\\AppData\\Local\\Temp\\ipykernel_4388\\1987068146.py\", line 46, in objective\n",
      "    _, _, eval_f1s = train(model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\", line 173, in train\n",
      "    eval_loss, eval_accu, eval_f1 = evaluate(\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\", line 89, in evaluate\n",
      "    save_results(output_proba, preds, labels, eval_loss, name, epoch)\n",
      "  File \"c:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\utils.py\", line 26, in save_results\n",
      "    json.dump({\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 431, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 405, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 325, in _iterencode_list\n",
      "    yield from chunks\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 438, in _iterencode\n",
      "    o = _default(o)\n",
      "  File \"c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type float32 is not JSON serializable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type float32 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m       \u001b[39mreturn\u001b[39;00m eval_f1s[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     50\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 51\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m _optimize(\n\u001b[0;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    410\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py:106\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[39m# Raise if exception occurred in executing the completed futures.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m                     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m completed:\n\u001b[1;32m--> 106\u001b[0m                         f\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    108\u001b[0m                 futures\u001b[39m.\u001b[39madd(\n\u001b[0;32m    109\u001b[0m                     executor\u001b[39m.\u001b[39msubmit(\n\u001b[0;32m    110\u001b[0m                         _optimize_sequential,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m                     )\n\u001b[0;32m    122\u001b[0m                 )\n\u001b[0;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\concurrent\\futures\\_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py:264\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch):\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    265\u001b[0m \u001b[39mreturn\u001b[39;00m trial\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[24], line 46\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     34\u001b[0m _, val_loader, _, _ \u001b[39m=\u001b[39m get_data_loader(\n\u001b[0;32m     35\u001b[0m returns_val, ecb, fed, y_val, method\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     36\u001b[0m separate\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mseparate\u001b[39m\u001b[39m\"\u001b[39m], max_corpus_len\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mmax_corpus_len\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     37\u001b[0m batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m _, _, _, _ \u001b[39m=\u001b[39m get_data_loader(\n\u001b[0;32m     41\u001b[0m returns_test, ecb, fed, y_test, method\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     42\u001b[0m separate\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mseparate\u001b[39m\u001b[39m\"\u001b[39m], max_corpus_len\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mmax_corpus_len\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     43\u001b[0m batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     44\u001b[0m )\n\u001b[1;32m---> 46\u001b[0m _, _, eval_f1s \u001b[39m=\u001b[39m train(model, train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader,config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     47\u001b[0m       device\u001b[39m=\u001b[39;49mdevice, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, eval_every\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mno_nlp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m eval_f1s[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py:173\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, config, device, max_epochs, eval_every, name, train_loss_history, starting_epoch)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39m# Evaluation\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m eval_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 173\u001b[0m     eval_loss, eval_accu, eval_f1 \u001b[39m=\u001b[39m evaluate(\n\u001b[0;32m    174\u001b[0m         model, val_loader, config, device, name, epoch)\n\u001b[0;32m    175\u001b[0m     eval_losses\u001b[39m.\u001b[39mappend(eval_loss)\n\u001b[0;32m    176\u001b[0m     eval_accus\u001b[39m.\u001b[39mappend(eval_accu)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py:89\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, val_loader, config, device, name, epoch)\u001b[0m\n\u001b[0;32m     87\u001b[0m eval_loss \u001b[39m=\u001b[39m log_loss(labels, output_proba)\n\u001b[0;32m     88\u001b[0m eval_f1 \u001b[39m=\u001b[39m f1_score(labels, preds)\n\u001b[1;32m---> 89\u001b[0m save_results(output_proba, preds, labels, eval_loss, name, epoch)\n\u001b[0;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m eval_loss, eval_accu, eval_f1\n",
      "File \u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\utils.py:26\u001b[0m, in \u001b[0;36msave_results\u001b[1;34m(outputs_proba, outputs, targets, logloss, name, epoch)\u001b[0m\n\u001b[0;32m     24\u001b[0m PATH_MODEL_EPOCH\u001b[39m.\u001b[39mmkdir(exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(PATH_MODEL_EPOCH \u001b[39m/\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel_\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m_results.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 26\u001b[0m     json\u001b[39m.\u001b[39;49mdump({\n\u001b[0;32m     27\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mstr\u001b[39;49m(name),\n\u001b[0;32m     28\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39moutputs_proba\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mlist\u001b[39;49m(outputs_proba),\n\u001b[0;32m     29\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39moutputs\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mlist\u001b[39;49m(outputs),\n\u001b[0;32m     30\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mtargets\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mlist\u001b[39;49m(targets),\n\u001b[0;32m     31\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mlogloss\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mfloat\u001b[39;49m(logloss),\n\u001b[0;32m     32\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m: epoch\n\u001b[0;32m     33\u001b[0m     }, f)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[39m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m    180\u001b[0m     fp\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 325\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[1;32m--> 438\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[0;32m    439\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type float32 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "      config ={\n",
    "\n",
    "                \"method\": None,\n",
    "\n",
    "                \"learning_rate\": 10**trial.suggest_float(\"lr_exp\", -6, -2),\n",
    "\n",
    "                \"weight_decay\": 10**trial.suggest_float(\"weight_decay_exp\", -6, -3),\n",
    "\n",
    "                \"batch_size\": 2**trial.suggest_int(\"batch_size_exp\", 4, 5),\n",
    "\n",
    "                \"layers\": trial.suggest_int(\"layers\", 1, 8),\n",
    "\n",
    "                \"mlp_hidden_dim\": 64,\n",
    "\n",
    "                \"separate\": False,\n",
    "\n",
    "                \"max_corpus_len\": 1,\n",
    "\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.2, 0.7),\n",
    "\n",
    "            }\n",
    "      model = MyModel(\n",
    "            nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "            separate=False, dropout=config[\"dropout\"]\n",
    "            ).to(device)\n",
    "\n",
    "      _, train_loader, _, _ = get_data_loader(\n",
    "      returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "      separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "      batch_size=config[\"batch_size\"]\n",
    "      )\n",
    "\n",
    "      _, val_loader, _, _ = get_data_loader(\n",
    "      returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "      separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "      batch_size=config[\"batch_size\"]\n",
    "      )\n",
    "\n",
    "      _, _, _, _ = get_data_loader(\n",
    "      returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "      separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "      batch_size=config[\"batch_size\"]\n",
    "      )\n",
    "\n",
    "      _, _, eval_f1s = train(model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
    "            device=device, max_epochs=1, eval_every=1, name=f\"no_nlp_{config['learning_rate']}\")\n",
    "      return eval_f1s[-1]\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50, n_jobs=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4c350da27618d5732fc58ebcab8d2c0381c51b7361f332741f21e30512bbdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
