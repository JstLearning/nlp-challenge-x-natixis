{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15728\\3011144690.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mecb_pipeline_en\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfast_detect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\preprocessing\\preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# from preprocessing.preprocessing import ecb_pipeline_en, fast_detect\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"data/train_series.csv\"\n",
    "FILENAME_ECB = \"data/ecb_data_preprocessed.csv\"\n",
    "FILENAME_FED = \"data/fed_data_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv(FILENAME, index_col=0)\n",
    "ecb = pd.read_csv(FILENAME_ECB, index_col=0)\n",
    "fed = pd.read_csv(FILENAME_FED, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.get_dummies(returns, columns=[\"Index Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[\"Sign\"] = (returns[\"Index + 1\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>-0.027519</td>\n",
       "      <td>-0.103565</td>\n",
       "      <td>-0.045086</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.054050</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.008436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-0.011304</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004980</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.001083</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.001623</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.006444</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "0   0.001045   0.005841   0.003832  -0.027519  -0.103565  -0.045086   \n",
       "1  -0.021497   0.007891  -0.013175  -0.008436   0.000000   0.026303   \n",
       "2  -0.001872  -0.008154   0.023588   0.004086   0.003493   0.003300   \n",
       "3   0.004980  -0.000864   0.001677   0.000000   0.006030  -0.001083   \n",
       "4   0.000360  -0.001893   0.005579  -0.003056  -0.001171  -0.001623   \n",
       "\n",
       "   Index - 3  Index - 2  Index - 1  Index - 0  ... Index Name_CVIX Index  \\\n",
       "0  -0.011265   0.005164   0.054050   0.015779  ...                     0   \n",
       "1   0.000556   0.001455   0.007422   0.000000  ...                     0   \n",
       "2   0.000885  -0.011304   0.005040   0.000156  ...                     0   \n",
       "3   0.000419   0.001492   0.001018  -0.002582  ...                     0   \n",
       "4  -0.002350  -0.006444  -0.000729  -0.000365  ...                     0   \n",
       "\n",
       "  Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "0                        0                            0   \n",
       "1                        0                            0   \n",
       "2                        0                            0   \n",
       "3                        0                            0   \n",
       "4                        1                            0   \n",
       "\n",
       "   Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "0                      0                     0                       0   \n",
       "1                      1                     0                       0   \n",
       "2                      0                     1                       0   \n",
       "3                      0                     1                       0   \n",
       "4                      0                     0                       0   \n",
       "\n",
       "   Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  Sign  \n",
       "0                      0                     1                     0     1  \n",
       "1                      0                     0                     0     1  \n",
       "2                      0                     0                     0     1  \n",
       "3                      0                     0                     0     1  \n",
       "4                      0                     0                     0     1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = returns[\"Sign\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4930\n",
       "1    4016\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns.drop([\"Sign\", \"Index + 1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index - 9', 'Index - 8', 'Index - 7', 'Index - 6', 'Index - 5',\n",
       "       'Index - 4', 'Index - 3', 'Index - 2', 'Index - 1', 'Index - 0',\n",
       "       'index ecb', 'index fed', 'Index Name_CVIX Index',\n",
       "       'Index Name_EURUSD Curncy', 'Index Name_EURUSDV1M Curncy',\n",
       "       'Index Name_MOVE Index', 'Index Name_SPX Index',\n",
       "       'Index Name_SRVIX Index', 'Index Name_SX5E Index',\n",
       "       'Index Name_V2X Index', 'Index Name_VIX Index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontextual_cols = ['Index - 9',\n",
    " 'Index - 8',\n",
    " 'Index - 7',\n",
    " 'Index - 6',\n",
    " 'Index - 5',\n",
    " 'Index - 4',\n",
    " 'Index - 3',\n",
    " 'Index - 2',\n",
    " 'Index - 1',\n",
    " 'Index - 0',\n",
    " 'Index Name_CVIX Index',\n",
    " 'Index Name_EURUSD Curncy',\n",
    " 'Index Name_EURUSDV1M Curncy',\n",
    " 'Index Name_MOVE Index',\n",
    " 'Index Name_SPX Index',\n",
    " 'Index Name_SRVIX Index',\n",
    " 'Index Name_SX5E Index',\n",
    " 'Index Name_V2X Index',\n",
    " 'Index Name_VIX Index']\n",
    "nb_nontextfeatures = len(nontextual_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% val, 20% test\n",
    "\n",
    "returns_, returns_test, y_, y_test = train_test_split(\n",
    "    returns, y, test_size=0.2, train_size=0.8,\n",
    "    random_state=0, stratify=y\n",
    "    )\n",
    "\n",
    "returns_train, returns_val, y_train, y_val = train_test_split(\n",
    "    returns_, y_, test_size=0.25, train_size=0.75,\n",
    "    random_state=42, stratify=y_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del returns, y\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comments by Yves Mersch at Financial Services ...</td>\n",
       "      <td>Yves Mersch</td>\n",
       "      <td>Comments by Yves Mersch at Financial Service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Securing sustained economic growth in the euro...</td>\n",
       "      <td>Vítor Constâncio</td>\n",
       "      <td>Securing sustained economic growth in the eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The role of monetary policy in addressing the ...</td>\n",
       "      <td>Mario Draghi</td>\n",
       "      <td>The role of monetary policy in addressing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pandemic emergency: the three challenges f...</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>SPEECH  The pandemic emergency: the three c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transmission channels of monetary policy in th...</td>\n",
       "      <td>Peter Praet</td>\n",
       "      <td>Transmission channels of monetary policy in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           speaker  \\\n",
       "0  Comments by Yves Mersch at Financial Services ...       Yves Mersch   \n",
       "1  Securing sustained economic growth in the euro...  Vítor Constâncio   \n",
       "2  The role of monetary policy in addressing the ...      Mario Draghi   \n",
       "3  The pandemic emergency: the three challenges f...    Philip R. Lane   \n",
       "4  Transmission channels of monetary policy in th...       Peter Praet   \n",
       "\n",
       "                                                text  \n",
       "0    Comments by Yves Mersch at Financial Service...  \n",
       "1    Securing sustained economic growth in the eu...  \n",
       "2    The role of monetary policy in addressing th...  \n",
       "3     SPEECH  The pandemic emergency: the three c...  \n",
       "4    Transmission channels of monetary policy in ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Importance of Economic Education and Finan...</td>\n",
       "      <td>Governor Frederic S. Mishkin</td>\n",
       "      <td>As ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial Innovation and Consumer Protection</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>The concept of financial innovation, it seems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implementing Basel II in the United States</td>\n",
       "      <td>Governor Randall S. Kroszner</td>\n",
       "      <td>Good afternoon. I would like to thank Standar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Assessment of the U.S. Economy</td>\n",
       "      <td>Vice Chair for Supervision Randal K. Quarles</td>\n",
       "      <td>Thank you for the opportunity to take part in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monetary Policy since the Onset of the Crisis</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>When we convened in Jackson Hole in August 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The Importance of Economic Education and Finan...   \n",
       "1       Financial Innovation and Consumer Protection   \n",
       "2         Implementing Basel II in the United States   \n",
       "3                  An Assessment of the U.S. Economy   \n",
       "4      Monetary Policy since the Onset of the Crisis   \n",
       "\n",
       "                                        speaker  \\\n",
       "0                  Governor Frederic S. Mishkin   \n",
       "1                      Chairman Ben S. Bernanke   \n",
       "2                  Governor Randall S. Kroszner   \n",
       "3  Vice Chair for Supervision Randal K. Quarles   \n",
       "4                      Chairman Ben S. Bernanke   \n",
       "\n",
       "                                                text  \n",
       "0                                             As ...  \n",
       "1   The concept of financial innovation, it seems...  \n",
       "2   Good afternoon. I would like to thank Standar...  \n",
       "3   Thank you for the opportunity to take part in...  \n",
       "4   When we convened in Jackson Hole in August 20...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"text_\"] = ecb.apply(ecb_pipeline_en, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"text\"].fillna(\"\", inplace=True)\n",
    "ecb[\"speaker\"].fillna(\"Unknown\", inplace=True)\n",
    "fed[\"speaker\"].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                         Auf neuen Wegen zum alten Ziel\n",
       "speaker                                          Yves Mersch\n",
       "text         Auf neuen Wegen zum alten Ziel   Rede von Yv...\n",
       "text_      Rede von Yves Mersch, Mitglied des Direktorium...\n",
       "Name: 151, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text in french\n",
    "ecb.loc[138]\n",
    "# Text in german\n",
    "ecb.loc[151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"lang\"] = ecb[\"text_\"].apply(fast_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comments by Yves Mersch at Financial Services ...</td>\n",
       "      <td>Yves Mersch</td>\n",
       "      <td>Comments by Yves Mersch at Financial Service...</td>\n",
       "      <td>Sustainable economic growth in the real econom...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Securing sustained economic growth in the euro...</td>\n",
       "      <td>Vítor Constâncio</td>\n",
       "      <td>Securing sustained economic growth in the eu...</td>\n",
       "      <td>Ladies and Gentlemen, Thank you for inviting m...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The role of monetary policy in addressing the ...</td>\n",
       "      <td>Mario Draghi</td>\n",
       "      <td>The role of monetary policy in addressing th...</td>\n",
       "      <td>There was a time, not too long ago, when centr...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pandemic emergency: the three challenges f...</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>SPEECH  The pandemic emergency: the three c...</td>\n",
       "      <td>Today, I will discuss the monetary policy meas...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transmission channels of monetary policy in th...</td>\n",
       "      <td>Peter Praet</td>\n",
       "      <td>Transmission channels of monetary policy in ...</td>\n",
       "      <td>Ladies and Gentlemen, Since the onset of the f...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           speaker  \\\n",
       "0  Comments by Yves Mersch at Financial Services ...       Yves Mersch   \n",
       "1  Securing sustained economic growth in the euro...  Vítor Constâncio   \n",
       "2  The role of monetary policy in addressing the ...      Mario Draghi   \n",
       "3  The pandemic emergency: the three challenges f...    Philip R. Lane   \n",
       "4  Transmission channels of monetary policy in th...       Peter Praet   \n",
       "\n",
       "                                                text  \\\n",
       "0    Comments by Yves Mersch at Financial Service...   \n",
       "1    Securing sustained economic growth in the eu...   \n",
       "2    The role of monetary policy in addressing th...   \n",
       "3     SPEECH  The pandemic emergency: the three c...   \n",
       "4    Transmission channels of monetary policy in ...   \n",
       "\n",
       "                                               text_ lang  \n",
       "0  Sustainable economic growth in the real econom...   en  \n",
       "1  Ladies and Gentlemen, Thank you for inviting m...   en  \n",
       "2  There was a time, not too long ago, when centr...   en  \n",
       "3  Today, I will discuss the monetary policy meas...   en  \n",
       "4  Ladies and Gentlemen, Since the onset of the f...   en  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed[\"lang\"] = fed[\"text\"].apply(fast_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Importance of Economic Education and Finan...</td>\n",
       "      <td>Governor Frederic S. Mishkin</td>\n",
       "      <td>As ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial Innovation and Consumer Protection</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>The concept of financial innovation, it seems...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implementing Basel II in the United States</td>\n",
       "      <td>Governor Randall S. Kroszner</td>\n",
       "      <td>Good afternoon. I would like to thank Standar...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Assessment of the U.S. Economy</td>\n",
       "      <td>Vice Chair for Supervision Randal K. Quarles</td>\n",
       "      <td>Thank you for the opportunity to take part in...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monetary Policy since the Onset of the Crisis</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>When we convened in Jackson Hole in August 20...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The Importance of Economic Education and Finan...   \n",
       "1       Financial Innovation and Consumer Protection   \n",
       "2         Implementing Basel II in the United States   \n",
       "3                  An Assessment of the U.S. Economy   \n",
       "4      Monetary Policy since the Onset of the Crisis   \n",
       "\n",
       "                                        speaker  \\\n",
       "0                  Governor Frederic S. Mishkin   \n",
       "1                      Chairman Ben S. Bernanke   \n",
       "2                  Governor Randall S. Kroszner   \n",
       "3  Vice Chair for Supervision Randal K. Quarles   \n",
       "4                      Chairman Ben S. Bernanke   \n",
       "\n",
       "                                                text lang  \n",
       "0                                             As ...   en  \n",
       "1   The concept of financial innovation, it seems...   en  \n",
       "2   Good afternoon. I would like to thank Standar...   en  \n",
       "3   Thank you for the opportunity to take part in...   en  \n",
       "4   When we convened in Jackson Hole in August 20...   en  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_langs = ecb[\"lang\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    1646\n",
       "de      76\n",
       "fr      30\n",
       "es      16\n",
       "it       4\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb[\"lang\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_dataset import get_data_loader\n",
    "from model.framework_model import CorpusEncoder, ClassificationHead, MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_01\",\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.,\n",
    "\n",
    "    \"batch_size\": 2,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.3,\n",
    "\n",
    "    \"separate\": True,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ce = CorpusEncoder(method=config[\"method\"],\n",
    "                   separate=config[\"separate\"],\n",
    "                   dropout=config[\"dropout\"]).to(device)\n",
    "clf = ClassificationHead(\n",
    "    corpus_emb_dim=ce.corpus_emb_dim, nontext_dim=nb_nontextfeatures,\n",
    "    layers=config[\"max_corpus_len\"], dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "my_model = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed output successfully. ce ouput = \n",
      " tensor([[ 0.1655,  0.5805,  0.1516,  0.1980,  0.1799, -0.0704,  0.0039,  0.2648,\n",
      "         -0.1464, -0.4822, -0.0637, -0.0381,  0.1207,  0.0595, -0.1349, -0.0389,\n",
      "         -0.4814, -0.4484, -0.2856,  0.5924, -0.0681,  0.0799, -0.0182,  0.1669,\n",
      "          0.0192, -0.2786, -0.0716, -0.0894,  0.2287, -0.2295, -0.1108,  0.1308,\n",
      "          0.0775, -0.4293, -0.0793,  0.3174,  0.0321,  0.2328,  0.2982, -0.0495,\n",
      "         -0.1027,  0.1390,  0.2921,  0.2360, -0.2802,  0.0855, -0.0006, -0.1196,\n",
      "          0.3278, -0.4202,  0.2727,  0.1209, -0.0837,  0.3232,  0.0220, -0.0669,\n",
      "          0.0854, -0.3017,  0.1084, -0.0993,  0.3060,  0.1382,  0.2276, -0.0841],\n",
      "        [ 0.2190,  0.6202,  0.0863,  0.1179,  0.2088,  0.0798, -0.0274,  0.3590,\n",
      "         -0.0182, -0.4043, -0.0105, -0.1185,  0.1783,  0.1159, -0.1525, -0.0539,\n",
      "         -0.5589, -0.4120, -0.4387,  0.4396, -0.0862, -0.0060, -0.0561,  0.2117,\n",
      "         -0.0965, -0.3359,  0.0290, -0.1410,  0.2277, -0.2775, -0.1915,  0.0551,\n",
      "          0.1653, -0.4295, -0.1160,  0.2208,  0.0043,  0.2019,  0.2048, -0.0903,\n",
      "         -0.1066,  0.2121,  0.1849,  0.3053, -0.1786,  0.0934, -0.0403, -0.1244,\n",
      "          0.3416, -0.4095,  0.3889,  0.0263, -0.0370,  0.3483,  0.1895, -0.0984,\n",
      "          0.0357, -0.3831,  0.0522, -0.1371,  0.2980,  0.0304,  0.2247, -0.0367]],\n",
      "       device='cuda:0')\n",
      "corpus encoder ouput shape =  torch.Size([2, 64]) \n",
      "corpus embed dim =  64\n",
      "torch.Size([2, 19])\n",
      "Classifier output =  \n",
      " tensor([0.4514, 0.4619], device='cuda:0')\n",
      "torch.Size([2, 2, 512])\n",
      "torch.Size([2, 2, 512])\n",
      "tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Test output\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ce.eval()\n",
    "    X_ecb = batch[\"X_ecb\"].to(device)\n",
    "    X_ecb_att = batch[\"X_ecb_mask\"].to(device)\n",
    "    X_fed = batch[\"X_fed\"].to(device)\n",
    "    X_fed_att = batch[\"X_fed_mask\"].to(device)\n",
    "    X_ind =  batch[\"X_ind\"].to(device)\n",
    "    y = batch[\"label\"]\n",
    "    X_text = (X_ecb, X_fed)\n",
    "    X_att = (X_ecb_att, X_fed_att)\n",
    "    ce_output = ce(X_text, X_att)\n",
    "    print(\"Computed output successfully. ce ouput = \\n\", ce_output)\n",
    "    print(\"corpus encoder ouput shape = \", ce_output.size(), \"\\ncorpus embed dim = \", ce.corpus_emb_dim)\n",
    "    print(X_ind.size())\n",
    "\n",
    "\n",
    "    clf_output = clf(ce_output, X_ind)\n",
    "    \n",
    "    print(\"Classifier output =  \\n\", clf_output)\n",
    "    my_model_output = my_model(X_text, X_att, X_ind)\n",
    "\n",
    "print(X_ecb.size())\n",
    "print(X_ecb_att.size())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": None,\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.01,\n",
    "\n",
    "    \"batch_size\": 16,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.5,\n",
    "\n",
    "    \"separate\": False,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ce = CorpusEncoder(method=config[\"method\"],\n",
    "                   separate=config[\"separate\"],\n",
    "                   dropout=config[\"dropout\"]).to(device)\n",
    "clf = ClassificationHead(\n",
    "    corpus_emb_dim=ce.corpus_emb_dim, nontext_dim=nb_nontextfeatures,\n",
    "    layers=config[\"max_corpus_len\"], dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "my_model = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huuta\\AppData\\Local\\Temp\\ipykernel_28708\\1290859485.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  z = torch.range(0, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.range(0, 15)\n",
    "print(z)\n",
    "z.view(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed output successfully. ce ouput = \n",
      " tensor([[ 0.0680, -0.4876, -0.3666, -0.1956,  0.1132,  0.0946,  0.1756, -0.0340,\n",
      "         -0.0052,  0.3330,  0.1639, -0.0179, -0.0024,  0.3258, -0.1272,  0.1548,\n",
      "          0.0966, -0.0413, -0.2932,  0.2296, -0.7422,  0.1561, -0.1577,  0.0229,\n",
      "          0.6316, -0.1441,  0.0673, -0.0629, -0.0406,  0.0709, -0.5268, -0.0249],\n",
      "        [ 0.0360, -0.5765, -0.3424, -0.1569,  0.1195,  0.1085,  0.2071, -0.0058,\n",
      "         -0.0093,  0.2382,  0.0986,  0.0924,  0.0303,  0.3314, -0.1299,  0.1669,\n",
      "          0.0670, -0.0543, -0.2332,  0.2621, -0.5938,  0.0526, -0.2652,  0.1079,\n",
      "          0.6401, -0.0916,  0.1027,  0.0037,  0.0228,  0.1233, -0.5413,  0.0193]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "corpus encoder ouput shape =  torch.Size([2, 32]) \n",
      "corpus embed dim =  32\n",
      "torch.Size([2, 19])\n",
      "Classifier output =  \n",
      " tensor([0.5068, 0.5185], device='cuda:0')\n",
      "tensor([[[  101,  1996, 23889,  ...,  4675,  7011,   102],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101, 21658,  3242,  ...,  3258,  1998,   102],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0]]], device='cuda:0')\n",
      "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Test output\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ce.eval()\n",
    "    X_text = (batch[\"X_text\"].to(device),)\n",
    "    X_mask = (batch[\"X_mask\"].to(device),)\n",
    "    X_ind =  batch[\"X_ind\"].to(device)\n",
    "    y = batch[\"label\"]\n",
    "    ce_output = ce(X_text, X_mask)\n",
    "    print(\"Computed output successfully. ce ouput = \\n\", ce_output)\n",
    "    print(\"corpus encoder ouput shape = \", ce_output.size(), \"\\ncorpus embed dim = \", ce.corpus_emb_dim)\n",
    "    print(X_ind.size())\n",
    "\n",
    "\n",
    "    clf_output = clf(ce_output, X_ind)\n",
    "    \n",
    "    print(\"Classifier output =  \\n\", clf_output)\n",
    "    my_model_output = my_model(X_text, X_mask, X_ind)\n",
    "\n",
    "print(X_ecb)\n",
    "print(X_ecb_att)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  12%|█▏        | 335/2684 [07:26<52:11,  1.33s/batch, accuracy=52.4, loss=0.69]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28708\\848911058.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train(my_model, train_loader=train_loader, val_loader=val_loader,config=config,\n\u001b[0m\u001b[0;32m      2\u001b[0m       device=device, max_epochs=1, eval_every=1, name=\"dummy\")\n",
      "\u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, config, device, max_epochs, eval_every, name)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtepoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[1;31m# tqdm desc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mtepoch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\model\\framework_dataset.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    220\u001b[0m     loader = DataLoader(\n\u001b[0;32m    221\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_corpus_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\Documents\\School\\3A\\nlp-challenge-x-natixis\\model\\framework_dataset.py\u001b[0m in \u001b[0;36mcollate_fn\u001b[1;34m(batch, separate, max_corpus_len)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             X_text_tokens = tokenizer(X_text, return_tensors=\"pt\",\n\u001b[0m\u001b[0;32m    206\u001b[0m                                       truncation=True, padding='max_length', max_length=512)\n\u001b[0;32m    207\u001b[0m             \u001b[1;31m# print(X_text_tokens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mencodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2607\u001b[0m                 )\n\u001b[0;32m   2608\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2609\u001b[1;33m             return self.batch_encode_plus(\n\u001b[0m\u001b[0;32m   2610\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2611\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2798\u001b[0m         )\n\u001b[0;32m   2799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m         return self._batch_encode_plus(\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    731\u001b[0m                 \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m             \u001b[0mfirst_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    734\u001b[0m             \u001b[0msecond_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpair_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m                 \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m                 \u001b[0mtokenized_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;31m# If the token is part of the never_split set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    476\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0xFFFD\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_is_whitespace\u001b[1;34m(char)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\\t\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m     \u001b[0mcat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Zs\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(my_model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
    "      device=device, max_epochs=1, eval_every=1, name=\"no_nlp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4c350da27618d5732fc58ebcab8d2c0381c51b7361f332741f21e30512bbdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
