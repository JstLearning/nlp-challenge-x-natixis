{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# from preprocessing.preprocessing import ecb_pipeline_en, fast_detect\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"data/train_series.csv\"\n",
    "FILENAME_ECB = \"data/ecb_data_preprocessed.csv\"\n",
    "FILENAME_FED = \"data/fed_data_preprocessed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv(FILENAME, index_col=0)\n",
    "ecb = pd.read_csv(FILENAME_ECB, index_col=0)\n",
    "fed = pd.read_csv(FILENAME_FED, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.get_dummies(returns, columns=[\"Index Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[\"Sign\"] = (returns[\"Index + 1\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_ecb = returns[\"index ecb\"]\n",
    "returns_fed = returns[\"index fed\"]\n",
    "# returns_index_name = returns[\"Index Name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = returns[\"Sign\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4930\n",
       "1    4016\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4930\n",
       "1    4016\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_dataset_size = len(y)\n",
    "y.iloc[:small_dataset_size].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns.drop([\"index ecb\", \"index fed\", \"Sign\", \"Index + 1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontextual_cols = ['Index - 9',\n",
    " 'Index - 8',\n",
    " 'Index - 7',\n",
    " 'Index - 6',\n",
    " 'Index - 5',\n",
    " 'Index - 4',\n",
    " 'Index - 3',\n",
    " 'Index - 2',\n",
    " 'Index - 1',\n",
    " 'Index - 0',\n",
    " 'Index Name_CVIX Index',\n",
    " 'Index Name_EURUSD Curncy',\n",
    " 'Index Name_EURUSDV1M Curncy',\n",
    " 'Index Name_MOVE Index',\n",
    " 'Index Name_SPX Index',\n",
    " 'Index Name_SRVIX Index',\n",
    " 'Index Name_SX5E Index',\n",
    " 'Index Name_V2X Index',\n",
    " 'Index Name_VIX Index']\n",
    "nb_nontextfeatures = len(nontextual_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_arr = np.arange(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% val, 20% test\n",
    "\n",
    "idx_, idx_test, y_, y_test = train_test_split(\n",
    "    idx_arr, y, test_size=0.1, train_size=0.9,\n",
    "    random_state=0, stratify=y.iloc[:small_dataset_size]\n",
    "    )\n",
    "\n",
    "idx_train, idx_val, y_train, y_val = train_test_split(\n",
    "    idx_, y_, test_size=0.1, train_size=0.9,\n",
    "    random_state=42, stratify=y_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_train = returns.iloc[idx_train]\n",
    "returns_val = returns.iloc[idx_val]\n",
    "returns_test = returns.iloc[idx_test]\n",
    "\n",
    "returns_ecb_train = returns_ecb.iloc[idx_train]\n",
    "returns_ecb_val = returns_ecb.iloc[idx_val]\n",
    "returns_ecb_test = returns_ecb.iloc[idx_test]\n",
    "\n",
    "returns_fed_train = returns_fed.iloc[idx_train]\n",
    "returns_fed_val = returns_fed.iloc[idx_val]\n",
    "returns_fed_test = returns_fed.iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    53\n",
       "1    47\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.iloc[:100].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>-0.027519</td>\n",
       "      <td>-0.103565</td>\n",
       "      <td>-0.045086</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.054050</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.008436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-0.011304</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004980</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.001083</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.001623</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.006444</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "0   0.001045   0.005841   0.003832  -0.027519  -0.103565  -0.045086   \n",
       "1  -0.021497   0.007891  -0.013175  -0.008436   0.000000   0.026303   \n",
       "2  -0.001872  -0.008154   0.023588   0.004086   0.003493   0.003300   \n",
       "3   0.004980  -0.000864   0.001677   0.000000   0.006030  -0.001083   \n",
       "4   0.000360  -0.001893   0.005579  -0.003056  -0.001171  -0.001623   \n",
       "\n",
       "   Index - 3  Index - 2  Index - 1  Index - 0  Index Name_CVIX Index  \\\n",
       "0  -0.011265   0.005164   0.054050   0.015779                      0   \n",
       "1   0.000556   0.001455   0.007422   0.000000                      0   \n",
       "2   0.000885  -0.011304   0.005040   0.000156                      0   \n",
       "3   0.000419   0.001492   0.001018  -0.002582                      0   \n",
       "4  -0.002350  -0.006444  -0.000729  -0.000365                      0   \n",
       "\n",
       "   Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "2                         0                            0   \n",
       "3                         0                            0   \n",
       "4                         1                            0   \n",
       "\n",
       "   Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "0                      0                     0                       0   \n",
       "1                      1                     0                       0   \n",
       "2                      0                     1                       0   \n",
       "3                      0                     1                       0   \n",
       "4                      0                     0                       0   \n",
       "\n",
       "   Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  \n",
       "0                      0                     1                     0  \n",
       "1                      0                     0                     0  \n",
       "2                      0                     0                     0  \n",
       "3                      0                     0                     0  \n",
       "4                      0                     0                     0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del returns, y\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "      <td>8946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.040715</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.039987</td>\n",
       "      <td>0.040587</td>\n",
       "      <td>0.039230</td>\n",
       "      <td>0.039386</td>\n",
       "      <td>0.040104</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.040365</td>\n",
       "      <td>0.040699</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.314287</td>\n",
       "      <td>0.314287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.355095</td>\n",
       "      <td>-0.355095</td>\n",
       "      <td>-0.355095</td>\n",
       "      <td>-0.355095</td>\n",
       "      <td>-0.355095</td>\n",
       "      <td>-0.355095</td>\n",
       "      <td>-0.350588</td>\n",
       "      <td>-0.350588</td>\n",
       "      <td>-0.345301</td>\n",
       "      <td>-0.345301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.011516</td>\n",
       "      <td>-0.011160</td>\n",
       "      <td>-0.011122</td>\n",
       "      <td>-0.010843</td>\n",
       "      <td>-0.010698</td>\n",
       "      <td>-0.010363</td>\n",
       "      <td>-0.010435</td>\n",
       "      <td>-0.010155</td>\n",
       "      <td>-0.010360</td>\n",
       "      <td>-0.010515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.008365</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>0.008156</td>\n",
       "      <td>0.008573</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>0.008732</td>\n",
       "      <td>0.008855</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.382167</td>\n",
       "      <td>0.382167</td>\n",
       "      <td>0.382167</td>\n",
       "      <td>0.496008</td>\n",
       "      <td>0.496008</td>\n",
       "      <td>0.496008</td>\n",
       "      <td>0.768245</td>\n",
       "      <td>0.768245</td>\n",
       "      <td>0.768245</td>\n",
       "      <td>0.768245</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Index - 9    Index - 8    Index - 7    Index - 6    Index - 5  \\\n",
       "count  8946.000000  8946.000000  8946.000000  8946.000000  8946.000000   \n",
       "mean     -0.000008     0.000200     0.000255     0.000339     0.000090   \n",
       "std       0.040715     0.040788     0.039987     0.040587     0.039230   \n",
       "min      -0.355095    -0.355095    -0.355095    -0.355095    -0.355095   \n",
       "25%      -0.011516    -0.011160    -0.011122    -0.010843    -0.010698   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.008365     0.008304     0.008276     0.008358     0.008156   \n",
       "max       0.382167     0.382167     0.382167     0.496008     0.496008   \n",
       "\n",
       "         Index - 4    Index - 3    Index - 2    Index - 1    Index - 0  \\\n",
       "count  8946.000000  8946.000000  8946.000000  8946.000000  8946.000000   \n",
       "mean      0.000407     0.000644     0.000988     0.000847     0.000950   \n",
       "std       0.039386     0.040104     0.039900     0.040365     0.040699   \n",
       "min      -0.355095    -0.350588    -0.350588    -0.345301    -0.345301   \n",
       "25%      -0.010363    -0.010435    -0.010155    -0.010360    -0.010515   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.008573     0.008795     0.008732     0.008855     0.009020   \n",
       "max       0.496008     0.768245     0.768245     0.768245     0.768245   \n",
       "\n",
       "       Index Name_CVIX Index  Index Name_EURUSD Curncy  \\\n",
       "count            8946.000000               8946.000000   \n",
       "mean                0.111111                  0.111111   \n",
       "std                 0.314287                  0.314287   \n",
       "min                 0.000000                  0.000000   \n",
       "25%                 0.000000                  0.000000   \n",
       "50%                 0.000000                  0.000000   \n",
       "75%                 0.000000                  0.000000   \n",
       "max                 1.000000                  1.000000   \n",
       "\n",
       "       Index Name_EURUSDV1M Curncy  Index Name_MOVE Index  \\\n",
       "count                  8946.000000            8946.000000   \n",
       "mean                      0.111111               0.111111   \n",
       "std                       0.314287               0.314287   \n",
       "min                       0.000000               0.000000   \n",
       "25%                       0.000000               0.000000   \n",
       "50%                       0.000000               0.000000   \n",
       "75%                       0.000000               0.000000   \n",
       "max                       1.000000               1.000000   \n",
       "\n",
       "       Index Name_SPX Index  Index Name_SRVIX Index  Index Name_SX5E Index  \\\n",
       "count           8946.000000             8946.000000            8946.000000   \n",
       "mean               0.111111                0.111111               0.111111   \n",
       "std                0.314287                0.314287               0.314287   \n",
       "min                0.000000                0.000000               0.000000   \n",
       "25%                0.000000                0.000000               0.000000   \n",
       "50%                0.000000                0.000000               0.000000   \n",
       "75%                0.000000                0.000000               0.000000   \n",
       "max                1.000000                1.000000               1.000000   \n",
       "\n",
       "       Index Name_V2X Index  Index Name_VIX Index  \n",
       "count           8946.000000           8946.000000  \n",
       "mean               0.111111              0.111111  \n",
       "std                0.314287              0.314287  \n",
       "min                0.000000              0.000000  \n",
       "25%                0.000000              0.000000  \n",
       "50%                0.000000              0.000000  \n",
       "75%                0.000000              0.000000  \n",
       "max                1.000000              1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct = ColumnTransformer([\n",
    "#     ('Standard Scaler', StandardScaler(), [\n",
    "#                                     'Index - 9',\n",
    "#                                     'Index - 8',\n",
    "#                                     'Index - 7',\n",
    "#                                     'Index - 6',\n",
    "#                                     'Index - 5',\n",
    "#                                     'Index - 4',\n",
    "#                                     'Index - 3',\n",
    "#                                     'Index - 2',\n",
    "#                                     'Index - 1',\n",
    "#                                     'Index - 0'])\n",
    "# ], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns_train = pd.DataFrame(ct.fit_transform(returns_train), columns=returns_train.columns)\n",
    "# returns_val = pd.DataFrame(ct.transform(returns_val), columns=returns_train.columns)\n",
    "# returns_test = pd.DataFrame(ct.transform(returns_test), columns=returns_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>-0.049060</td>\n",
       "      <td>-0.040567</td>\n",
       "      <td>-0.024014</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>-0.020061</td>\n",
       "      <td>-0.005588</td>\n",
       "      <td>-0.030342</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>-0.012848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>-0.009237</td>\n",
       "      <td>-0.003332</td>\n",
       "      <td>-0.013174</td>\n",
       "      <td>-0.004549</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.005962</td>\n",
       "      <td>-0.003199</td>\n",
       "      <td>-0.010558</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>-0.003705</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>-0.001332</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>-0.011753</td>\n",
       "      <td>-0.001797</td>\n",
       "      <td>-0.007600</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.008813</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7142</th>\n",
       "      <td>-0.015253</td>\n",
       "      <td>0.067968</td>\n",
       "      <td>-0.001604</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.014383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010156</td>\n",
       "      <td>0.030114</td>\n",
       "      <td>-0.022277</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>-0.031336</td>\n",
       "      <td>0.004195</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.096325</td>\n",
       "      <td>-0.027058</td>\n",
       "      <td>-0.051207</td>\n",
       "      <td>-0.000578</td>\n",
       "      <td>-0.048530</td>\n",
       "      <td>-0.023935</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "3534  -0.049060  -0.040567  -0.024014   0.012658  -0.020061  -0.005588   \n",
       "5161  -0.009237  -0.003332  -0.013174  -0.004549   0.001697   0.005962   \n",
       "2825  -0.003705   0.003926  -0.001332   0.001110  -0.011753  -0.001797   \n",
       "7142  -0.015253   0.067968  -0.001604   0.033489   0.014383   0.000000   \n",
       "6174  -0.031336   0.004195   0.017196   0.096325  -0.027058  -0.051207   \n",
       "\n",
       "      Index - 3  Index - 2  Index - 1  Index - 0  Index Name_CVIX Index  \\\n",
       "3534  -0.030342   0.028812   0.038370  -0.012848                      0   \n",
       "5161  -0.003199  -0.010558   0.002874   0.001792                      0   \n",
       "2825  -0.007600   0.005574   0.001276   0.008813                      0   \n",
       "7142  -0.010156   0.030114  -0.022277   0.005800                      0   \n",
       "6174  -0.000578  -0.048530  -0.023935  -0.012500                      0   \n",
       "\n",
       "      Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "3534                         0                            1   \n",
       "5161                         1                            0   \n",
       "2825                         1                            0   \n",
       "7142                         0                            0   \n",
       "6174                         0                            0   \n",
       "\n",
       "      Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "3534                      0                     0                       0   \n",
       "5161                      0                     0                       0   \n",
       "2825                      0                     0                       0   \n",
       "7142                      0                     1                       0   \n",
       "6174                      1                     0                       0   \n",
       "\n",
       "      Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  \n",
       "3534                      0                     0                     0  \n",
       "5161                      0                     0                     0  \n",
       "2825                      0                     0                     0  \n",
       "7142                      0                     0                     0  \n",
       "6174                      0                     0                     0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_idx(string):\n",
    "    index_text = [int(i) for i in string.split(\",\")]\n",
    "    return index_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_text_train = pd.DataFrame()\n",
    "indices_text_val = pd.DataFrame()\n",
    "indices_text_test = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_text_train[\"index ecb\"] = returns_ecb_train.apply(parse_text_idx)\n",
    "indices_text_train[\"index fed\"] = returns_fed_train.apply(parse_text_idx)\n",
    "\n",
    "indices_text_val[\"index ecb\"] = returns_ecb_val.apply(parse_text_idx)\n",
    "indices_text_val[\"index fed\"] = returns_fed_val.apply(parse_text_idx)\n",
    "\n",
    "indices_text_test[\"index ecb\"] = returns_ecb_test.apply(parse_text_idx)\n",
    "indices_text_test[\"index fed\"] = returns_fed_test.apply(parse_text_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index ecb</th>\n",
       "      <th>index fed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>[605]</td>\n",
       "      <td>[389]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>[1036]</td>\n",
       "      <td>[134]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>[1581]</td>\n",
       "      <td>[336]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7142</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>[318]</td>\n",
       "      <td>[376]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>[1082]</td>\n",
       "      <td>[13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>[210]</td>\n",
       "      <td>[164]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8233</th>\n",
       "      <td>[653]</td>\n",
       "      <td>[594]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>[1237]</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>[404]</td>\n",
       "      <td>[261]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7245 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index ecb index fed\n",
       "3534        [605]     [389]\n",
       "5161       [1036]     [134]\n",
       "2825       [1581]     [336]\n",
       "7142  [1614, 634]     [316]\n",
       "6174        [318]     [376]\n",
       "...           ...       ...\n",
       "1160       [1082]      [13]\n",
       "7007        [210]     [164]\n",
       "8233        [653]     [594]\n",
       "1815       [1237]      [32]\n",
       "2619        [404]     [261]\n",
       "\n",
       "[7245 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_text_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index ecb</th>\n",
       "      <th>index fed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7587</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2180</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8232</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7649</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5589</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4815</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7951</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4728</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6712</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6976</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3629</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3814</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7656</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8747</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6259</th>\n",
       "      <td>[1008, 531, 1319, 1389]</td>\n",
       "      <td>[168]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    index ecb index fed\n",
       "7587  [1008, 531, 1319, 1389]     [168]\n",
       "2180  [1008, 531, 1319, 1389]     [168]\n",
       "8232  [1008, 531, 1319, 1389]     [168]\n",
       "7649  [1008, 531, 1319, 1389]     [168]\n",
       "6039  [1008, 531, 1319, 1389]     [168]\n",
       "5589  [1008, 531, 1319, 1389]     [168]\n",
       "5946  [1008, 531, 1319, 1389]     [168]\n",
       "4815  [1008, 531, 1319, 1389]     [168]\n",
       "1613  [1008, 531, 1319, 1389]     [168]\n",
       "7951  [1008, 531, 1319, 1389]     [168]\n",
       "4728  [1008, 531, 1319, 1389]     [168]\n",
       "4750  [1008, 531, 1319, 1389]     [168]\n",
       "5400  [1008, 531, 1319, 1389]     [168]\n",
       "29    [1008, 531, 1319, 1389]     [168]\n",
       "6712  [1008, 531, 1319, 1389]     [168]\n",
       "6976  [1008, 531, 1319, 1389]     [168]\n",
       "3629  [1008, 531, 1319, 1389]     [168]\n",
       "3681  [1008, 531, 1319, 1389]     [168]\n",
       "3814  [1008, 531, 1319, 1389]     [168]\n",
       "7656  [1008, 531, 1319, 1389]     [168]\n",
       "8747  [1008, 531, 1319, 1389]     [168]\n",
       "6555  [1008, 531, 1319, 1389]     [168]\n",
       "6259  [1008, 531, 1319, 1389]     [168]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_mask = indices_text_train[\"index ecb\"].apply(lambda x : 1008 in x)\n",
    "indices_text_train[example_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[example_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_times = ['Index - 9',\n",
    " 'Index - 8',\n",
    " 'Index - 7',\n",
    " 'Index - 6',\n",
    " 'Index - 5',\n",
    " 'Index - 4',\n",
    " 'Index - 3',\n",
    " 'Index - 2',\n",
    " 'Index - 1',\n",
    " 'Index - 0']\n",
    "\n",
    "std_col_names = [\"std3 \" + name for name in index_times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index - 9   -0.049060\n",
       "Index - 8   -0.040567\n",
       "Index - 7   -0.024014\n",
       "Index - 6    0.012658\n",
       "Index - 5   -0.020061\n",
       "Index - 4   -0.005588\n",
       "Index - 3   -0.030342\n",
       "Index - 2    0.028812\n",
       "Index - 1    0.038370\n",
       "Index - 0   -0.012848\n",
       "Name: 3534, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_index_times = returns_train[index_times].astype(float)\n",
    "returns_index_times.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>std3 Index - 7</th>\n",
       "      <th>std3 Index - 6</th>\n",
       "      <th>std3 Index - 5</th>\n",
       "      <th>std3 Index - 4</th>\n",
       "      <th>std3 Index - 3</th>\n",
       "      <th>std3 Index - 2</th>\n",
       "      <th>std3 Index - 1</th>\n",
       "      <th>std3 Index - 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>0.012737</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.020129</td>\n",
       "      <td>0.016396</td>\n",
       "      <td>0.012436</td>\n",
       "      <td>0.029708</td>\n",
       "      <td>0.037220</td>\n",
       "      <td>0.027234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>0.004953</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>0.006727</td>\n",
       "      <td>0.007463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.006746</td>\n",
       "      <td>0.005001</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>0.006718</td>\n",
       "      <td>0.003781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7142</th>\n",
       "      <td>0.044633</td>\n",
       "      <td>0.034787</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.012330</td>\n",
       "      <td>0.020943</td>\n",
       "      <td>0.027427</td>\n",
       "      <td>0.026218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6174</th>\n",
       "      <td>0.025122</td>\n",
       "      <td>0.049864</td>\n",
       "      <td>0.062508</td>\n",
       "      <td>0.079133</td>\n",
       "      <td>0.025324</td>\n",
       "      <td>0.028490</td>\n",
       "      <td>0.023979</td>\n",
       "      <td>0.018411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>0.019376</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.010306</td>\n",
       "      <td>0.009769</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>0.063652</td>\n",
       "      <td>0.069971</td>\n",
       "      <td>0.064639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>0.029202</td>\n",
       "      <td>0.015134</td>\n",
       "      <td>0.017234</td>\n",
       "      <td>0.008815</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>0.004269</td>\n",
       "      <td>0.026487</td>\n",
       "      <td>0.022863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8233</th>\n",
       "      <td>0.041342</td>\n",
       "      <td>0.012998</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.056133</td>\n",
       "      <td>0.049985</td>\n",
       "      <td>0.067473</td>\n",
       "      <td>0.025794</td>\n",
       "      <td>0.020503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>0.008642</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.011826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2619</th>\n",
       "      <td>0.138976</td>\n",
       "      <td>0.102143</td>\n",
       "      <td>0.102437</td>\n",
       "      <td>0.051939</td>\n",
       "      <td>0.067396</td>\n",
       "      <td>0.072879</td>\n",
       "      <td>0.069580</td>\n",
       "      <td>0.030011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7245 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      std3 Index - 7  std3 Index - 6  std3 Index - 5  std3 Index - 4  \\\n",
       "3534        0.012737        0.027239        0.020129        0.016396   \n",
       "5161        0.004953        0.005365        0.007467        0.005287   \n",
       "2825        0.003905        0.002631        0.006832        0.006746   \n",
       "7142        0.044633        0.034787        0.017570        0.016800   \n",
       "6174        0.025122        0.049864        0.062508        0.079133   \n",
       "...              ...             ...             ...             ...   \n",
       "1160        0.019376        0.008602        0.010306        0.009769   \n",
       "7007        0.029202        0.015134        0.017234        0.008815   \n",
       "8233        0.041342        0.012998        0.008060        0.056133   \n",
       "1815        0.008642        0.009656        0.002939        0.005509   \n",
       "2619        0.138976        0.102143        0.102437        0.051939   \n",
       "\n",
       "      std3 Index - 3  std3 Index - 2  std3 Index - 1  std3 Index - 0  \n",
       "3534        0.012436        0.029708        0.037220        0.027234  \n",
       "5161        0.004584        0.008277        0.006727        0.007463  \n",
       "2825        0.005001        0.006603        0.006718        0.003781  \n",
       "7142        0.012330        0.020943        0.027427        0.026218  \n",
       "6174        0.025324        0.028490        0.023979        0.018411  \n",
       "...              ...             ...             ...             ...  \n",
       "1160        0.014544        0.063652        0.069971        0.064639  \n",
       "7007        0.008802        0.004269        0.026487        0.022863  \n",
       "8233        0.049985        0.067473        0.025794        0.020503  \n",
       "1815        0.005044        0.008843        0.006455        0.011826  \n",
       "2619        0.067396        0.072879        0.069580        0.030011  \n",
       "\n",
       "[7245 rows x 8 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_std_train = returns_train[index_times].astype(float).rolling(window=3, axis=1, center=False).std()\n",
    "rolling_std_train = rolling_std_train.drop(columns=['Index - 9', 'Index - 8'])\n",
    "rolling_std_train.columns = std_col_names[-8:]\n",
    "\n",
    "rolling_std_val = returns_val[index_times].astype(float).rolling(window=3, axis=1, center=False).std()\n",
    "rolling_std_val = rolling_std_val.drop(columns=['Index - 9', 'Index - 8'])\n",
    "rolling_std_val.columns = std_col_names[-8:]\n",
    "\n",
    "rolling_std_test = returns_test[index_times].astype(float).rolling(window=3, axis=1, center=False).std()\n",
    "rolling_std_test = rolling_std_test.drop(columns=['Index - 9', 'Index - 8'])\n",
    "rolling_std_test.columns = std_col_names[-8:]\n",
    "\n",
    "rolling_std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5015375153751538"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((returns_index_times[\"Index - 0\"] - returns_index_times[\"Index - 1\"])[y==1] > 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((returns_index_times[\"Index - 0\"] - returns_index_times[\"Index - 1\"])[y_train==1], bins=50, alpha=.25)\n",
    "plt.hist((returns_index_times[\"Index - 0\"] - returns_index_times[\"Index - 1\"])[y_train==0], bins=50, alpha=.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7245"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_mask = indices_text_train[\"index ecb\"].apply(lambda x : 634 in x)\n",
    "example_mask.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.325"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[example_mask].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index ecb</th>\n",
       "      <th>index fed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7142</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8181</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7647</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8503</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6546</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7135</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>[1614, 634]</td>\n",
       "      <td>[316]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index ecb index fed\n",
       "7142  [1614, 634]     [316]\n",
       "8181  [1614, 634]     [316]\n",
       "7647  [1614, 634]     [316]\n",
       "8503  [1614, 634]     [316]\n",
       "5418  [1614, 634]     [316]\n",
       "...           ...       ...\n",
       "3678  [1614, 634]     [316]\n",
       "6546  [1614, 634]     [316]\n",
       "7135  [1614, 634]     [316]\n",
       "1813  [1614, 634]     [316]\n",
       "5640  [1614, 634]     [316]\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_text_train[example_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7142</th>\n",
       "      <td>-0.015253</td>\n",
       "      <td>0.067968</td>\n",
       "      <td>-0.001604</td>\n",
       "      <td>0.033489</td>\n",
       "      <td>0.014383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010156</td>\n",
       "      <td>0.030114</td>\n",
       "      <td>-0.022277</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8181</th>\n",
       "      <td>-0.008118</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>-0.021079</td>\n",
       "      <td>-0.072269</td>\n",
       "      <td>-0.017400</td>\n",
       "      <td>-0.052656</td>\n",
       "      <td>-0.062690</td>\n",
       "      <td>-0.029992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7647</th>\n",
       "      <td>-0.017589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019436</td>\n",
       "      <td>-0.087143</td>\n",
       "      <td>0.013998</td>\n",
       "      <td>-0.008118</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>-0.021079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8503</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019436</td>\n",
       "      <td>-0.087143</td>\n",
       "      <td>0.013998</td>\n",
       "      <td>-0.008118</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>0.044735</td>\n",
       "      <td>-0.021079</td>\n",
       "      <td>-0.072269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>-0.038266</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>-0.041459</td>\n",
       "      <td>0.015485</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>-0.015328</td>\n",
       "      <td>0.025662</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678</th>\n",
       "      <td>0.078412</td>\n",
       "      <td>-0.018036</td>\n",
       "      <td>-0.050100</td>\n",
       "      <td>0.138793</td>\n",
       "      <td>0.035414</td>\n",
       "      <td>-0.078539</td>\n",
       "      <td>-0.014396</td>\n",
       "      <td>-0.141225</td>\n",
       "      <td>-0.076316</td>\n",
       "      <td>0.008376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6546</th>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>-0.008746</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>-0.003402</td>\n",
       "      <td>-0.005554</td>\n",
       "      <td>-0.015397</td>\n",
       "      <td>-0.023624</td>\n",
       "      <td>-0.007751</td>\n",
       "      <td>-0.013710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7135</th>\n",
       "      <td>-0.001830</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>-0.004387</td>\n",
       "      <td>-0.007632</td>\n",
       "      <td>0.003777</td>\n",
       "      <td>-0.000460</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>-0.002585</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>-0.086459</td>\n",
       "      <td>0.078412</td>\n",
       "      <td>-0.018036</td>\n",
       "      <td>-0.050100</td>\n",
       "      <td>0.138793</td>\n",
       "      <td>0.035414</td>\n",
       "      <td>-0.078539</td>\n",
       "      <td>-0.014396</td>\n",
       "      <td>-0.141225</td>\n",
       "      <td>-0.076316</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.040074</td>\n",
       "      <td>0.013378</td>\n",
       "      <td>-0.021265</td>\n",
       "      <td>0.039917</td>\n",
       "      <td>-0.009831</td>\n",
       "      <td>0.013086</td>\n",
       "      <td>-0.001084</td>\n",
       "      <td>-0.050038</td>\n",
       "      <td>-0.005718</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "7142  -0.015253   0.067968  -0.001604   0.033489   0.014383   0.000000   \n",
       "8181  -0.008118  -0.001288   0.010823   0.044735  -0.021079  -0.072269   \n",
       "7647  -0.017589   0.000000   0.019436  -0.087143   0.013998  -0.008118   \n",
       "8503   0.000000   0.019436  -0.087143   0.013998  -0.008118  -0.001288   \n",
       "5418  -0.038266   0.001477   0.026648   0.007313  -0.041459   0.015485   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3678   0.078412  -0.018036  -0.050100   0.138793   0.035414  -0.078539   \n",
       "6546   0.000563   0.000984  -0.008746   0.001133  -0.003402  -0.005554   \n",
       "7135  -0.001830   0.004387  -0.004387  -0.007632   0.003777  -0.000460   \n",
       "1813  -0.086459   0.078412  -0.018036  -0.050100   0.138793   0.035414   \n",
       "5640  -0.002334   0.040074   0.013378  -0.021265   0.039917  -0.009831   \n",
       "\n",
       "      Index - 3  Index - 2  Index - 1  Index - 0  Index Name_CVIX Index  \\\n",
       "7142  -0.010156   0.030114  -0.022277   0.005800                      0   \n",
       "8181  -0.017400  -0.052656  -0.062690  -0.029992                      0   \n",
       "7647  -0.001288   0.010823   0.044735  -0.021079                      0   \n",
       "8503   0.010823   0.044735  -0.021079  -0.072269                      0   \n",
       "5418   0.006175  -0.015328   0.025662   0.017190                      0   \n",
       "...         ...        ...        ...        ...                    ...   \n",
       "3678  -0.014396  -0.141225  -0.076316   0.008376                      0   \n",
       "6546  -0.015397  -0.023624  -0.007751  -0.013710                      0   \n",
       "7135  -0.002395  -0.002585  -0.000185   0.000832                      0   \n",
       "1813  -0.078539  -0.014396  -0.141225  -0.076316                      0   \n",
       "5640   0.013086  -0.001084  -0.050038  -0.005718                      1   \n",
       "\n",
       "      Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "7142                         0                            0   \n",
       "8181                         0                            0   \n",
       "7647                         0                            0   \n",
       "8503                         0                            0   \n",
       "5418                         0                            0   \n",
       "...                        ...                          ...   \n",
       "3678                         0                            0   \n",
       "6546                         0                            0   \n",
       "7135                         1                            0   \n",
       "1813                         0                            0   \n",
       "5640                         0                            0   \n",
       "\n",
       "      Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "7142                      0                     1                       0   \n",
       "8181                      1                     0                       0   \n",
       "7647                      1                     0                       0   \n",
       "8503                      1                     0                       0   \n",
       "5418                      0                     0                       0   \n",
       "...                     ...                   ...                     ...   \n",
       "3678                      0                     0                       0   \n",
       "6546                      0                     0                       1   \n",
       "7135                      0                     0                       0   \n",
       "1813                      0                     0                       0   \n",
       "5640                      0                     0                       0   \n",
       "\n",
       "      Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  \n",
       "7142                      0                     0                     0  \n",
       "8181                      0                     0                     0  \n",
       "7647                      0                     0                     0  \n",
       "8503                      0                     0                     0  \n",
       "5418                      1                     0                     0  \n",
       "...                     ...                   ...                   ...  \n",
       "3678                      0                     0                     1  \n",
       "6546                      0                     0                     0  \n",
       "7135                      0                     0                     0  \n",
       "1813                      0                     0                     1  \n",
       "5640                      0                     0                     0  \n",
       "\n",
       "[80 rows x 19 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_train[example_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index ecb     index fed\n",
       "[350]         [614]          3\n",
       "[627]         [401]          4\n",
       "[107]         [20]           4\n",
       "[1566, 667]   [389]          4\n",
       "[1068, 1358]  [220]          4\n",
       "                          ... \n",
       "[673]         [586]         75\n",
       "[643]         [734]         79\n",
       "[1614, 634]   [316]         80\n",
       "[1325]        [459]         85\n",
       "[571]         [497]        146\n",
       "Length: 521, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_text_train.astype(str).groupby([\"index ecb\", \"index fed\"]).size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggest_mask = indices_text_train.apply(lambda x : x[\"index ecb\"][0]==1325 and x[\"index fed\"][0] == 459, axis=1)\n",
    "biggest_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:36:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huuta\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\huuta\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(rolling_std_train[example_mask], y_train[example_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(xgbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:36:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huuta\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\huuta\\anaconda3\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=12,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = pd.concat([rolling_std_train, returns_train], axis=1)\n",
    "\n",
    "xgbc.fit(x_train[biggest_mask], y_train[biggest_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(xgbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_test = indices_text_test.apply(lambda x : x[\"index ecb\"][0]==1325 and x[\"index fed\"][0] == 459, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>std3 Index - 7</th>\n",
       "      <th>std3 Index - 6</th>\n",
       "      <th>std3 Index - 5</th>\n",
       "      <th>std3 Index - 4</th>\n",
       "      <th>std3 Index - 3</th>\n",
       "      <th>std3 Index - 2</th>\n",
       "      <th>std3 Index - 1</th>\n",
       "      <th>std3 Index - 0</th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>...</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>0.019995</td>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.021727</td>\n",
       "      <td>0.036214</td>\n",
       "      <td>0.020398</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>0.013250</td>\n",
       "      <td>-0.036056</td>\n",
       "      <td>-0.019564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032489</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.005530</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.009651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001978</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>0.018560</td>\n",
       "      <td>0.017514</td>\n",
       "      <td>0.033074</td>\n",
       "      <td>0.054679</td>\n",
       "      <td>0.056699</td>\n",
       "      <td>0.102345</td>\n",
       "      <td>0.061158</td>\n",
       "      <td>0.065090</td>\n",
       "      <td>0.028690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.006791</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>-0.001059</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>-0.004515</td>\n",
       "      <td>-0.003778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>0.003793</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.006791</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6449</th>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.052954</td>\n",
       "      <td>0.043201</td>\n",
       "      <td>0.035610</td>\n",
       "      <td>0.047668</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>0.057346</td>\n",
       "      <td>0.069025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8138</th>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7822</th>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.017159</td>\n",
       "      <td>0.019995</td>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.021727</td>\n",
       "      <td>0.036214</td>\n",
       "      <td>0.020398</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>-0.001745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046624</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>-0.004882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0.015139</td>\n",
       "      <td>0.012606</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.011994</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.014505</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002829</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7788</th>\n",
       "      <td>0.017159</td>\n",
       "      <td>0.019995</td>\n",
       "      <td>0.020586</td>\n",
       "      <td>0.021727</td>\n",
       "      <td>0.036214</td>\n",
       "      <td>0.020398</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>0.020487</td>\n",
       "      <td>-0.001745</td>\n",
       "      <td>-0.036056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.021576</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>0.028888</td>\n",
       "      <td>0.043544</td>\n",
       "      <td>0.044009</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>0.004590</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.005757</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>-0.000417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>0.003858</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>0.007150</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>-0.007311</td>\n",
       "      <td>-0.005791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003606</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.021576</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>0.028888</td>\n",
       "      <td>0.043544</td>\n",
       "      <td>0.032489</td>\n",
       "      <td>-0.023989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8816</th>\n",
       "      <td>0.036944</td>\n",
       "      <td>0.041637</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.052954</td>\n",
       "      <td>0.043201</td>\n",
       "      <td>0.035610</td>\n",
       "      <td>0.047668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057346</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032797</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>0.012606</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.011994</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.014505</td>\n",
       "      <td>0.012677</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.006627</td>\n",
       "      <td>-0.017322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      std3 Index - 7  std3 Index - 6  std3 Index - 5  std3 Index - 4  \\\n",
       "6736        0.019995        0.020586        0.021727        0.036214   \n",
       "6086        0.002823        0.003201        0.001607        0.001103   \n",
       "5909        0.018560        0.017514        0.033074        0.054679   \n",
       "2222        0.001464        0.003793        0.003646        0.006791   \n",
       "330         0.001478        0.001086        0.002402        0.003402   \n",
       "6997        0.003793        0.003646        0.006791        0.004333   \n",
       "6449        0.041637        0.042423        0.005600        0.052954   \n",
       "8138        0.001540        0.001906        0.002073        0.004296   \n",
       "7822        0.020880        0.017159        0.019995        0.020586   \n",
       "563         0.004986        0.002819        0.000523        0.003985   \n",
       "3330        0.015139        0.012606        0.003652        0.011994   \n",
       "7788        0.017159        0.019995        0.020586        0.021727   \n",
       "750         0.016215        0.005510        0.021576        0.021827   \n",
       "1104        0.002591        0.002766        0.003071        0.003924   \n",
       "8799        0.003858        0.002957        0.004806        0.010313   \n",
       "2935        0.028345        0.016430        0.016215        0.005510   \n",
       "8816        0.036944        0.041637        0.042423        0.005600   \n",
       "4251        0.012606        0.003652        0.011994        0.014019   \n",
       "\n",
       "      std3 Index - 3  std3 Index - 2  std3 Index - 1  std3 Index - 0  \\\n",
       "6736        0.020398        0.020756        0.020487        0.013250   \n",
       "6086        0.003957        0.005530        0.009827        0.009651   \n",
       "5909        0.056699        0.102345        0.061158        0.065090   \n",
       "2222        0.004333        0.002129        0.001575        0.002680   \n",
       "330         0.004960        0.005874        0.001746        0.002216   \n",
       "6997        0.002129        0.001575        0.002680        0.003196   \n",
       "6449        0.043201        0.035610        0.047668        0.032277   \n",
       "8138        0.002823        0.003201        0.001607        0.001103   \n",
       "7822        0.021727        0.036214        0.020398        0.020756   \n",
       "563         0.003346        0.003858        0.002957        0.004806   \n",
       "3330        0.014019        0.014505        0.012677        0.005078   \n",
       "7788        0.036214        0.020398        0.020756        0.020487   \n",
       "750         0.028888        0.043544        0.044009        0.014925   \n",
       "1104        0.004590        0.000481        0.004493        0.005757   \n",
       "8799        0.005526        0.007150        0.005029        0.003316   \n",
       "2935        0.021576        0.021827        0.028888        0.043544   \n",
       "8816        0.052954        0.043201        0.035610        0.047668   \n",
       "4251        0.014505        0.012677        0.005078        0.009857   \n",
       "\n",
       "      Index - 9  Index - 8  ...  Index - 0  Index Name_CVIX Index  \\\n",
       "6736  -0.036056  -0.019564  ...   0.032489                      0   \n",
       "6086   0.000000   0.005035  ...  -0.001978                      0   \n",
       "5909   0.028690   0.000000  ...   0.007621                      0   \n",
       "2222  -0.001059   0.000791  ...   0.001661                      0   \n",
       "330   -0.004515  -0.003778  ...   0.005964                      0   \n",
       "6997   0.000791   0.001832  ...   0.001302                      0   \n",
       "6449   0.057346   0.069025  ...  -0.020513                      0   \n",
       "8138   0.000000  -0.002588  ...  -0.000917                      0   \n",
       "7822   0.001715  -0.001745  ...   0.046624                      0   \n",
       "563    0.005090  -0.004882  ...   0.005626                      0   \n",
       "3330   0.010695   0.006627  ...  -0.002829                      1   \n",
       "7788  -0.001745  -0.036056  ...   0.020144                      0   \n",
       "750    0.000000  -0.031442  ...  -0.028130                      0   \n",
       "1104   0.004341  -0.000417  ...   0.006501                      0   \n",
       "8799  -0.007311  -0.005791  ...  -0.003606                      0   \n",
       "2935   0.032489  -0.023989  ...  -0.032401                      0   \n",
       "8816   0.000000   0.057346  ...   0.032797                      0   \n",
       "4251   0.006627  -0.017322  ...   0.012667                      1   \n",
       "\n",
       "      Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "6736                         0                            0   \n",
       "6086                         0                            0   \n",
       "5909                         0                            0   \n",
       "2222                         0                            0   \n",
       "330                          0                            0   \n",
       "6997                         0                            0   \n",
       "6449                         0                            0   \n",
       "8138                         0                            0   \n",
       "7822                         0                            0   \n",
       "563                          0                            0   \n",
       "3330                         0                            0   \n",
       "7788                         0                            0   \n",
       "750                          0                            0   \n",
       "1104                         1                            0   \n",
       "8799                         0                            0   \n",
       "2935                         0                            0   \n",
       "8816                         0                            0   \n",
       "4251                         0                            0   \n",
       "\n",
       "      Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "6736                      1                     0                       0   \n",
       "6086                      0                     0                       1   \n",
       "5909                      0                     0                       0   \n",
       "2222                      0                     1                       0   \n",
       "330                       0                     0                       1   \n",
       "6997                      0                     1                       0   \n",
       "6449                      0                     0                       0   \n",
       "8138                      0                     0                       1   \n",
       "7822                      1                     0                       0   \n",
       "563                       0                     0                       0   \n",
       "3330                      0                     0                       0   \n",
       "7788                      1                     0                       0   \n",
       "750                       1                     0                       0   \n",
       "1104                      0                     0                       0   \n",
       "8799                      0                     0                       0   \n",
       "2935                      1                     0                       0   \n",
       "8816                      0                     0                       0   \n",
       "4251                      0                     0                       0   \n",
       "\n",
       "      Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  \n",
       "6736                      0                     0                     0  \n",
       "6086                      0                     0                     0  \n",
       "5909                      0                     0                     1  \n",
       "2222                      0                     0                     0  \n",
       "330                       0                     0                     0  \n",
       "6997                      0                     0                     0  \n",
       "6449                      0                     1                     0  \n",
       "8138                      0                     0                     0  \n",
       "7822                      0                     0                     0  \n",
       "563                       1                     0                     0  \n",
       "3330                      0                     0                     0  \n",
       "7788                      0                     0                     0  \n",
       "750                       0                     0                     0  \n",
       "1104                      0                     0                     0  \n",
       "8799                      1                     0                     0  \n",
       "2935                      0                     0                     0  \n",
       "8816                      0                     1                     0  \n",
       "4251                      0                     0                     0  \n",
       "\n",
       "[18 rows x 27 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = pd.concat([rolling_std_test, returns_test], axis=1)\n",
    "x_test[mask_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss:  0.8942543609688679\n",
      "accuracy:  0.6111111111111112\n"
     ]
    }
   ],
   "source": [
    "preds_test_proba = xgbc.predict_proba(x_test[mask_test])\n",
    "print(\"log loss: \", log_loss(y_test[mask_test], preds_test_proba))\n",
    "print(\"accuracy: \", accuracy_score(y_test[mask_test],  xgbc.predict(x_test[mask_test])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_dataset import get_data_loader\n",
    "from model.framework_model import MyModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_03\",\n",
    "\n",
    "    \"learning_rate\": 1e-3,\n",
    "\n",
    "    \"weight_decay\": 0,\n",
    "\n",
    "    \"batch_size\": 2,\n",
    "\n",
    "    \"layers\": 4,\n",
    "\n",
    "    \"mlp_hidden_dim\": 128,\n",
    "\n",
    "    \"dropout\": 0,\n",
    "\n",
    "    \"separate\": False,\n",
    "    \n",
    "    \"max_corpus_len\": 2,\n",
    "\n",
    "    \"max_epochs\": 20,\n",
    "\n",
    "    \"scheduler_step\": -1,\n",
    "\n",
    "    \"scheduler_ratio\": 0.1,\n",
    "\n",
    "    \"scheduler_last_epoch\": 20,\n",
    "\n",
    "    \"early_stopping\": False,\n",
    "\n",
    "    \"preload\": False\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_03\n"
     ]
    }
   ],
   "source": [
    "print(config[\"method\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((['In this context, the outcome of the UKâ€™s EU referendum is triggering a debate not just on the future relationship between the EU and the UK, but also â€“ and perhaps more importantly â€“ on how to improve the functioning of the EU and of Economic and Monetary Union (EMU) as one of its key elements.\\r\\nSo today I will be considering how we could boost support and strengthen the institutional arrangements for European integration.\\r\\nIn material terms, the EU has facilitated a level of prosperity unprecedented in European history, if only because by ensuring peace it prevented the strife and destruction of the past.\\r\\nPeople believe in Europe and European public goods when they are tangible and deliver results.\\r\\nEconomically, the pace of recovery in the euro area remains unsatisfactory, with unemployment levels still too high.\\r\\nRisk sharing involves both the public and private sectors, for example by ensuring the same level of deposit protection through a European deposit insurance scheme, but also by continuing to build a capital markets union in Europe.\\r\\nOn average, peopleâ€™s trust in both national and EU political institutions is very low.\\r\\nAs regards the   national level  , we know from the economic literature that strong institutions are crucial for sustainable economic growth and governance indicators suggest that there is room for improvement in many Member States.\\r\\nAs regards the European level, the gap between peopleâ€™s expectations and the EUâ€™s institutional setup is widening:    First, the perceived legitimacy of the EU remains low.\\r\\nSecond, opinion polls suggest that citizens are not opposed to European decision-making.\\r\\nWhen responding to these expectations, the EU and EMU face however two challenges:   The first has to do with subsidiarity: it should be made clearer where the EU has an advantage over Member States in terms of exercising competence and where not.\\r\\nA better demarcation of competences between the European and national level is desirable â€“ but not easy.\\r\\nThis, in my view, means that responsibilities and accountability lines should be more clearly defined and assigned, as Europe has done with the single monetary policy and the creation of the European Central Bank.\\r\\nThe risk is however that decision-making becomes a diplomatic exercise, resulting in protracted and opaque negotiations, a lack of long-term strategies and national interests being pitted against each other.\\r\\nThe EU is not only a union of Member States but also of citizens.\\r\\nWhere competences are assigned to the European level, they should be part of the Union method, with clear executive powers, full democratic accountability and rigorous judicial arrangements at that level.\\r\\nThis also means that the EUâ€™s agenda should be set by those who were duly elected at European level rather than by a few large Member States.\\r\\nOne of the ideas currently being discussed is a euro area treasury that would be responsible for fiscal policy decisions at European level.\\r\\nIt would ensure that the actions of a European finance minister are legitimised at the same level at which decisions are taken.',\n",
       "   ''],\n",
       "  [' The Global Financial Crisis and Great Recession posed daunting new challenges for central banks around the world and spurred innovations in the design, implementation, and communication of monetary policy. With the U.S. economy now nearing the Federal Reserve\\'s statutory goals of maximum employment and price stability, this conference provides a timely opportunity to consider how the lessons we learned are likely to influence the conduct of monetary policy in the future. The theme of the conference, \"Designing Resilient Monetary Policy Frameworks for the Future,\" encompasses many aspects of monetary policy, from the nitty-gritty details of implementing policy in financial markets to broader questions about how policy affects the economy. Within the operational realm, key choices include the selection of policy instruments, the specific markets in which the central bank participates, and the size and structure of the central bank\\'s balance sheet. These topics are of great importance to the Federal Reserve. As noted in the minutes of last month\\'s Federal Open Market Committee (FOMC) meeting, we are studying many issues related to policy implementation, research which ultimately will inform the FOMC\\'s views on how to most effectively conduct monetary policy in the years ahead. I expect that the work discussed at this conference will make valuable contributions to the understanding of many of these important issues. My focus today will be the policy tools that are needed to ensure that we have a resilient monetary policy framework. In particular, I will focus on whether our existing tools are adequate to respond to future economic downturns. As I will argue, one lesson from the crisis is that our pre-crisis toolkit was inadequate to address the range of economic circumstances that we faced. Looking ahead, we will likely need to retain many of the monetary policy tools that were developed to promote recovery from the crisis. In addition, policymakers inside and outside the Fed may wish at some point to consider additional options to secure a strong and resilient economy. But before I turn to these longer-run issues, I would like to offer a few remarks on the near-term outlook for the U.S. economy and the potential implications for monetary policy. Current Economic Situation and Outlook U.S. economic activity continues to expand, led by solid growth in household spending. But business investment remains soft and subdued foreign demand and the appreciation of the dollar since mid-2014 continue to restrain exports. While economic growth has not been rapid, it has been sufficient to generate further improvement in the labor market. Smoothing through the monthly ups and downs, job gains averaged 190,000 per month over the past three months. Although the unemployment rate has remained fairly steady this year, near 5 percent, broader measures of labor utilization have improved. Inflation has continued to run below the FOMC\\'s objective of 2 percent, reflecting in part the transitory effects of earlier declines in energy and import prices. Looking ahead, the FOMC expects moderate growth in real gross domestic product (GDP), additional strengthening in the labor market, and inflation rising to 2 percent over the next few years. Based on this economic outlook, the FOMC continues to anticipate that gradual increases in the federal funds rate will be appropriate over time to achieve and sustain employment and inflation near our statutory objectives. Indeed, in light of the continued solid performance of the labor market and our outlook for economic activity and inflation, I believe the case for an increase in the federal funds rate has strengthened in recent months. Of course, our decisions always depend on the degree to which incoming data continues to confirm the Committee\\'s outlook. And, as ever, the economic outlook is uncertain, and so monetary policy is not on a preset course. Our ability to predict how the federal funds rate will evolve over time is quite limited because monetary policy will need to respond to whatever disturbances may buffet the economy. In addition, the level of short-term interest rates consistent with the dual mandate varies over time in response to shifts in underlying economic conditions that are often evident only in hindsight. For these reasons, the range of reasonably likely outcomes for the federal funds rate is quite wide--a point illustrated by figure 1 in your handout. The line in the center is the median path for the federal funds rate based on the FOMC\\'s Summary of Economic Projections in June.1 The shaded region, which is based on the historical accuracy of private and government forecasters, shows a 70 percent probability that the federal funds rate will be between 0 and 3-1/4 percent at the end of next year and between 0 and 4-1/2 percent at the end of 2018.2 The reason for the wide range is that the economy is frequently buffeted by shocks and thus rarely evolves as predicted. When shocks occur and the economic outlook changes, monetary policy needs to adjust. What we do know, however, is that we want a policy toolkit that will allow us to respond to a wide range of possible conditions. The Pre-Crisis Toolkit Prior to the financial crisis, the Federal Reserve\\'s monetary policy toolkit was simple but effective in the circumstances that then prevailed. Our main tool consisted of open market operations to manage the amount of reserve balances available to the banking sector.3 These operations, in turn, influenced the interest rate in the federal funds market, where banks experiencing reserve shortfalls could borrow from banks with excess reserves. Before the onset of the crisis, the volume of reserves was generally small--only about $45 billion or so.4 Thus, even small open market operations could have a significant effect on the federal funds rate. Changes in the federal funds rate would then be transmitted to other short-term interest rates, affecting longer-term interest rates and overall financial conditions and hence inflation and economic activity. This simple, light-touch system allowed the Federal Reserve to operate with a relatively small balance sheet--less than $1 trillion before the crisis--the size of which was largely determined by the need to supply enough U.S. currency to meet demand.5 The global financial crisis revealed two main shortcomings of this simple toolkit. The first was an inability to control the federal funds rate once reserves were no longer relatively scarce. Starting in late 2007, faced with acute financial market distress, the Federal Reserve created programs to keep credit flowing to households and businesses.6 The loans extended under those programs helped stabilize the financial system. But the additional reserves created by these programs, if left unchecked, would have pushed down the federal funds rate, driving it well below the FOMC\\'s target. To prevent such an outcome, the Federal Reserve took several steps to offset (or sterilize) the effect of its liquidity and credit operations on reserves.7 By the fall of 2008, however, the reserve effects of our liquidity and credit programs threatened to become too large to sterilize via asset sales and other existing tools. Without sufficient sterilization capacity, the quantity of reserves increased to a point that the Federal Reserve had difficulty maintaining effective control over the federal funds rate. Of course, by the end of 2008, stabilizing the federal funds rate at a level materially above zero was not an immediate concern because the economy clearly needed very low short-term interest rates. Faced with a steep rise in unemployment and declining inflation, the FOMC lowered its target for the federal funds rate to near zero, a reduction of roughly 5 percentage points over the previous year and a half. Nonetheless, a variety of policy benchmarks would, at least in hindsight, have called for pushing the federal funds rate well below zero during the economic downturn.8 That doing so was impossible highlights the second serious limitation of our pre-crisis policy toolkit: its inability to generate substantially more accommodation than could be provided by a near-zero federal funds rate. Our Expanded Toolkit To address the challenges posed by the financial crisis and the subsequent severe recession and slow recovery, the Federal Reserve significantly expanded its monetary policy toolkit. In 2006, the Congress had approved plans to allow the Fed, beginning in 2011, to pay interest on banks\\' reserve balances.9 In the fall of 2008, the Congress moved up the effective date of this authority to October 2008. That authority was essential. Paying interest on reserve balances enables the Fed to break the strong link between the quantity of reserves and the level of the federal funds rate and, in turn, allows the Federal Reserve to control short-term interest rates when reserves are plentiful. In particular, once economic conditions warrant a higher level for market interest rates, the Federal Reserve could raise the interest rate paid on excess reserves--the IOER rate. A higher IOER rate encourages banks to raise the interest rates they charge, putting upward pressure on market interest rates regardless of the level of reserves in the banking sector. While adjusting the IOER rate is an effective way to move market interest rates when reserves are plentiful, federal funds have generally traded below this rate. This relative softness of the federal funds rate reflects, in part, the fact that only depository institutions can earn the IOER rate. To put a more effective floor under short-term interest rates, the Federal Reserve created supplementary tools to be used as needed. For instance, the overnight reverse repurchase agreement (ON RRP) facility is available to a variety of counterparties, including eligible money market funds, government-sponsored enterprises, broker-dealers, and depository institutions. Through it, eligible counterparties may invest funds overnight with the Federal Reserve at a rate determined by the FOMC. Similar to the payment of IOER, the ON RRP facility discourages participating institutions from lending at a rate substantially below that offered by the Fed.10 Our current toolkit proved effective last December. In an environment of superabundant reserves, the FOMC raised the effective federal funds rate--that is, the weighted average rate on federal funds transactions among participants in that market--by the desired amount, and we have since maintained the federal funds rate in its target range. Two other major additions to the Fed\\'s toolkit were large-scale asset purchases and increasingly explicit forward guidance.11 Both were used to provide additional monetary policy accommodation after short-term interest rates fell close to zero. Our purchases of Treasury and mortgage-related securities in the open market pushed down longer-term borrowing rates for millions of American families and businesses. Extended forward rate guidance--announcing that we intended to keep short-term interest rates lower for longer than might have otherwise been expected--also put significant downward pressure on longer-term borrowing rates, as did guidance regarding the size and scope of our asset purchases. In light of the slowness of the economic recovery, some have questioned the effectiveness of asset purchases and extended forward rate guidance. But this criticism fails to consider the unusual headwinds the economy faced after the crisis. Those headwinds included substantial household and business deleveraging, unfavorable demand shocks from abroad, a period of contractionary fiscal policy, and unusually tight credit, especially for housing. Studies have found that our asset purchases and extended forward rate guidance put appreciable downward pressure on long-term interest rates and, as a result, helped spur growth in demand for goods and services, lower the unemployment rate, and prevent inflation from falling further below our 2 percent objective.12 Two of the Fed\\'s most important new tools--our authority to pay interest on excess reserves and our asset purchases--interacted importantly. Without IOER authority, the Federal Reserve would have been reluctant to buy as many assets as it did because of the longer-run implications for controlling the stance of monetary policy. While we were buying assets aggressively to help bring the U.S. economy out of a severe recession, we also had to keep in mind whether and how we would be able to remove monetary policy accommodation when appropriate. That issue was particularly relevant because we fund our asset purchases through the creation of reserves, and those additional reserves would have made it ever more difficult for the pre-crisis toolkit to raise short-term interest rates when needed. The FOMC considered removing accommodation by first reducing our asset holdings (including through asset sales) and raising the federal funds rate only after our balance sheet had contracted substantially. But we decided against this approach because our ability to predict the effects of changes in the balance sheet on the economy is less than that associated with changes in the federal funds rate. Excessive inflationary pressures could arise if assets were sold too slowly. Conversely, financial markets and the economy could potentially be destabilized if assets were sold too aggressively. Indeed, the so-called taper tantrum of 2013 illustrates the difficulty of predicting financial market reactions to announcements about the balance sheet. Given the uncertainty and potential costs associated with large-scale asset sales, the FOMC instead decided to begin removing monetary policy accommodation primarily by adjusting short-term interest rates rather than by actively managing its asset holdings.13 That strategy--raising short-term interest rates once the recovery was sufficiently advanced while maintaining a relatively large balance sheet and plentiful bank reserves--depended on our ability to pay interest on excess reserves. Where Do We Go from Here? What does the future hold for the Fed\\'s toolkit? For starters, our ability to use interest on reserves is likely to play a key role for years to come. In part, this reflects the outlook for our balance sheet over the next few years. As the FOMC has noted in its recent statements, at some point after the process of raising the federal funds rate is well under way, we will cease or phase out reinvesting repayments of principal from our securities holdings. Once we stop reinvestment, it should take several years for our asset holdings--and the bank reserves used to finance them--to passively decline to a more normal level. But even after the volume of reserves falls substantially, IOER will still be important as a contingency tool, because we may need to purchase assets during future recessions to supplement conventional interest rate reductions.14 Forecasts now show the federal funds rate settling at about 3 percent in the longer run.15 In contrast, the federal funds rate averaged more than 7 percent between 1965 and 2000. Thus, we expect to have less scope for interest rate cuts than we have had historically. In part, current expectations for a low future federal funds rate reflect the FOMC\\'s success in stabilizing inflation at around 2 percent--a rate much lower than rates that prevailed during the 1970s and 1980s. Another key factor is the marked decline over the past decade, both here and abroad, in the long-run neutral real rate of interest--that is, the inflation-adjusted short-term interest rate consistent with keeping output at its potential on average over time.16 Several developments could have contributed to this apparent decline, including slower growth in the working-age populations of many countries, smaller productivity gains in the advanced economies, a decreased propensity to spend in the wake of the financial crises around the world since the late 1990s, and perhaps a paucity of attractive capital projects worldwide.17 Although these factors may help explain why bond yields have fallen to such low levels here and abroad, our understanding of the forces driving long-run trends in interest rates is nevertheless limited, and thus all predictions in this area are highly uncertain.18 Would an average federal funds rate of about 3 percent impair the Fed\\'s ability to fight recessions? Based on the FOMC\\'s behavior in past recessions, one might think that such a low interest rate could substantially impair policy effectiveness. As shown in the first column of the table in the handout, during the past nine recessions, the FOMC cut the federal funds rate by amounts ranging from about 3 percentage points to more than 10 percentage points. On average, the FOMC reduced rates by about 5-1/2 percentage points, which seems to suggest that the FOMC would face a shortfall of about 2-1/2 percentage points for dealing with an average-sized recession. But this simple comparison exaggerates the limitations on policy created by the zero lower bound. As shown in the second column, the federal funds rate at the start of the past seven recessions was appreciably above the level consistent with the economy operating at potential in the longer run. In most cases, this tighter-than-normal stance of policy before the recession appears to have reflected some combination of initially higher-than-normal labor utilization and elevated inflation pressures. As a result, a large portion of the rate cuts that subsequently occurred during these recessions represented the undoing of the earlier tight stance of monetary policy. Of course, this situation could occur again in the future. But if it did, the federal funds rate at the onset of the recession would be well above its normal level, and the FOMC would be able to cut short-term interest rates by substantially more than 3 percentage points. A recent paper takes a different approach to assessing the FOMC\\'s ability to respond to future recessions by using simulations of the FRB/US model.19 This analysis begins by asking how the economy would respond to a set of highly adverse shocks if policymakers followed a fairly aggressive policy rule, hypothetically assuming that they can cut the federal funds rate without limit.20 It then imposes the zero lower bound and asks whether some combination of forward guidance and asset purchases would be sufficient to generate economic conditions at least as good as those that occur under the hypothetical unconstrained policy. In general, the study concludes that, even if the average level of the federal funds rate in the future is only 3 percent, these new tools should be sufficient unless the recession were to be unusually severe and persistent. Figure 2 in your handout illustrates this point. It shows simulated paths for interest rates, the unemployment rate, and inflation under three different monetary policy responses--the aggressive rule in the absence of the zero lower bound constraint, the constrained aggressive rule, and the constrained aggressive rule combined with $2 trillion in asset purchases and guidance that the federal funds rate will depart from the rule by staying lower for longer.21 As the red dashed line shows, the federal funds rate would fall far below zero if policy were unconstrained, thereby causing long-term interest rates to fall sharply.i But despite the lower bound, asset purchases and forward guidance can push long-term interest rates even lower on average than in the unconstrained case (especially when adjusted for inflation) by reducing term premiums and increasing the downward pressure on the expected average value of future short-term interest rates. Thus, the use of such tools could result in even better outcomes for unemployment and inflation on average. Of course, this analysis could be too optimistic. For one, the FRB/US simulations may overstate the effectiveness of forward guidance and asset purchases, particularly in an environment where long-term interest rates are also likely to be unusually low.22 In addition, policymakers could have less ability to cut short-term interest rates in the future than the simulations assume. By some calculations, the real neutral rate is currently close to zero, and it could remain at this low level if we were to continue to see slow productivity growth and high global saving.23 If so, then the average level of the nominal federal funds rate down the road might turn out to be only 2 percent, implying that asset purchases and forward guidance might have to be pushed to extremes to compensate.24 Moreover, relying too heavily on these nontraditional tools could have unintended consequences. For example, if future policymakers responded to a severe recession by announcing their intention to keep the federal funds rate near zero for a very long time after the economy had substantially recovered and followed through on that guidance, then they might inadvertently encourage excessive risk-taking and so undermine financial stability. Finally, the simulation analysis certainly overstates the FOMC\\'s current ability to respond to a recession, given that there is little scope to cut the federal funds rate at the moment. But that does not mean that the Federal Reserve would be unable to provide appreciable accommodation should the ongoing expansion falter in the near term. In addition to taking the federal funds rate back down to nearly zero, the FOMC could resume asset purchases and announce its intention to keep the federal funds rate at this level until conditions had improved markedly--although with long-term interest rates already quite low, the net stimulus that would result might be somewhat reduced. Despite these caveats, I expect that forward guidance and asset purchases will remain important components of the Fed\\'s policy toolkit. In addition, it is critical that the Federal Reserve and other supervisory agencies continue to do all they can to ensure a strong and resilient financial system. That said, these tools are not a panacea, and future policymakers could find that they are not adequate to deal with deep and prolonged economic downturns. For these reasons, policymakers and society more broadly may want to explore additional options for helping to foster a strong economy. On the monetary policy side, future policymakers might choose to consider some additional tools that have been employed by other central banks, though adding them to our toolkit would require a very careful weighing of costs and benefits and, in some cases, could require legislation. For example, future policymakers may wish to explore the possibility of purchasing a broader range of assets. Beyond that, some observers have suggested raising the FOMC\\'s 2 percent inflation objective or implementing policy through alternative monetary policy frameworks, such as price-level or nominal GDP targeting. I should stress, however, that the FOMC is not actively considering these additional tools and policy frameworks, although they are important subjects for research. Beyond monetary policy, fiscal policy has traditionally played an important role in dealing with severe economic downturns. A wide range of possible fiscal policy tools and approaches could enhance the cyclical stability of the economy.25 For example, steps could be taken to increase the effectiveness of the automatic stabilizers, and some economists have proposed that greater fiscal support could be usefully provided to state and local governments during recessions. As always, it would be important to ensure that any fiscal policy changes did not compromise long-run fiscal sustainability. Finally, and most ambitiously, as a society we should explore ways to raise productivity growth. Stronger productivity growth would tend to raise the average level of interest rates and therefore would provide the Federal Reserve with greater scope to ease monetary policy in the event of a recession. But more importantly, stronger productivity growth would enhance Americans\\' living standards. Though outside the narrow field of monetary policy, many possibilities in this arena are worth considering, including improving our educational system and investing more in worker training; promoting capital investment and research spending, both private and public; and looking for ways to reduce regulatory burdens while protecting important economic, financial, and social goals. Conclusion Although fiscal policies and structural reforms can play an important role in strengthening the U.S. economy, my primary message today is that I expect monetary policy will continue to play a vital part in promoting a stable and healthy economy. New policy tools, which helped the Federal Reserve respond to the financial crisis and Great Recession, are likely to remain useful in dealing with future downturns. Additional tools may be needed and will be the subject of research and debate. But even if average interest rates remain lower than in the past, I believe that monetary policy will, under most conditions, be able to respond effectively.    1. The June 2016 Summary of Economic Projections (SEP) is an addendum to the minutes of the June 2016 FOMC meeting and is available on the Board\\'s website at www.federalreserve.gov/monetarypolicy/files/fomcminutes20160615.pdf. Return to text 2. The confidence interval equals (subject to a lower bound of 12.5 basis points) the median SEP path for the federal funds rate plus or minus average root mean squared prediction errors (RMSPEs) of the three-month Treasury bill rate, for horizons from zero to nine quarters ahead, based on forecast errors made over the past 20 years. Average RMSPEs are calculated as the mean of the RMSPEs of the following forecasters, subject to availability for the horizon in question: the Federal Reserve Board staff (Greenbook/Tealbook), the Administration, the Congressional Budget Office, the Blue Chip consensus forecast, and the Survey of Professional Forecasters. Differences in predictive accuracy among these forecasters are not statistically significant. For more information on the general methodology used to construct confidence intervals using historical forecasting errors, see David Reifschneider and Peter Tulip (2007), \"Gauging the Uncertainty of the Economic Outlook from Historical Forecasting Errors (PDF),\" Finance and Economics Discussion Series 2007-60 (Washington: Board of Governors of the Federal Reserve System, November). Return to text 3. Open market operations at the time were primarily repurchase agreements based on Treasury securities, with primary dealers as counterparties. Return to text 4. Reserves of depository institutions include vault cash and balances maintained with Federal Reserve Banks. Excess reserves are the reserves held over and above required reserves. See the Board\\'s webpage \"Reserve Requirements\" at www.federalreserve.gov/monetarypolicy/reservereq.htm. Return to text 5. Prior to the financial crisis, the size of the Fed\\'s balance sheet was about $900 billion. Assets consisted almost entirely of Treasury securities. Liabilities included currency held by the public and a relatively small volume of reserve balances. For more on the Fed\\'s balance sheet, see www.federalreserve.gov/monetarypolicy/bst_fedsbalancesheet.htm. Return to text 6. For information on the Federal Reserve\\'s credit and liquidity programs that were implemented in response to the financial crisis, see www.federalreserve.gov/monetarypolicy/bst_crisisresponse.htm on the Board\\'s website. Return to text 7. Reserves were initially taken out of the banking system by not reinvesting principal payments from maturing securities and later by selling portions of securities holdings. In September 2008, the Department of the Treasury announced the temporary Supplementary Financing Program, in which the proceeds of a series of Treasury bill auctions, separate from Treasury\\'s routine borrowing, were maintained in an account at the Federal Reserve Bank of New York. The funds in this account served to drain reserves from the banking system. Return to text 8. Consider the following policy rule: R(t) = R* + p(t) + 0.5[p(t)-p*]-2.0[U(t)-U*], where R is the federal funds rate, R* is the longer-run normal value of the federal funds rate adjusted for inflation, Ã\\x80 is the four-quarter moving average of core PCE inflation, Ã\\x80* is the FOMC\\'s target for inflation (2 percent), U is the unemployment rate, and U* is the longer-run normal rate of unemployment. Based on the medians of FOMC participants\\' latest longer-run projections, R* is approximately 1 percent and U* is about 4.8 percent. Accordingly, with the unemployment rate climbing to 10 percent and core PCE inflation falling to 1 percent in 2009, this rule would have prescribed lowering the federal funds rate to minus 9 percent at the depths of the recession. In contrast, the standard Taylor rule, which is half as responsive to movements in resource utilization, would have prescribed lowering the federal funds rate to minus 3-3/4 percent using the same estimates for R* and U*. The more aggressive rule does a reasonably good job of accounting for movements in the federal funds rate in the decade prior to its falling to its effective lower bound in late 2008, see David Reifschneider (2016), \"Gauging the Ability of the FOMC to Respond to Future Recessions (PDF),\" Finance and Economics Discussion Series 2016-068 (Washington: Board of Governors of the Federal Reserve System, August). For more information on the standard Taylor rule, see John B. Taylor (1993), \"Discretion versus Policy Rules in Practice,\" Carnegie-Rochester Conference Series on Public Policy, vol. 39 (December), pp. 195-214. Return to text 9. Paying interest on reserves is a tool commonly used by central banks, including the Bank of England, the Bank of Japan, and the European Central Bank. Return to text 10. Other tools that could help strengthen the floor under short-term interest rates but are not currently in use include the Term Deposit Facility and term reverse repurchase agreements. Return to text 11. Prior to the crisis, the Fed occasionally used forward guidance pertaining to the likely future path of interest rates, but that guidance was usually confined to a relatively short time frame. Return to text 12. See, for instance, Joseph Gagnon, Matthew Raskin, Julie Remache, and Brian Sack (2011), \"The Financial Market Effects of the Federal Reserve\\'s Large-Scale Asset Purchases,\"  International Journal of Central Banking, vol. 7 (March), pp. 3-43; and Stefania D\\'Amico, William English, David LÃ³pez-Salido, and Edward Nelson (2012), \"The Federal Reserve\\'s Large-Scale Asset Purchase Programmes: Rationale and Effects,\" Economic Journal, vol. 122 (November), pp. F415-46. Moreover, the Federal Reserve\\'s forward guidance and asset purchase policies have been estimated to have helped lower unemployment and boost inflation; see Eric M. Engen, Thomas Laubach, and David Reifschneider (2015), \"The Macroeconomic Effects of the Federal Reserve\\'s Unconventional Monetary Policies,\" Finance and Economics Discussion Series 2015-005 (Washington: Board of Governors of the Federal Reserve System, January). Return to text 13. The FOMC\\'s \"Policy Normalization Principles and Plans\" call for reducing the Federal Reserve\\'s securities holdings in a \"gradual and predictable manner primarily by ceasing to reinvest repayments of principal on securities held in the [System Open Market Account]\" (Board of Governors of the Federal Reserve System (2014), \"Federal Reserve Issues FOMC Statement on Policy Normalization Principles and Plans,\" press release, September 17, second bullet). Consistent with those plans, the Federal Open Market Committee anticipates that it will maintain its current reinvestment strategy \"until normalization of the level of the federal funds rate is well under way\" (for instance, see Board of Governors of the Federal Reserve System (2015), \"Federal Reserve Issues FOMC Statement,\" press release, December 16, paragraph 5. Return to text 14. If the FOMC were to again increase the size of the balance sheet markedly in response to a future recession, then the ability to pay interest on reserves could be critical during the subsequent recovery period to help control short-term interest rates while the balance sheet remains elevated. Beyond this motivation for retaining IOER, the ability to pay interest on reserves could also be important to the operation of any special liquidity and credit facilities that might be created to deal with systemic disruptions to the financial system during a future emergency. In particular, such facilities could significantly expand the supply of reserves, which would be problematic if the FOMC wished to keep short-term interest rates from falling to zero. Return to text 15. In the Blue Chip Financial Indicators survey released on June 1, 2016, the consensus forecast for the longer-run level of the federal funds rate was 3.2 percent. FOMC participants in June 2016 generally anticipated a slightly lower longer-run level, in that the median of their individual forecasts was 3 percent (see table 1 of the June 2016 SEP, available at www.federalreserve.gov/monetarypolicy/fomcminutes20160615ep.htm). The latest long-run forecast from the Administration (www.whitehouse.gov/sites/default/files/omb/budget/fy2016/assets/16msr.pdf) is also close to 3 percent, as was the projection made by the Congressional Budget Office earlier in the year (see www.cbo.gov/about/products/budget_economic_data). Return to text 16. Updated estimates from the model developed by Laubach and Williams (2003) indicate that the real long-run neutral or \"equilibrium\" short-term interest rate in the United States is currently about 2-1/2 percentage points lower than it was on average in the 1980s and 1990s (see Thomas Laubach and John C. Williams (2003), \"Measuring the Natural Rate of Interest,\" Review of Economics and Statistics, vol. 85 (November), pp. 1063-70; updated estimates are available at www.frbsf.org/economic-research/economists/john-williams/Laubach_Williams_updated_estimates.xlsx.)  In addition, Holston, Laubach, and Williams (2016) find significant but somewhat smaller declines in equilibrium rates for the euro area, Canada, and the United Kingdom (see Kathryn Holston, Thomas Laubach, and John C. Williams (2016), \"Measuring the Natural Rate of Interest: International Trends and Determinants,\" Working Paper 2016-11 (San Francisco: Federal Reserve Bank of San Francisco, June), www.frbsf.org/economic-research/files/wp2016-11.pdf).  Return to text 17. For a discussion of the possible role played by these factors in explaining the current low level of interest rates in the United States and other advanced economies, see Lawrence H. Summers (2014), \"U.S. Economic Prospects: Secular Stagnation, Hysteresis, and the Zero Lower Bound,\" Business Economics, vol. 49 (April), pp. 65-73; Robert J. Gordon (2014), \"The Demise of U.S. Economic Growth: Restatement, Rebuttal, and Reflections,\"  NBER Working Paper 19895 (Cambridge, Mass.: National Bureau of Economic Research, February); and Ben S. Bernanke (2015), \"Why Are Interest Rates So Low, Part 2: Secular Stagnation,\"  Ben Bernanke\\'s Blog, blog post (Washington: Brookings Institution, March 31). Return to text 18. For example, see James D. Hamilton, Ethan S. Harris, Jan Hatzius, and Kenneth D. West (2015), \"The Equilibrium Real Funds Rate: Past, Present, and Future,\"  NBER Working Paper 21476 (Cambridge, Mass.: National Bureau of Economic Research, August); and Olivier Blanchard (2016), \"Three Remarks on the U.S. Treasury Yield Curve,\"  Realtime Economic Issues Watch (Washington: Peterson Institute for International Economics, June 22). Return to text 19. FRB/US model simulations have several advantages for analyzing this issue. For one, the model\\'s structure allows the public\\'s expectations for interest rates, inflation, and other factors to take full account of the implications of the effective lower bound on nominal interest rates, changes in future monetary policy as signaled by forward guidance, and asset purchases. In addition, the model incorporates the low responsiveness of inflation to movements in resource utilization seen in recent years as well as the effects of asset purchases on term premiums, and thus a variety of longer-term interest rates, equity prices, and the foreign exchange value of the dollar. For a further discussion about the advantages (and possible disadvantages) of using the FRB/US model to study this issue, see Reifschneider, \"Gauging the Ability of the FOMC to Respond to Future Recessions,\" in note 8. Return to text 20. The aggressive rule is R(t) = 1.0 + p(t) + 0.5 [p(t)-2] - 2.0 [U(t)-4.8], where R is the federal funds rate, Ã\\x80 is the four-quarter moving average of core PCE inflation, and U is the unemployment rate [Note: On August 30, 2016, a typo in the equation for the aggressive rule was corrected to change \"[4.8 - U(t)]\" to \"[U(t) - 4.8]\"]. Note that baseline values of the equilibrium real rate, the natural rate of unemployment, and the target rate of inflation used in the simulation analysis--1.0 percent, 4.8 percent, and 2.0 percent, respectively--are consistent with the medians of the latest long-run projections of individual FOMC participants. As discussed by Taylor (1999), this rule appears to do a good job in stabilizing real activity and inflation in a wide range of economic models (see John B. Taylor (1999), \"Introduction,\" in John B. Taylor, ed., Monetary Policy Rules (Chicago: University of Chicago Press), pp. 1-14). Return to text 21. The forward guidance is provided at the start of the recession and has three components. First, the federal funds rate will be lowered to zero more quickly than prescribed by the rule. Second, the federal funds rate will remain at zero as long as the unemployment rate is greater than 5 percent. And, finally, that after the initial increase in the federal funds rate, policymakers will proceed gradually in returning to the prescriptions of the policy rule. Return to text 22. As shown in figure 2, the 10-year Treasury yield in the simulation starts out at just over 4 percent, well below its level pre-crisis, suggesting that there may be less room to push down long-term interest rates in the future than in the past. Another potential source of overstatement could be the FRB/US assumption that changes in long-term interest rates, whether driven by shifts in term premiums or shifts in the expected path of short-term interest rates, have the same influence on real activity, as there is some empirical evidence that the estimated sensitivity of spending to movements in term premiums alone may be relatively small; see Michael T. Kiley (2014), \"The Aggregate Demand Effects of Short- and Long-Term Interest Rates,\"  International Journal of Central Banking, vol. 10 (December), pp. 69-104. On the other hand, the effectiveness of forward guidance in the FRB/US model is materially less than it is in some other models, implying that the FRB/US simulation results could potentially understate the stimulus provided by the announcement of a lower-for-longer policy. See Hess Chung (2015), \"The Effects of Forward Guidance in Three Macro Models,\" FEDS Notes (Washington: Board of Governors of the Federal Reserve System, February 26). Return to text 23. In principle, the federal funds rate in the longer run could also turn out to be lower than currently predicted if inflation were to remain persistently below 2 percent. However, because a higher rate of inflation can arguably be achieved over time through a sufficiently tight labor market, this risk seems low to me as long as the Federal Reserve is committed to achieving its inflation objective. Return to text 24. In the simulations reported by Reifschneider, \"Gauging the Ability of the FOMC to Respond to Future Recessions,\" in note 8, overcoming the effects of the zero lower bound during a severe recession would require about $4 trillion in asset purchases and pledging to stay low for even longer if the average future level of the federal funds rate is only 2 percent. Return to text 25. For further discussion of ways to enhance the effectiveness of fiscal policy in stabilizing the economy, see Xavier Debrun and Radhicka Kapoor (2010), \"Fiscal Policy and Macroeconomic Stability: Automatic Stabilizers Work, Always and Everywhere (PDF),\"  IMF Working Paper WP/10/111 (Washington: International Monetary Fund, May); Antonio FatÃ¡s and Ilian Mihov (2012), \"Fiscal Policy as a Stabilization Tool,\" B.E. Journal of Macroeconomics, vol. 12 (October), pp. 1-66; International Monetary Fund (2015), \"Can Fiscal Policy Stabilize Output? (PDF)\"  chapter 2 in Fiscal Monitor (Washington: IMF, April), pp. 21-48; and Alisdair McKay and Ricardo Reis (2016), \"The Role of Automatic Stabilizers in the U.S. Business Cycle,\" Econometrica, vol. 84 (January), pp. 141-94. Return to text i. Note: On August 29, 2016, a typo was corrected to change the line color cited from Ã¢\\x80\\x9cblueÃ¢\\x80\\x9d to Ã¢\\x80\\x9credÃ¢\\x80\\x9d in the following sentence: Ã¢\\x80\\x9cAs the red dashed line shows, the federal funds rate would fall far below zero if policy were unconstrained, thereby causing long-term interest rates to fall sharply.Ã¢\\x80\\x9d Return to text     View speech charts and figures  Accessible Version    ',\n",
       "   ''],\n",
       "  tensor([ 1.0800, -1.1973, -0.6084,  1.7277,  0.1911,  0.1618, -0.9417, -1.2882,\n",
       "           0.0537,  0.2673,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000])),\n",
       " 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index - 9                      1.079978\n",
       "Index - 8                     -1.197337\n",
       "Index - 7                     -0.608429\n",
       "Index - 6                      1.727739\n",
       "Index - 5                       0.19114\n",
       "Index - 4                      0.161762\n",
       "Index - 3                     -0.941655\n",
       "Index - 2                     -1.288233\n",
       "Index - 1                      0.053673\n",
       "Index - 0                      0.267276\n",
       "index ecb                           309\n",
       "index fed                           492\n",
       "Index Name_CVIX Index                 0\n",
       "Index Name_EURUSD Curncy              0\n",
       "Index Name_EURUSDV1M Curncy           1\n",
       "Index Name_MOVE Index                 0\n",
       "Index Name_SPX Index                  0\n",
       "Index Name_SRVIX Index                0\n",
       "Index Name_SX5E Index                 0\n",
       "Index Name_V2X Index                  0\n",
       "Index Name_VIX Index                  0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_print(text, line_char_lim=150):\n",
    "    text_ = text.split('\\n')\n",
    "    for subtext in text_:\n",
    "        n = len(subtext)\n",
    "        k = 0\n",
    "        while k <= n:\n",
    "            print(subtext[k:min(n, k + line_char_lim)])\n",
    "            k += line_char_lim\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = MyModel(method=config[\"method\"],\n",
    "                layers=config[\"layers\"],\n",
    "                mlp_hidden_dim=config[\"mlp_hidden_dim\"],\n",
    "                separate=config[\"separate\"],\n",
    "                dropout=config[\"dropout\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (nontext_network): NontextualNetwork()\n",
       "  (corpus_encoder): CorpusEncoder(\n",
       "    (encoder): CorpusEncoder(\n",
       "      (doc_encoder): DocumentEncoder(\n",
       "        (text_encoder): DistilBertModel(\n",
       "          (embeddings): Embeddings(\n",
       "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "            (position_embeddings): Embedding(512, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (transformer): Transformer(\n",
       "            (layer): ModuleList(\n",
       "              (0): TransformerBlock(\n",
       "                (attention): MultiHeadSelfAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (ffn): FFN(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (activation): GELUActivation()\n",
       "                )\n",
       "                (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (1): TransformerBlock(\n",
       "                (attention): MultiHeadSelfAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (ffn): FFN(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (activation): GELUActivation()\n",
       "                )\n",
       "                (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (2): TransformerBlock(\n",
       "                (attention): MultiHeadSelfAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (ffn): FFN(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (activation): GELUActivation()\n",
       "                )\n",
       "                (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (3): TransformerBlock(\n",
       "                (attention): MultiHeadSelfAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (ffn): FFN(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (activation): GELUActivation()\n",
       "                )\n",
       "                (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (4): TransformerBlock(\n",
       "                (attention): MultiHeadSelfAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (ffn): FFN(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (activation): GELUActivation()\n",
       "                )\n",
       "                (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (5): TransformerBlock(\n",
       "                (attention): MultiHeadSelfAttention(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (ffn): FFN(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (activation): GELUActivation()\n",
       "                )\n",
       "                (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (W): Linear(in_features=768, out_features=32, bias=True)\n",
       "      (pooling): Pooling()\n",
       "    )\n",
       "  )\n",
       "  (classifier): ClassificationHead(\n",
       "    (mlp): SimpleMLP(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=787, out_features=128, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0, inplace=False)\n",
       "        (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Dropout(p=0, inplace=False)\n",
       "        (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (7): ReLU()\n",
       "        (8): Dropout(p=0, inplace=False)\n",
       "        (9): Linear(in_features=128, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_text': tensor([[[ 101, 1999, 2023,  ..., 2011, 2216,  102],\n",
       "          [ 101,  102,    0,  ...,    0,    0,    0],\n",
       "          [ 101, 1996, 3795,  ..., 1010, 3105,  102],\n",
       "          [ 101,  102,    0,  ...,    0,    0,    0]],\n",
       " \n",
       "         [[ 101, 2256, 2490,  ..., 3296, 3930,  102],\n",
       "          [ 101,  102,    0,  ...,    0,    0,    0],\n",
       "          [ 101, 3361, 9211,  ..., 8485, 2015,  102],\n",
       "          [ 101,  102,    0,  ...,    0,    0,    0]]]),\n",
       " 'X_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 0,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 0,  ..., 0, 0, 0]]]),\n",
       " 'X_ind': tensor([[ 1.0800, -1.1973, -0.6084,  1.7277,  0.1911,  0.1618, -0.9417, -1.2882,\n",
       "           0.0537,  0.2673,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [-0.8345,  0.7644, -0.1862, -0.0223, -2.5006,  0.4220,  0.2116,  0.4965,\n",
       "           0.8059, -1.5744,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]),\n",
       " 'label': tensor([0., 1.])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First text of first ecb corpus of batch\n",
    "# batch['X_text'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECB texts\n",
    "# tokenizer(train_set[0][0][0], padding='max_length', max_length=512, truncation=True)[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test output\n",
    "# with torch.no_grad():\n",
    "#     my_model.eval()\n",
    "#     batch = next(iter(train_loader))\n",
    "#     print(batch)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         X_ind, y = batch\n",
    "#         my_model_output = my_model(None, None, X_ind.float().to(device))\n",
    "\n",
    "#     print(my_model_output.size(0)/64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationHead(\n",
       "  (mlp): SimpleMLP(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=787, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0, inplace=False)\n",
       "      (6): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0, inplace=False)\n",
       "      (9): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:42<00:00,  1.04s/batch, accuracy=45.7, batch_loss=0.728, loss=0.708]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:32<00:00,  1.27batch/s, accuracy=42, batch_loss=0.704, loss=0.7]    \n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:30<00:00,  1.34batch/s, accuracy=53.1, batch_loss=0.73, loss=0.695] \n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:30<00:00,  1.36batch/s, accuracy=53.1, batch_loss=0.724, loss=0.694]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:31<00:00,  1.31batch/s, accuracy=53.1, batch_loss=0.734, loss=0.693]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:30<00:00,  1.34batch/s, accuracy=53.1, batch_loss=0.722, loss=0.694]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:37<00:00,  1.11batch/s, accuracy=53.1, batch_loss=0.723, loss=0.692]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:36<00:00,  1.11batch/s, accuracy=53.1, batch_loss=0.722, loss=0.692]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:35<00:00,  1.15batch/s, accuracy=53.1, batch_loss=0.716, loss=0.693]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:37<00:00,  1.08batch/s, accuracy=53.1, batch_loss=0.715, loss=0.691]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:37<00:00,  1.10batch/s, accuracy=53.1, batch_loss=0.721, loss=0.691]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:36<00:00,  1.12batch/s, accuracy=53.1, batch_loss=0.698, loss=0.689]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:36<00:00,  1.12batch/s, accuracy=54.3, batch_loss=0.677, loss=0.686]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:37<00:00,  1.10batch/s, accuracy=54.3, batch_loss=0.668, loss=0.682]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:37<00:00,  1.11batch/s, accuracy=61.7, batch_loss=0.621, loss=0.672]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:37<00:00,  1.09batch/s, accuracy=64.2, batch_loss=0.539, loss=0.655]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:37<00:00,  1.08batch/s, accuracy=61.7, batch_loss=0.479, loss=0.66] \n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:36<00:00,  1.12batch/s, accuracy=65.4, batch_loss=0.411, loss=0.621]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:36<00:00,  1.13batch/s, accuracy=67.9, batch_loss=0.322, loss=0.606]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:37<00:00,  1.10batch/s, accuracy=71.6, batch_loss=0.279, loss=0.55]  \n"
     ]
    }
   ],
   "source": [
    "epochs = config[\"max_epochs\"]\n",
    "lr = config[\"learning_rate\"]\n",
    "method = config[\"method\"]\n",
    "optimizer = Adam(model.parameters(),\n",
    "                    lr=config[\"learning_rate\"],\n",
    "                    weight_decay=config[\"weight_decay\"])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    total_loss = 0\n",
    "    total_entries = 0\n",
    "    correct = 0\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get inputs\n",
    "            if method is None:\n",
    "                X_ind, y_ = batch\n",
    "                X_ind = torch.Tensor(X_ind).float().to(device)\n",
    "                y_ = torch.Tensor(y_).float().to(device)\n",
    "                \n",
    "                X_text = None\n",
    "                X_mask = None\n",
    "            else:\n",
    "                X_ind = batch[\"X_ind\"].to(device)\n",
    "                y_ = batch[\"label\"].to(device)\n",
    "\n",
    "                if config[\"separate\"]:\n",
    "                    X_ecb = batch[\"X_ecb\"].to(device)\n",
    "                    X_ecb_att = batch[\"X_ecb_mask\"].to(device)\n",
    "                    X_fed = batch[\"X_fed\"].to(device)\n",
    "                    X_fed_att = batch[\"X_fed_mask\"].to(device)\n",
    "\n",
    "                    X_text = (X_ecb, X_fed)\n",
    "                    X_mask = (X_ecb_att, X_fed_att)\n",
    "                else:\n",
    "                    X_text = (batch[\"X_text\"].to(device),)\n",
    "                    X_mask = (batch[\"X_mask\"].to(device),)\n",
    "            \n",
    "            # Compute output\n",
    "            output = model(X_text, X_mask, X_ind)\n",
    "            # print(output)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, y_)\n",
    "            \n",
    "            # Update model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Computing predictions\n",
    "\n",
    "            ## Batch loss\n",
    "            batch_loss = loss.item()\n",
    "\n",
    "            # Accuracy computation\n",
    "            output_proba = sigmoid(output)\n",
    "            batch_size_ = y_.size(0)\n",
    "            preds = output_proba.round()\n",
    "            correct += (preds == y_).sum().item()\n",
    "            ## Total loss with no reduction\n",
    "            total_loss += loss.item() * batch_size_\n",
    "            total_entries += batch_size_\n",
    "            tepoch.set_postfix(loss=total_loss/total_entries,\n",
    "                                accuracy=100. * correct/total_entries,\n",
    "                                batch_loss=batch_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-09 01:12:10,663]\u001b[0m A new study created in memory with name: no-name-06989159-1152-408d-b2ac-beadafbca427\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.25batch/s, accuracy=52.2, loss=0.694]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.37batch/s, accuracy=55.1, loss=0.683]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.39batch/s, accuracy=54.8, loss=0.683]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=54.8, loss=0.681]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.34batch/s, accuracy=55.5, loss=0.68] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.48batch/s, accuracy=56.4, loss=0.681]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.25batch/s, accuracy=56.2, loss=0.678]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.34batch/s, accuracy=56.4, loss=0.677]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=56.2, loss=0.678]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.36batch/s, accuracy=55.7, loss=0.677]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=56.7, loss=0.678]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.48batch/s, accuracy=56.8, loss=0.681]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=56.3, loss=0.677]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.12batch/s, accuracy=55.6, loss=0.679]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.86batch/s, accuracy=56.3, loss=0.676]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.12batch/s, accuracy=56.1, loss=0.676]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.86batch/s, accuracy=57.2, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.20batch/s, accuracy=56.8, loss=0.681]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.27batch/s, accuracy=56.8, loss=0.675]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:23<00:00,  3.52batch/s, accuracy=56.7, loss=0.676]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:26<00:00,  3.23batch/s, accuracy=57.7, loss=0.675]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.18batch/s, accuracy=56.6, loss=0.676]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=56.4, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.43batch/s, accuracy=56.8, loss=0.68] \n",
      "\u001b[32m[I 2023-03-09 01:19:25,151]\u001b[0m Trial 0 finished with value: 0.44857142857142857 and parameters: {'lr_exp': -3.820881786827314, 'weight_decay_exp': -5.1198501828596985, 'layers': 6, 'dropout': 0.381783652210378}. Best is trial 0 with value: 0.44857142857142857.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.28batch/s, accuracy=54.3, loss=0.687]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.25batch/s, accuracy=54.7, loss=0.682]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.11batch/s, accuracy=55.4, loss=0.679]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.26batch/s, accuracy=55.7, loss=0.679]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.36batch/s, accuracy=55.4, loss=0.679]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.64batch/s, accuracy=56.8, loss=0.681]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=56.4, loss=0.676]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=56.5, loss=0.677]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=55.3, loss=0.678]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=55.6, loss=0.679]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=56.8, loss=0.677]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.68batch/s, accuracy=56.6, loss=0.682]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.44batch/s, accuracy=56.3, loss=0.677]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=55.8, loss=0.677]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=56.4, loss=0.677]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=56.7, loss=0.674]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=56.6, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.65batch/s, accuracy=57, loss=0.68]   \n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:22<00:00,  3.71batch/s, accuracy=56.6, loss=0.676]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.27batch/s, accuracy=56.6, loss=0.674]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.35batch/s, accuracy=56.3, loss=0.675]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=57, loss=0.674]  \n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=56.5, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.39batch/s, accuracy=57, loss=0.681]  \n",
      "\u001b[32m[I 2023-03-09 01:26:18,716]\u001b[0m Trial 1 finished with value: 0.4588318085855032 and parameters: {'lr_exp': -3.159445455594589, 'weight_decay_exp': -3.97802436593108, 'layers': 2, 'dropout': 0.43882404892551674}. Best is trial 1 with value: 0.4588318085855032.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.06batch/s, accuracy=52.2, loss=0.697]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.14batch/s, accuracy=54.9, loss=0.682]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=54.9, loss=0.682]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=54.9, loss=0.68] \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=56.7, loss=0.677]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.4, loss=0.677]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=56, loss=0.677]  \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=55.6, loss=0.678]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=56.3, loss=0.678]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=55.5, loss=0.679]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.67batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=56.4, loss=0.676]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=56.1, loss=0.677]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=56.5, loss=0.675]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.2, loss=0.678]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=56.4, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.73batch/s, accuracy=56.8, loss=0.679]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.3, loss=0.676]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=56.5, loss=0.675]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=56.8, loss=0.676]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=57.4, loss=0.674]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.34batch/s, accuracy=57.3, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.64batch/s, accuracy=56.8, loss=0.681]\n",
      "\u001b[32m[I 2023-03-09 01:33:01,510]\u001b[0m Trial 2 finished with value: 0.44857142857142857 and parameters: {'lr_exp': -3.5681933492112323, 'weight_decay_exp': -4.264875561227063, 'layers': 2, 'dropout': 0.4128033413514227}. Best is trial 1 with value: 0.4588318085855032.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=48, loss=0.727]  \n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.92batch/s, accuracy=48, loss=0.727]  \n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.13batch/s, accuracy=48.4, loss=0.724]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:22<00:00,  3.80batch/s, accuracy=48.4, loss=0.724]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.05batch/s, accuracy=47.6, loss=0.725]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.57batch/s, accuracy=45.3, loss=0.702]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=48.7, loss=0.719]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.94batch/s, accuracy=47.6, loss=0.726]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:26<00:00,  3.22batch/s, accuracy=48.2, loss=0.72] \n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.96batch/s, accuracy=49.1, loss=0.719]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=48.8, loss=0.722]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.66batch/s, accuracy=47.6, loss=0.698]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.25batch/s, accuracy=49.9, loss=0.716]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.33batch/s, accuracy=48.9, loss=0.715]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=49.3, loss=0.716]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.15batch/s, accuracy=48.8, loss=0.716]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.97batch/s, accuracy=50.4, loss=0.709]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:07<00:00,  3.89batch/s, accuracy=48.9, loss=0.697]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.11batch/s, accuracy=50.4, loss=0.714]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.33batch/s, accuracy=48.8, loss=0.714]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.31batch/s, accuracy=49.7, loss=0.711]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.22batch/s, accuracy=49.4, loss=0.712]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=49.2, loss=0.712]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.44batch/s, accuracy=50, loss=0.695]  \n",
      "\u001b[32m[I 2023-03-09 01:40:17,684]\u001b[0m Trial 3 finished with value: 0.5382231404958677 and parameters: {'lr_exp': -5.822950909025103, 'weight_decay_exp': -2.6337945891552654, 'layers': 3, 'dropout': 0.5312050050429422}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.17batch/s, accuracy=49.2, loss=0.701]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.99batch/s, accuracy=49.5, loss=0.701]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=50.1, loss=0.699]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.18batch/s, accuracy=51.2, loss=0.697]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.25batch/s, accuracy=51.2, loss=0.696]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.67batch/s, accuracy=53.6, loss=0.693]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=52.2, loss=0.696]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=52.6, loss=0.693]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.28batch/s, accuracy=53.1, loss=0.694]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=53.9, loss=0.69] \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.23batch/s, accuracy=52.1, loss=0.694]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.56batch/s, accuracy=54, loss=0.69]   \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.16batch/s, accuracy=53, loss=0.693]  \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.85batch/s, accuracy=53, loss=0.692]  \n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.23batch/s, accuracy=54, loss=0.689]  \n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.37batch/s, accuracy=53.1, loss=0.691]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=53.3, loss=0.69] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.59batch/s, accuracy=54.5, loss=0.688]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=54.1, loss=0.69] \n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.24batch/s, accuracy=52.8, loss=0.692]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=53, loss=0.69]   \n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=53.7, loss=0.689]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.23batch/s, accuracy=54.8, loss=0.687]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:07<00:00,  3.97batch/s, accuracy=54.6, loss=0.687]\n",
      "\u001b[32m[I 2023-03-09 01:47:18,244]\u001b[0m Trial 4 finished with value: 0.39762611275964393 and parameters: {'lr_exp': -5.357459875725292, 'weight_decay_exp': -4.103904542461522, 'layers': 2, 'dropout': 0.4451446442339658}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.36batch/s, accuracy=53.8, loss=0.694]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=53.8, loss=0.689]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.37batch/s, accuracy=54.8, loss=0.684]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=55.4, loss=0.683]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.28batch/s, accuracy=55.5, loss=0.681]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.73batch/s, accuracy=55.1, loss=0.683]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=54.4, loss=0.683]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.10batch/s, accuracy=55, loss=0.679]  \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=55, loss=0.681]  \n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.17batch/s, accuracy=56.2, loss=0.679]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.29batch/s, accuracy=55.5, loss=0.68] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.54batch/s, accuracy=56.2, loss=0.682]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.37batch/s, accuracy=55.7, loss=0.679]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.31batch/s, accuracy=55.9, loss=0.678]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.23batch/s, accuracy=55.6, loss=0.678]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.09batch/s, accuracy=54.7, loss=0.678]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.26batch/s, accuracy=55.1, loss=0.678]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.65batch/s, accuracy=56.2, loss=0.682]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=56.1, loss=0.677]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.3, loss=0.678]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=55.4, loss=0.678]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.14batch/s, accuracy=56.2, loss=0.677]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=55.4, loss=0.678]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.69batch/s, accuracy=56.2, loss=0.682]\n",
      "\u001b[32m[I 2023-03-09 01:54:13,025]\u001b[0m Trial 5 finished with value: 0.34912718204488774 and parameters: {'lr_exp': -2.9389678706193645, 'weight_decay_exp': -3.4194576502977707, 'layers': 5, 'dropout': 0.6863113347914375}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.26batch/s, accuracy=48.4, loss=0.722]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.36batch/s, accuracy=47.2, loss=0.723]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=47.4, loss=0.725]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.25batch/s, accuracy=47.4, loss=0.721]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.43batch/s, accuracy=47.3, loss=0.721]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.57batch/s, accuracy=44.8, loss=0.704]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=47.9, loss=0.721]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.42batch/s, accuracy=47.5, loss=0.718]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.31batch/s, accuracy=48.3, loss=0.718]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.12batch/s, accuracy=47.3, loss=0.719]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=47.6, loss=0.716]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.44batch/s, accuracy=46.3, loss=0.7]  \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.27batch/s, accuracy=48, loss=0.718]  \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=49.9, loss=0.711]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=50.4, loss=0.706]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.42batch/s, accuracy=48.8, loss=0.711]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.35batch/s, accuracy=47.9, loss=0.712]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.61batch/s, accuracy=47.4, loss=0.698]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.42batch/s, accuracy=49.3, loss=0.708]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=49.4, loss=0.707]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=49, loss=0.708]  \n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.25batch/s, accuracy=50.2, loss=0.705]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.37batch/s, accuracy=49.1, loss=0.707]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.79batch/s, accuracy=47.7, loss=0.696]\n",
      "\u001b[32m[I 2023-03-09 02:01:04,243]\u001b[0m Trial 6 finished with value: 0.43203883495145634 and parameters: {'lr_exp': -5.577460343987779, 'weight_decay_exp': -5.407554492369377, 'layers': 5, 'dropout': 0.5927725280458964}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.33batch/s, accuracy=50.8, loss=0.702]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.29batch/s, accuracy=53.4, loss=0.691]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.61batch/s, accuracy=52.4, loss=0.691]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.64batch/s, accuracy=53.7, loss=0.687]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=54.7, loss=0.683]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.84batch/s, accuracy=56.1, loss=0.681]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.33batch/s, accuracy=55.4, loss=0.677]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.39batch/s, accuracy=55, loss=0.681]  \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.33batch/s, accuracy=56.5, loss=0.676]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=55.1, loss=0.68] \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=55.9, loss=0.679]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.85batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=55.2, loss=0.68] \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.93batch/s, accuracy=55.9, loss=0.679]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.99batch/s, accuracy=55.6, loss=0.679]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=54.8, loss=0.679]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=55.5, loss=0.678]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.59batch/s, accuracy=56.8, loss=0.681]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=55.4, loss=0.677]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=56.8, loss=0.677]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.34batch/s, accuracy=56.1, loss=0.677]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.92batch/s, accuracy=56.6, loss=0.675]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=57.4, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=56.8, loss=0.681]\n",
      "\u001b[32m[I 2023-03-09 02:07:55,114]\u001b[0m Trial 7 finished with value: 0.44857142857142857 and parameters: {'lr_exp': -4.374588747428268, 'weight_decay_exp': -5.8846609384800335, 'layers': 2, 'dropout': 0.29528421835374175}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=54, loss=0.687]  \n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=55.9, loss=0.678]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=55.9, loss=0.677]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=56.7, loss=0.676]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.5, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.79batch/s, accuracy=56.8, loss=0.681]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.43batch/s, accuracy=56.7, loss=0.675]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.26batch/s, accuracy=57.1, loss=0.675]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.20batch/s, accuracy=56.7, loss=0.675]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:24<00:00,  3.45batch/s, accuracy=56.5, loss=0.677]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:23<00:00,  3.57batch/s, accuracy=55.5, loss=0.677]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.50batch/s, accuracy=56.2, loss=0.681]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:22<00:00,  3.71batch/s, accuracy=57, loss=0.674]  \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:23<00:00,  3.61batch/s, accuracy=56.6, loss=0.675]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=56.2, loss=0.677]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=56.9, loss=0.676]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=56.7, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.52batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=56.5, loss=0.676]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=56.8, loss=0.675]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.42batch/s, accuracy=57, loss=0.675]  \n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=56.2, loss=0.676]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=55.9, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.77batch/s, accuracy=56.8, loss=0.68] \n",
      "\u001b[32m[I 2023-03-09 02:14:57,793]\u001b[0m Trial 8 finished with value: 0.44857142857142857 and parameters: {'lr_exp': -3.338971600722692, 'weight_decay_exp': -5.2338714890706015, 'layers': 6, 'dropout': 0.2922305455850151}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.36batch/s, accuracy=54.8, loss=0.686]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.31batch/s, accuracy=55, loss=0.683]  \n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55, loss=0.68]   \n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.1, loss=0.678]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.14batch/s, accuracy=55, loss=0.68]   \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.50batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=56.2, loss=0.678]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=56.5, loss=0.678]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.2, loss=0.678]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=55.9, loss=0.677]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.44batch/s, accuracy=56.2, loss=0.679]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.42batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.11batch/s, accuracy=56.3, loss=0.678]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.93batch/s, accuracy=56.3, loss=0.677]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.28batch/s, accuracy=56.4, loss=0.678]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.1, loss=0.677]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=57.1, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.77batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=56.1, loss=0.679]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.8, loss=0.677]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55.8, loss=0.678]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=56.7, loss=0.677]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=56.8, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.77batch/s, accuracy=56.8, loss=0.679]\n",
      "\u001b[32m[I 2023-03-09 02:21:46,337]\u001b[0m Trial 9 finished with value: 0.44857142857142857 and parameters: {'lr_exp': -3.095712072556521, 'weight_decay_exp': -5.604469016970637, 'layers': 6, 'dropout': 0.49159122171455855}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=55.3, loss=0.686]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.3, loss=0.679]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.44batch/s, accuracy=55.3, loss=0.683]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=55.1, loss=0.682]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=55, loss=0.682]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.77batch/s, accuracy=52.8, loss=0.689]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=54.9, loss=0.684]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.1, loss=0.686]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55.1, loss=0.684]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=55.1, loss=0.685]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55.1, loss=0.684]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=55.1, loss=0.685]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=55.1, loss=0.686]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55.1, loss=0.685]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.1, loss=0.686]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=55.1, loss=0.685]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.1, loss=0.688]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.76batch/s, accuracy=55.1, loss=0.688]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.1, loss=0.688]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=55.1, loss=0.688]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=55.1, loss=0.688]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=55.1, loss=0.688]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.25batch/s, accuracy=55.1, loss=0.688]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.42batch/s, accuracy=55.1, loss=0.688]\n",
      "\u001b[32m[I 2023-03-09 02:28:24,160]\u001b[0m Trial 10 finished with value: 0.0 and parameters: {'lr_exp': -2.1014068268708175, 'weight_decay_exp': -2.0468141907962405, 'layers': 3, 'dropout': 0.5656663261186661}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.11batch/s, accuracy=50.5, loss=0.707]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=51.6, loss=0.701]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=51.7, loss=0.701]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=52.2, loss=0.701]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=51.6, loss=0.701]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.84batch/s, accuracy=53.8, loss=0.689]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=51.4, loss=0.702]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=52.5, loss=0.698]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=53.7, loss=0.694]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.96batch/s, accuracy=52.9, loss=0.694]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.34batch/s, accuracy=54.1, loss=0.691]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.70batch/s, accuracy=54.1, loss=0.686]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=53.4, loss=0.694]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=53.4, loss=0.692]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=53.9, loss=0.691]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=53.5, loss=0.693]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=54.6, loss=0.687]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.72batch/s, accuracy=54.4, loss=0.685]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=52.7, loss=0.693]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=55, loss=0.687]  \n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=54.3, loss=0.689]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=54, loss=0.688]  \n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=54.6, loss=0.686]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.79batch/s, accuracy=54.9, loss=0.684]\n",
      "\u001b[32m[I 2023-03-09 02:35:04,869]\u001b[0m Trial 11 finished with value: 0.2277511961722488 and parameters: {'lr_exp': -4.824566161408088, 'weight_decay_exp': -2.728256612958095, 'layers': 3, 'dropout': 0.5363153981485146}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=53.4, loss=0.696]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=54.7, loss=0.684]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.31batch/s, accuracy=55.6, loss=0.682]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.03batch/s, accuracy=55.2, loss=0.681]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:23<00:00,  3.55batch/s, accuracy=55.2, loss=0.68] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.39batch/s, accuracy=55.1, loss=0.682]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=55.7, loss=0.681]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.6, loss=0.68] \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.62batch/s, accuracy=55.2, loss=0.681]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=55.4, loss=0.68] \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55.9, loss=0.678]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.79batch/s, accuracy=55.1, loss=0.682]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.5, loss=0.68] \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.3, loss=0.68] \n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.6, loss=0.682]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=55.8, loss=0.68] \n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=56, loss=0.678]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.73batch/s, accuracy=56.2, loss=0.681]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=55, loss=0.68]   \n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.7, loss=0.679]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=56.3, loss=0.68] \n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.1, loss=0.68] \n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=56.1, loss=0.68] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.72batch/s, accuracy=56.7, loss=0.681]\n",
      "\u001b[32m[I 2023-03-09 02:41:49,822]\u001b[0m Trial 12 finished with value: 0.22132796780684103 and parameters: {'lr_exp': -2.0391865866779666, 'weight_decay_exp': -3.302082013489639, 'layers': 3, 'dropout': 0.6635071742375033}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=48.1, loss=0.705]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=49.2, loss=0.701]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=51.1, loss=0.699]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=51.5, loss=0.697]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=52.7, loss=0.694]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.71batch/s, accuracy=57.1, loss=0.69] \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=53.4, loss=0.692]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=52.8, loss=0.692]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=54.9, loss=0.688]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=54.8, loss=0.687]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=53.6, loss=0.69] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.77batch/s, accuracy=57.4, loss=0.685]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.4, loss=0.687]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=55, loss=0.686]  \n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55, loss=0.685]  \n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55, loss=0.685]  \n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=55.8, loss=0.682]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.88batch/s, accuracy=57.4, loss=0.682]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=54.8, loss=0.683]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.7, loss=0.681]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=54.4, loss=0.684]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=55.2, loss=0.682]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=55.9, loss=0.682]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.22batch/s, accuracy=57.4, loss=0.681]\n",
      "\u001b[32m[I 2023-03-09 02:48:25,367]\u001b[0m Trial 13 finished with value: 0.36043587594300086 and parameters: {'lr_exp': -4.748796651905899, 'weight_decay_exp': -2.0350168439490206, 'layers': 4, 'dropout': 0.3706267852327655}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=54.4, loss=0.684]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=55.5, loss=0.679]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.7, loss=0.676]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=56.7, loss=0.676]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=57.1, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.80batch/s, accuracy=56.8, loss=0.681]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=56.9, loss=0.676]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=57, loss=0.674]  \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.3, loss=0.674]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.9, loss=0.675]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=57.2, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.77batch/s, accuracy=56.2, loss=0.681]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=56.9, loss=0.675]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=57.1, loss=0.674]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.8, loss=0.674]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=56.4, loss=0.674]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=57.3, loss=0.674]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.69batch/s, accuracy=56.3, loss=0.681]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=56.4, loss=0.675]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=56.8, loss=0.675]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.5, loss=0.675]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=57.4, loss=0.673]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.60batch/s, accuracy=56.2, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.74batch/s, accuracy=56.2, loss=0.68] \n",
      "\u001b[32m[I 2023-03-09 02:55:03,286]\u001b[0m Trial 14 finished with value: 0.34912718204488774 and parameters: {'lr_exp': -2.5578915549961847, 'weight_decay_exp': -3.091637755362537, 'layers': 3, 'dropout': 0.20381028589599476}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=51.1, loss=0.698]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=51.9, loss=0.694]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=54.2, loss=0.687]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=56, loss=0.683]  \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=54.6, loss=0.685]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.72batch/s, accuracy=56.7, loss=0.681]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=54.2, loss=0.685]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=55.1, loss=0.683]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=56, loss=0.68]   \n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=54.2, loss=0.685]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=54.7, loss=0.682]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.55batch/s, accuracy=56.2, loss=0.68] \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=55.8, loss=0.68] \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=55.2, loss=0.679]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.9, loss=0.679]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=55.7, loss=0.68] \n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.7, loss=0.68] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=55.9, loss=0.679]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.8, loss=0.678]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=55.2, loss=0.679]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=55.6, loss=0.681]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.58batch/s, accuracy=55.8, loss=0.681]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.69batch/s, accuracy=56.8, loss=0.68] \n",
      "\u001b[32m[I 2023-03-09 03:01:40,515]\u001b[0m Trial 15 finished with value: 0.44857142857142857 and parameters: {'lr_exp': -4.207459955108831, 'weight_decay_exp': -4.6736711016945565, 'layers': 4, 'dropout': 0.4972995500140744}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=49.3, loss=0.711]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=48.8, loss=0.714]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=49.1, loss=0.712]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=47.8, loss=0.715]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=49.7, loss=0.71] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.81batch/s, accuracy=52, loss=0.695]  \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=49.7, loss=0.707]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=49.7, loss=0.708]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=49.2, loss=0.712]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=49, loss=0.706]  \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=49.3, loss=0.709]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=52, loss=0.694]  \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=50.4, loss=0.706]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=49.9, loss=0.707]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=48.6, loss=0.709]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=49.8, loss=0.704]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=49.4, loss=0.708]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=52.4, loss=0.693]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=49.6, loss=0.704]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=49.4, loss=0.705]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=50.6, loss=0.701]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=50.1, loss=0.705]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=49.8, loss=0.704]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.77batch/s, accuracy=51.7, loss=0.693]\n",
      "\u001b[32m[I 2023-03-09 03:08:18,044]\u001b[0m Trial 16 finished with value: 0.45523329129886503 and parameters: {'lr_exp': -5.99027800623254, 'weight_decay_exp': -2.6734353345955353, 'layers': 2, 'dropout': 0.6046361800598342}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.44batch/s, accuracy=54.4, loss=0.686]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=55.3, loss=0.682]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=54.8, loss=0.681]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=55.6, loss=0.679]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=56, loss=0.679]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.68batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.6, loss=0.677]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.42batch/s, accuracy=55.7, loss=0.677]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=57, loss=0.677]  \n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.9, loss=0.676]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=56.1, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.74batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=57, loss=0.675]  \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=57, loss=0.677]  \n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=56.5, loss=0.674]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.6, loss=0.676]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=56.6, loss=0.674]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.77batch/s, accuracy=56.8, loss=0.679]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.7, loss=0.676]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=56.6, loss=0.677]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=56.1, loss=0.676]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=56.3, loss=0.676]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=57.2, loss=0.675]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=57.5, loss=0.682]\n",
      "\u001b[32m[I 2023-03-09 03:14:57,405]\u001b[0m Trial 17 finished with value: 0.40778210116731517 and parameters: {'lr_exp': -2.6085982574050717, 'weight_decay_exp': -3.8442743877957786, 'layers': 3, 'dropout': 0.4891578912629462}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=50.8, loss=0.707]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=49.6, loss=0.714]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=50.7, loss=0.708]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=50.7, loss=0.707]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=51.2, loss=0.709]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.70batch/s, accuracy=51.6, loss=0.692]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.43batch/s, accuracy=51.7, loss=0.702]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=51.3, loss=0.703]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=51.4, loss=0.704]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=51.4, loss=0.7]  \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=50.5, loss=0.706]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.76batch/s, accuracy=55.7, loss=0.689]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=52.7, loss=0.699]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=52.5, loss=0.699]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.59batch/s, accuracy=52.5, loss=0.699]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=53.3, loss=0.696]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=53.4, loss=0.699]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=55.4, loss=0.688]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=52.8, loss=0.697]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.57batch/s, accuracy=54, loss=0.693]  \n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=54.2, loss=0.695]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=53.4, loss=0.694]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=53.2, loss=0.696]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.71batch/s, accuracy=55.7, loss=0.686]\n",
      "\u001b[32m[I 2023-03-09 03:21:36,016]\u001b[0m Trial 18 finished with value: 0.334453781512605 and parameters: {'lr_exp': -5.013916588123999, 'weight_decay_exp': -4.599423634658208, 'layers': 4, 'dropout': 0.6302865541656238}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=54.2, loss=0.691]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=54.9, loss=0.685]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=55, loss=0.682]  \n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=56.2, loss=0.68] \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=55.6, loss=0.678]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.71batch/s, accuracy=56.9, loss=0.681]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=55.6, loss=0.68] \n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=56, loss=0.677]  \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=55.9, loss=0.678]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=56, loss=0.68]   \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=56.4, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.78batch/s, accuracy=56.8, loss=0.681]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=57.3, loss=0.675]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=55.9, loss=0.678]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=56.3, loss=0.676]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=55.3, loss=0.676]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=56.5, loss=0.676]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.73batch/s, accuracy=56.8, loss=0.68] \n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=55.9, loss=0.676]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=57, loss=0.674]  \n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.28batch/s, accuracy=55.4, loss=0.678]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.28batch/s, accuracy=56.5, loss=0.676]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.28batch/s, accuracy=57, loss=0.677]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.24batch/s, accuracy=56.8, loss=0.681]\n",
      "\u001b[32m[I 2023-03-09 03:28:18,274]\u001b[0m Trial 19 finished with value: 0.44857142857142857 and parameters: {'lr_exp': -3.997558358802013, 'weight_decay_exp': -3.667146780633574, 'layers': 2, 'dropout': 0.3332514016184818}. Best is trial 3 with value: 0.5382231404958677.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:21<00:00,  3.88batch/s, accuracy=46.9, loss=0.723]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.27batch/s, accuracy=47.3, loss=0.721]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=47.5, loss=0.722]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=47.4, loss=0.718]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=47, loss=0.718]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.75batch/s, accuracy=47.8, loss=0.702]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=47.7, loss=0.718]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=47.2, loss=0.718]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=47.7, loss=0.715]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=47.9, loss=0.714]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=47.9, loss=0.713]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.64batch/s, accuracy=48.1, loss=0.7]  \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=47, loss=0.715]  \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=48.8, loss=0.712]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=48.9, loss=0.712]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.42batch/s, accuracy=47.7, loss=0.714]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=48.2, loss=0.71] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.67batch/s, accuracy=48.3, loss=0.699]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.39batch/s, accuracy=49.8, loss=0.707]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=48.4, loss=0.711]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=48.6, loss=0.711]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=49.3, loss=0.705]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=49.9, loss=0.706]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.74batch/s, accuracy=48.4, loss=0.696]\n",
      "\u001b[32m[I 2023-03-09 03:35:02,329]\u001b[0m Trial 20 finished with value: 0.543026706231454 and parameters: {'lr_exp': -5.921813909379311, 'weight_decay_exp': -2.5898268850169917, 'layers': 4, 'dropout': 0.4470162225606836}. Best is trial 20 with value: 0.543026706231454.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.43batch/s, accuracy=46.7, loss=0.722]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=46.1, loss=0.722]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=46.4, loss=0.723]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=46.6, loss=0.72] \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.44batch/s, accuracy=46.4, loss=0.72] \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.26batch/s, accuracy=46.9, loss=0.703]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=46.6, loss=0.718]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.33batch/s, accuracy=46.4, loss=0.719]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.30batch/s, accuracy=47.4, loss=0.717]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=47.9, loss=0.713]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=46.4, loss=0.719]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.73batch/s, accuracy=47.2, loss=0.701]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=47.1, loss=0.714]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=47.4, loss=0.711]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=46.6, loss=0.714]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=47.4, loss=0.714]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=47.9, loss=0.712]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.68batch/s, accuracy=48.2, loss=0.699]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=48.1, loss=0.71] \n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=47.2, loss=0.712]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.37batch/s, accuracy=48, loss=0.708]  \n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=48.3, loss=0.709]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=48.1, loss=0.708]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.64batch/s, accuracy=48.1, loss=0.697]\n",
      "\u001b[32m[I 2023-03-09 03:41:45,578]\u001b[0m Trial 21 finished with value: 0.5872945357618835 and parameters: {'lr_exp': -5.961894715777749, 'weight_decay_exp': -2.427291780099218, 'layers': 4, 'dropout': 0.4329101757872429}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=50.3, loss=0.703]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=50.4, loss=0.702]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=51, loss=0.701]  \n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=51.7, loss=0.701]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=51.7, loss=0.697]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.61batch/s, accuracy=51.6, loss=0.694]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=50.4, loss=0.703]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=51.2, loss=0.7]  \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=50.9, loss=0.699]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=53, loss=0.697]  \n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=50, loss=0.702]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.72batch/s, accuracy=51.6, loss=0.693]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=51.4, loss=0.701]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=52, loss=0.696]  \n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=51.3, loss=0.7]  \n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=51.8, loss=0.699]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=51.7, loss=0.702]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.76batch/s, accuracy=51.5, loss=0.693]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=51.9, loss=0.698]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=51.9, loss=0.698]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=51.6, loss=0.696]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=51.6, loss=0.701]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=52.7, loss=0.696]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.69batch/s, accuracy=51.6, loss=0.692]\n",
      "\u001b[32m[I 2023-03-09 03:48:25,527]\u001b[0m Trial 22 finished with value: 0.2650807136788445 and parameters: {'lr_exp': -5.889505565364988, 'weight_decay_exp': -2.517324747969055, 'layers': 4, 'dropout': 0.529233324764302}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.33batch/s, accuracy=49.7, loss=0.704]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=48.6, loss=0.706]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=49.6, loss=0.705]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=49.7, loss=0.7]  \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=51.6, loss=0.701]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.63batch/s, accuracy=54.1, loss=0.694]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=49.9, loss=0.703]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=50.5, loss=0.701]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=51.7, loss=0.697]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=51.6, loss=0.699]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=50.4, loss=0.699]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.67batch/s, accuracy=53.8, loss=0.691]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=51.3, loss=0.701]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=51, loss=0.698]  \n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=51.6, loss=0.699]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=53.3, loss=0.695]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=52.3, loss=0.699]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.72batch/s, accuracy=54.1, loss=0.69] \n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=52.4, loss=0.695]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=51.3, loss=0.696]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=53.3, loss=0.695]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=52.6, loss=0.695]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=52.2, loss=0.695]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.70batch/s, accuracy=54.4, loss=0.689]\n",
      "\u001b[32m[I 2023-03-09 03:55:06,295]\u001b[0m Trial 23 finished with value: 0.3191311612364244 and parameters: {'lr_exp': -5.413387558881479, 'weight_decay_exp': -2.354565561939259, 'layers': 5, 'dropout': 0.3991409201967556}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=48.9, loss=0.709]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=49.1, loss=0.705]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=49, loss=0.708]  \n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=51.4, loss=0.7]  \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.44batch/s, accuracy=49.8, loss=0.705]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.71batch/s, accuracy=48.4, loss=0.696]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=50.5, loss=0.701]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=49.5, loss=0.704]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=49.7, loss=0.702]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=51.9, loss=0.697]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=50.8, loss=0.7]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.71batch/s, accuracy=52.3, loss=0.694]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=51, loss=0.698]  \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=50.9, loss=0.698]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=52.1, loss=0.698]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=51.2, loss=0.696]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=50.6, loss=0.699]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.69batch/s, accuracy=52.4, loss=0.691]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.35batch/s, accuracy=51.8, loss=0.695]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=52.5, loss=0.694]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=52, loss=0.695]  \n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=53.8, loss=0.693]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=51.1, loss=0.696]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.63batch/s, accuracy=52.7, loss=0.69] \n",
      "\u001b[32m[I 2023-03-09 04:01:46,844]\u001b[0m Trial 24 finished with value: 0.46628859483301827 and parameters: {'lr_exp': -5.670605357862252, 'weight_decay_exp': -2.9000876564777003, 'layers': 4, 'dropout': 0.45883960750003244}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=49.6, loss=0.704]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=49.5, loss=0.704]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=50.2, loss=0.703]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.02batch/s, accuracy=50, loss=0.701]  \n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.34batch/s, accuracy=49.7, loss=0.702]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.72batch/s, accuracy=51.4, loss=0.696]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=49.3, loss=0.701]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=51, loss=0.7]    \n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=50.7, loss=0.697]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=51.8, loss=0.696]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=51.5, loss=0.697]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.67batch/s, accuracy=51.3, loss=0.693]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=52.9, loss=0.692]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=52.8, loss=0.692]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=52.5, loss=0.694]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=52.3, loss=0.694]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=53.2, loss=0.692]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.67batch/s, accuracy=51.9, loss=0.69] \n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=53.5, loss=0.692]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.42batch/s, accuracy=52.5, loss=0.694]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=54, loss=0.692]  \n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=53.2, loss=0.692]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=52.8, loss=0.692]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.68batch/s, accuracy=54.4, loss=0.688]\n",
      "\u001b[32m[I 2023-03-09 04:08:30,113]\u001b[0m Trial 25 finished with value: 0.1986234021632252 and parameters: {'lr_exp': -5.150219697463907, 'weight_decay_exp': -2.2984335913573735, 'layers': 5, 'dropout': 0.5318367842362582}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=47.3, loss=0.712]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.14batch/s, accuracy=47.1, loss=0.71] \n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=47.4, loss=0.711]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.45batch/s, accuracy=46.3, loss=0.711]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=48, loss=0.707]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.68batch/s, accuracy=49.5, loss=0.698]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.07batch/s, accuracy=48.4, loss=0.707]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.23batch/s, accuracy=48.6, loss=0.705]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=48.7, loss=0.706]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=48.3, loss=0.704]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=49.6, loss=0.703]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.68batch/s, accuracy=49, loss=0.696]  \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.42batch/s, accuracy=49.2, loss=0.703]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=49.8, loss=0.702]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=50, loss=0.703]  \n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=50.1, loss=0.7]  \n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=50.4, loss=0.698]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.69batch/s, accuracy=48.8, loss=0.694]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=50.7, loss=0.699]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=48.4, loss=0.703]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=50.5, loss=0.7]  \n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=50.1, loss=0.698]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=50, loss=0.699]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.78batch/s, accuracy=49.1, loss=0.693]\n",
      "\u001b[32m[I 2023-03-09 04:15:15,642]\u001b[0m Trial 26 finished with value: 0.4678362573099415 and parameters: {'lr_exp': -5.701590890028571, 'weight_decay_exp': -3.0402340636551637, 'layers': 4, 'dropout': 0.34927487398754753}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.44batch/s, accuracy=51, loss=0.702]  \n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=51.1, loss=0.695]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=53, loss=0.691]  \n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=54.2, loss=0.689]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.03batch/s, accuracy=53.3, loss=0.691]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.41batch/s, accuracy=53, loss=0.686]  \n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.16batch/s, accuracy=54.2, loss=0.688]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=54.9, loss=0.687]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=54.1, loss=0.685]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=54.6, loss=0.685]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=55, loss=0.685]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.55batch/s, accuracy=56.1, loss=0.683]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.13batch/s, accuracy=55, loss=0.684]  \n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.13batch/s, accuracy=54.6, loss=0.683]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.41batch/s, accuracy=55.2, loss=0.682]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.32batch/s, accuracy=55.1, loss=0.684]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.19batch/s, accuracy=54.7, loss=0.683]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.51batch/s, accuracy=56.9, loss=0.681]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=55, loss=0.682]  \n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.27batch/s, accuracy=55.5, loss=0.683]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=54.6, loss=0.681]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.30batch/s, accuracy=55.6, loss=0.68] \n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.09batch/s, accuracy=56, loss=0.679]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.10batch/s, accuracy=56.9, loss=0.681]\n",
      "\u001b[32m[I 2023-03-09 04:22:14,156]\u001b[0m Trial 27 finished with value: 0.44967880085653106 and parameters: {'lr_exp': -4.503488558894034, 'weight_decay_exp': -3.4005696346595022, 'layers': 3, 'dropout': 0.48057268923599306}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.21batch/s, accuracy=53.4, loss=0.697]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.30batch/s, accuracy=52.7, loss=0.7]  \n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.21batch/s, accuracy=52.2, loss=0.696]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.21batch/s, accuracy=52.7, loss=0.696]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.30batch/s, accuracy=53.1, loss=0.694]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.27batch/s, accuracy=53.2, loss=0.693]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.29batch/s, accuracy=53.4, loss=0.696]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=53.9, loss=0.695]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.55batch/s, accuracy=53.9, loss=0.693]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=53.2, loss=0.696]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=54, loss=0.693]  \n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.65batch/s, accuracy=54.1, loss=0.69] \n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=54.1, loss=0.693]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=53.6, loss=0.693]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=53.1, loss=0.693]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=54.3, loss=0.691]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.16batch/s, accuracy=53.9, loss=0.691]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.59batch/s, accuracy=54.3, loss=0.688]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.16batch/s, accuracy=54.8, loss=0.688]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.39batch/s, accuracy=54.7, loss=0.687]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:20<00:00,  4.13batch/s, accuracy=55.6, loss=0.687]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.50batch/s, accuracy=54.9, loss=0.689]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.56batch/s, accuracy=54.3, loss=0.689]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.74batch/s, accuracy=54.3, loss=0.687]\n",
      "\u001b[32m[I 2023-03-09 04:29:06,620]\u001b[0m Trial 28 finished with value: 0.21366698748796925 and parameters: {'lr_exp': -5.2416047494855125, 'weight_decay_exp': -2.3324547483832188, 'layers': 4, 'dropout': 0.4174544673400084}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.46batch/s, accuracy=46.4, loss=0.718]\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.48batch/s, accuracy=47.1, loss=0.719]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.22batch/s, accuracy=46.7, loss=0.718]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.27batch/s, accuracy=47.5, loss=0.718]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.38batch/s, accuracy=48.2, loss=0.714]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.72batch/s, accuracy=46.8, loss=0.697]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=47.9, loss=0.714]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=48.9, loss=0.713]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.24batch/s, accuracy=47.4, loss=0.713]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=47.5, loss=0.714]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=48.6, loss=0.711]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.70batch/s, accuracy=47.2, loss=0.695]\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=48.4, loss=0.711]\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.51batch/s, accuracy=48.9, loss=0.709]\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.49batch/s, accuracy=47.8, loss=0.711]\n",
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.53batch/s, accuracy=48.5, loss=0.709]\n",
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=48.2, loss=0.712]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.72batch/s, accuracy=47.1, loss=0.694]\n",
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=47.9, loss=0.709]\n",
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:19<00:00,  4.40batch/s, accuracy=49.3, loss=0.708]\n",
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.47batch/s, accuracy=49.7, loss=0.704]\n",
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.54batch/s, accuracy=48.8, loss=0.707]\n",
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:18<00:00,  4.52batch/s, accuracy=49.8, loss=0.705]\n",
      "Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  4.70batch/s, accuracy=50.8, loss=0.693]\n",
      "\u001b[32m[I 2023-03-09 04:35:50,899]\u001b[0m Trial 29 finished with value: 0.4925028835063437 and parameters: {'lr_exp': -5.959909057202454, 'weight_decay_exp': -2.6655341404779476, 'layers': 5, 'dropout': 0.5556244703801031}. Best is trial 21 with value: 0.5872945357618835.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "      config ={\n",
    "\n",
    "                \"method\": None,\n",
    "\n",
    "                \"learning_rate\": 10**trial.suggest_float(\"lr_exp\", -6, -2),\n",
    "\n",
    "                \"weight_decay\": 10**trial.suggest_float(\"weight_decay_exp\", -6, -2),\n",
    "\n",
    "                \"batch_size\": 64,\n",
    "\n",
    "                \"layers\": trial.suggest_int(\"layers\", 2, 6),\n",
    "\n",
    "                \"mlp_hidden_dim\": 64,\n",
    "\n",
    "                \"separate\": False,\n",
    "\n",
    "                \"max_corpus_len\": 1,\n",
    "\n",
    "                \"dropout\": trial.suggest_float(\"dropout\", 0.2, 0.7),\n",
    "\n",
    "            }\n",
    "      model = MyModel(\n",
    "            nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "            separate=False, dropout=config[\"dropout\"], mlp_hidden_dim=config[\"mlp_hidden_dim\"]\n",
    "            ).to(device)\n",
    "\n",
    "      _, train_loader, tokenizer, _ = get_data_loader(\n",
    "      returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "      separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "      batch_size=config[\"batch_size\"]\n",
    "      )\n",
    "\n",
    "      _, val_loader, _, _ = get_data_loader(\n",
    "      returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "      separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "      batch_size=config[\"batch_size\"]\n",
    "      )\n",
    "\n",
    "      _, _, _, _ = get_data_loader(\n",
    "      returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "      separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "      batch_size=config[\"batch_size\"]\n",
    "      )\n",
    "\n",
    "      _, _, eval_f1s = train(model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
    "            device=device, max_epochs=20, eval_every=5, name=f\"no_nlp_{config['learning_rate']}\")\n",
    "      return eval_f1s[-1]\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr_exp': -5.961894715777749,\n",
       " 'weight_decay_exp': -2.427291780099218,\n",
       " 'layers': 4,\n",
       " 'dropout': 0.4329101757872429}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5872945357618835"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4c350da27618d5732fc58ebcab8d2c0381c51b7361f332741f21e30512bbdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
