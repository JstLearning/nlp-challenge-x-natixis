{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from preprocessing.preprocessing import ecb_pipeline_en, fast_detect\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"data/train_series.csv\"\n",
    "FILENAME_ECB = \"data/ecb_data.csv\"\n",
    "FILENAME_FED = \"data/fed_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv(FILENAME, index_col=0)\n",
    "ecb = pd.read_csv(FILENAME_ECB, index_col=0)\n",
    "fed = pd.read_csv(FILENAME_FED, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.get_dummies(returns, columns=[\"Index Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[\"Sign\"] = (returns[\"Index + 1\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>-0.027519</td>\n",
       "      <td>-0.103565</td>\n",
       "      <td>-0.045086</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.054050</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.008436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-0.011304</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004980</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.001083</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.001623</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.006444</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "0   0.001045   0.005841   0.003832  -0.027519  -0.103565  -0.045086   \n",
       "1  -0.021497   0.007891  -0.013175  -0.008436   0.000000   0.026303   \n",
       "2  -0.001872  -0.008154   0.023588   0.004086   0.003493   0.003300   \n",
       "3   0.004980  -0.000864   0.001677   0.000000   0.006030  -0.001083   \n",
       "4   0.000360  -0.001893   0.005579  -0.003056  -0.001171  -0.001623   \n",
       "\n",
       "   Index - 3  Index - 2  Index - 1  Index - 0  ... Index Name_CVIX Index  \\\n",
       "0  -0.011265   0.005164   0.054050   0.015779  ...                     0   \n",
       "1   0.000556   0.001455   0.007422   0.000000  ...                     0   \n",
       "2   0.000885  -0.011304   0.005040   0.000156  ...                     0   \n",
       "3   0.000419   0.001492   0.001018  -0.002582  ...                     0   \n",
       "4  -0.002350  -0.006444  -0.000729  -0.000365  ...                     0   \n",
       "\n",
       "  Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "0                        0                            0   \n",
       "1                        0                            0   \n",
       "2                        0                            0   \n",
       "3                        0                            0   \n",
       "4                        1                            0   \n",
       "\n",
       "   Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "0                      0                     0                       0   \n",
       "1                      1                     0                       0   \n",
       "2                      0                     1                       0   \n",
       "3                      0                     1                       0   \n",
       "4                      0                     0                       0   \n",
       "\n",
       "   Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  Sign  \n",
       "0                      0                     1                     0     1  \n",
       "1                      0                     0                     0     1  \n",
       "2                      0                     0                     0     1  \n",
       "3                      0                     0                     0     1  \n",
       "4                      0                     0                     0     1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = returns[\"Sign\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4930\n",
       "1    4016\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns.drop([\"Sign\", \"Index + 1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index - 9', 'Index - 8', 'Index - 7', 'Index - 6', 'Index - 5',\n",
       "       'Index - 4', 'Index - 3', 'Index - 2', 'Index - 1', 'Index - 0',\n",
       "       'index ecb', 'index fed', 'Index Name_CVIX Index',\n",
       "       'Index Name_EURUSD Curncy', 'Index Name_EURUSDV1M Curncy',\n",
       "       'Index Name_MOVE Index', 'Index Name_SPX Index',\n",
       "       'Index Name_SRVIX Index', 'Index Name_SX5E Index',\n",
       "       'Index Name_V2X Index', 'Index Name_VIX Index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontextual_cols = ['Index - 9',\n",
    " 'Index - 8',\n",
    " 'Index - 7',\n",
    " 'Index - 6',\n",
    " 'Index - 5',\n",
    " 'Index - 4',\n",
    " 'Index - 3',\n",
    " 'Index - 2',\n",
    " 'Index - 1',\n",
    " 'Index - 0',\n",
    " 'Index Name_CVIX Index',\n",
    " 'Index Name_EURUSD Curncy',\n",
    " 'Index Name_EURUSDV1M Curncy',\n",
    " 'Index Name_MOVE Index',\n",
    " 'Index Name_SPX Index',\n",
    " 'Index Name_SRVIX Index',\n",
    " 'Index Name_SX5E Index',\n",
    " 'Index Name_V2X Index',\n",
    " 'Index Name_VIX Index']\n",
    "nb_nontextfeatures = len(nontextual_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% val, 20% test\n",
    "\n",
    "returns_, returns_test, y_, y_test = train_test_split(\n",
    "    returns, y, test_size=0.2, train_size=0.8,\n",
    "    random_state=0, stratify=y\n",
    "    )\n",
    "\n",
    "returns_train, returns_val, y_train, y_val = train_test_split(\n",
    "    returns_, y_, test_size=0.25, train_size=0.75,\n",
    "    random_state=42, stratify=y_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del returns, y\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comments by Yves Mersch at Financial Services ...</td>\n",
       "      <td>Yves Mersch</td>\n",
       "      <td>Comments by Yves Mersch at Financial Service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Securing sustained economic growth in the euro...</td>\n",
       "      <td>Vítor Constâncio</td>\n",
       "      <td>Securing sustained economic growth in the eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The role of monetary policy in addressing the ...</td>\n",
       "      <td>Mario Draghi</td>\n",
       "      <td>The role of monetary policy in addressing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pandemic emergency: the three challenges f...</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>SPEECH  The pandemic emergency: the three c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transmission channels of monetary policy in th...</td>\n",
       "      <td>Peter Praet</td>\n",
       "      <td>Transmission channels of monetary policy in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           speaker  \\\n",
       "0  Comments by Yves Mersch at Financial Services ...       Yves Mersch   \n",
       "1  Securing sustained economic growth in the euro...  Vítor Constâncio   \n",
       "2  The role of monetary policy in addressing the ...      Mario Draghi   \n",
       "3  The pandemic emergency: the three challenges f...    Philip R. Lane   \n",
       "4  Transmission channels of monetary policy in th...       Peter Praet   \n",
       "\n",
       "                                                text  \n",
       "0    Comments by Yves Mersch at Financial Service...  \n",
       "1    Securing sustained economic growth in the eu...  \n",
       "2    The role of monetary policy in addressing th...  \n",
       "3     SPEECH  The pandemic emergency: the three c...  \n",
       "4    Transmission channels of monetary policy in ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Importance of Economic Education and Finan...</td>\n",
       "      <td>Governor Frederic S. Mishkin</td>\n",
       "      <td>As ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial Innovation and Consumer Protection</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>The concept of financial innovation, it seems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implementing Basel II in the United States</td>\n",
       "      <td>Governor Randall S. Kroszner</td>\n",
       "      <td>Good afternoon. I would like to thank Standar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Assessment of the U.S. Economy</td>\n",
       "      <td>Vice Chair for Supervision Randal K. Quarles</td>\n",
       "      <td>Thank you for the opportunity to take part in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monetary Policy since the Onset of the Crisis</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>When we convened in Jackson Hole in August 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The Importance of Economic Education and Finan...   \n",
       "1       Financial Innovation and Consumer Protection   \n",
       "2         Implementing Basel II in the United States   \n",
       "3                  An Assessment of the U.S. Economy   \n",
       "4      Monetary Policy since the Onset of the Crisis   \n",
       "\n",
       "                                        speaker  \\\n",
       "0                  Governor Frederic S. Mishkin   \n",
       "1                      Chairman Ben S. Bernanke   \n",
       "2                  Governor Randall S. Kroszner   \n",
       "3  Vice Chair for Supervision Randal K. Quarles   \n",
       "4                      Chairman Ben S. Bernanke   \n",
       "\n",
       "                                                text  \n",
       "0                                             As ...  \n",
       "1   The concept of financial innovation, it seems...  \n",
       "2   Good afternoon. I would like to thank Standar...  \n",
       "3   Thank you for the opportunity to take part in...  \n",
       "4   When we convened in Jackson Hole in August 20...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"text_\"] = ecb.apply(ecb_pipeline_en, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"text\"].fillna(\"\", inplace=True)\n",
    "ecb[\"speaker\"].fillna(\"Unknown\", inplace=True)\n",
    "fed[\"speaker\"].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                         Auf neuen Wegen zum alten Ziel\n",
       "speaker                                          Yves Mersch\n",
       "text         Auf neuen Wegen zum alten Ziel   Rede von Yv...\n",
       "text_      Rede von Yves Mersch, Mitglied des Direktorium...\n",
       "Name: 151, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text in french\n",
    "ecb.loc[138]\n",
    "# Text in german\n",
    "ecb.loc[151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"lang\"] = ecb[\"text_\"].apply(fast_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comments by Yves Mersch at Financial Services ...</td>\n",
       "      <td>Yves Mersch</td>\n",
       "      <td>Comments by Yves Mersch at Financial Service...</td>\n",
       "      <td>Sustainable economic growth in the real econom...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Securing sustained economic growth in the euro...</td>\n",
       "      <td>Vítor Constâncio</td>\n",
       "      <td>Securing sustained economic growth in the eu...</td>\n",
       "      <td>Ladies and Gentlemen, Thank you for inviting m...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The role of monetary policy in addressing the ...</td>\n",
       "      <td>Mario Draghi</td>\n",
       "      <td>The role of monetary policy in addressing th...</td>\n",
       "      <td>There was a time, not too long ago, when centr...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pandemic emergency: the three challenges f...</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>SPEECH  The pandemic emergency: the three c...</td>\n",
       "      <td>Today, I will discuss the monetary policy meas...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transmission channels of monetary policy in th...</td>\n",
       "      <td>Peter Praet</td>\n",
       "      <td>Transmission channels of monetary policy in ...</td>\n",
       "      <td>Ladies and Gentlemen, Since the onset of the f...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           speaker  \\\n",
       "0  Comments by Yves Mersch at Financial Services ...       Yves Mersch   \n",
       "1  Securing sustained economic growth in the euro...  Vítor Constâncio   \n",
       "2  The role of monetary policy in addressing the ...      Mario Draghi   \n",
       "3  The pandemic emergency: the three challenges f...    Philip R. Lane   \n",
       "4  Transmission channels of monetary policy in th...       Peter Praet   \n",
       "\n",
       "                                                text  \\\n",
       "0    Comments by Yves Mersch at Financial Service...   \n",
       "1    Securing sustained economic growth in the eu...   \n",
       "2    The role of monetary policy in addressing th...   \n",
       "3     SPEECH  The pandemic emergency: the three c...   \n",
       "4    Transmission channels of monetary policy in ...   \n",
       "\n",
       "                                               text_ lang  \n",
       "0  Sustainable economic growth in the real econom...   en  \n",
       "1  Ladies and Gentlemen, Thank you for inviting m...   en  \n",
       "2  There was a time, not too long ago, when centr...   en  \n",
       "3  Today, I will discuss the monetary policy meas...   en  \n",
       "4  Ladies and Gentlemen, Since the onset of the f...   en  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed[\"lang\"] = fed[\"text\"].apply(fast_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Importance of Economic Education and Finan...</td>\n",
       "      <td>Governor Frederic S. Mishkin</td>\n",
       "      <td>As ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial Innovation and Consumer Protection</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>The concept of financial innovation, it seems...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implementing Basel II in the United States</td>\n",
       "      <td>Governor Randall S. Kroszner</td>\n",
       "      <td>Good afternoon. I would like to thank Standar...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Assessment of the U.S. Economy</td>\n",
       "      <td>Vice Chair for Supervision Randal K. Quarles</td>\n",
       "      <td>Thank you for the opportunity to take part in...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monetary Policy since the Onset of the Crisis</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>When we convened in Jackson Hole in August 20...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The Importance of Economic Education and Finan...   \n",
       "1       Financial Innovation and Consumer Protection   \n",
       "2         Implementing Basel II in the United States   \n",
       "3                  An Assessment of the U.S. Economy   \n",
       "4      Monetary Policy since the Onset of the Crisis   \n",
       "\n",
       "                                        speaker  \\\n",
       "0                  Governor Frederic S. Mishkin   \n",
       "1                      Chairman Ben S. Bernanke   \n",
       "2                  Governor Randall S. Kroszner   \n",
       "3  Vice Chair for Supervision Randal K. Quarles   \n",
       "4                      Chairman Ben S. Bernanke   \n",
       "\n",
       "                                                text lang  \n",
       "0                                             As ...   en  \n",
       "1   The concept of financial innovation, it seems...   en  \n",
       "2   Good afternoon. I would like to thank Standar...   en  \n",
       "3   Thank you for the opportunity to take part in...   en  \n",
       "4   When we convened in Jackson Hole in August 20...   en  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_langs = ecb[\"lang\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    1646\n",
       "de      75\n",
       "fr      31\n",
       "es      16\n",
       "it       4\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb[\"lang\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test hierarchical BERT here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to C:\\Users\\huuta\\.huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.\n",
      "\n",
      "The tokenizer class to instantiate is selected based on the `model_type` property of the config object (either\n",
      "passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
      "falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
      "\n",
      "    - **albert** -- [`AlbertTokenizerFast`] (ALBERT model)\n",
      "    - **bart** -- [`BartTokenizer`] or [`BartTokenizerFast`] (BART model)\n",
      "    - **barthez** -- [`BarthezTokenizerFast`] (BARThez model)\n",
      "    - **bartpho** -- [`BartphoTokenizer`] (BARTpho model)\n",
      "    - **bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BERT model)\n",
      "    - **bert-generation** --  (Bert Generation model)\n",
      "    - **bert-japanese** -- [`BertJapaneseTokenizer`] (BertJapanese model)\n",
      "    - **bertweet** -- [`BertweetTokenizer`] (BERTweet model)\n",
      "    - **big_bird** -- [`BigBirdTokenizerFast`] (BigBird model)\n",
      "    - **bigbird_pegasus** -- [`PegasusTokenizer`] or [`PegasusTokenizerFast`] (BigBird-Pegasus model)\n",
      "    - **biogpt** -- [`BioGptTokenizer`] (BioGpt model)\n",
      "    - **blenderbot** -- [`BlenderbotTokenizer`] or [`BlenderbotTokenizerFast`] (Blenderbot model)\n",
      "    - **blenderbot-small** -- [`BlenderbotSmallTokenizer`] (BlenderbotSmall model)\n",
      "    - **blip** -- [`BertTokenizer`] or [`BertTokenizerFast`] (BLIP model)\n",
      "    - **bloom** -- [`BloomTokenizerFast`] (BLOOM model)\n",
      "    - **byt5** -- [`ByT5Tokenizer`] (ByT5 model)\n",
      "    - **camembert** -- [`CamembertTokenizerFast`] (CamemBERT model)\n",
      "    - **canine** -- [`CanineTokenizer`] (CANINE model)\n",
      "    - **chinese_clip** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Chinese-CLIP model)\n",
      "    - **clip** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (CLIP model)\n",
      "    - **clipseg** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (CLIPSeg model)\n",
      "    - **codegen** -- [`CodeGenTokenizer`] or [`CodeGenTokenizerFast`] (CodeGen model)\n",
      "    - **convbert** -- [`ConvBertTokenizer`] or [`ConvBertTokenizerFast`] (ConvBERT model)\n",
      "    - **cpm** -- [`CpmTokenizerFast`] (CPM model)\n",
      "    - **ctrl** -- [`CTRLTokenizer`] (CTRL model)\n",
      "    - **data2vec-text** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (Data2VecText model)\n",
      "    - **deberta** -- [`DebertaTokenizer`] or [`DebertaTokenizerFast`] (DeBERTa model)\n",
      "    - **deberta-v2** -- [`DebertaV2TokenizerFast`] (DeBERTa-v2 model)\n",
      "    - **distilbert** -- [`DistilBertTokenizer`] or [`DistilBertTokenizerFast`] (DistilBERT model)\n",
      "    - **dpr** -- [`DPRQuestionEncoderTokenizer`] or [`DPRQuestionEncoderTokenizerFast`] (DPR model)\n",
      "    - **electra** -- [`ElectraTokenizer`] or [`ElectraTokenizerFast`] (ELECTRA model)\n",
      "    - **ernie** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ERNIE model)\n",
      "    - **esm** -- [`EsmTokenizer`] (ESM model)\n",
      "    - **flaubert** -- [`FlaubertTokenizer`] (FlauBERT model)\n",
      "    - **fnet** -- [`FNetTokenizer`] or [`FNetTokenizerFast`] (FNet model)\n",
      "    - **fsmt** -- [`FSMTTokenizer`] (FairSeq Machine-Translation model)\n",
      "    - **funnel** -- [`FunnelTokenizer`] or [`FunnelTokenizerFast`] (Funnel Transformer model)\n",
      "    - **git** -- [`BertTokenizer`] or [`BertTokenizerFast`] (GIT model)\n",
      "    - **gpt-sw3** --  (GPT-Sw3 model)\n",
      "    - **gpt2** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (OpenAI GPT-2 model)\n",
      "    - **gpt_neo** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPT Neo model)\n",
      "    - **gpt_neox** -- [`GPTNeoXTokenizerFast`] (GPT NeoX model)\n",
      "    - **gpt_neox_japanese** -- [`GPTNeoXJapaneseTokenizer`] (GPT NeoX Japanese model)\n",
      "    - **gptj** -- [`GPT2Tokenizer`] or [`GPT2TokenizerFast`] (GPT-J model)\n",
      "    - **groupvit** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (GroupViT model)\n",
      "    - **herbert** -- [`HerbertTokenizer`] or [`HerbertTokenizerFast`] (HerBERT model)\n",
      "    - **hubert** -- [`Wav2Vec2CTCTokenizer`] (Hubert model)\n",
      "    - **ibert** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (I-BERT model)\n",
      "    - **jukebox** -- [`JukeboxTokenizer`] (Jukebox model)\n",
      "    - **layoutlm** -- [`LayoutLMTokenizer`] or [`LayoutLMTokenizerFast`] (LayoutLM model)\n",
      "    - **layoutlmv2** -- [`LayoutLMv2Tokenizer`] or [`LayoutLMv2TokenizerFast`] (LayoutLMv2 model)\n",
      "    - **layoutlmv3** -- [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`] (LayoutLMv3 model)\n",
      "    - **layoutxlm** -- [`LayoutXLMTokenizer`] or [`LayoutXLMTokenizerFast`] (LayoutXLM model)\n",
      "    - **led** -- [`LEDTokenizer`] or [`LEDTokenizerFast`] (LED model)\n",
      "    - **lilt** -- [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`] (LiLT model)\n",
      "    - **longformer** -- [`LongformerTokenizer`] or [`LongformerTokenizerFast`] (Longformer model)\n",
      "    - **longt5** -- [`T5TokenizerFast`] (LongT5 model)\n",
      "    - **luke** -- [`LukeTokenizer`] (LUKE model)\n",
      "    - **lxmert** -- [`LxmertTokenizer`] or [`LxmertTokenizerFast`] (LXMERT model)\n",
      "    - **m2m_100** --  (M2M100 model)\n",
      "    - **marian** --  (Marian model)\n",
      "    - **mbart** -- [`MBartTokenizerFast`] (mBART model)\n",
      "    - **mbart50** -- [`MBart50TokenizerFast`] (mBART-50 model)\n",
      "    - **megatron-bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Megatron-BERT model)\n",
      "    - **mluke** --  (mLUKE model)\n",
      "    - **mobilebert** -- [`MobileBertTokenizer`] or [`MobileBertTokenizerFast`] (MobileBERT model)\n",
      "    - **mpnet** -- [`MPNetTokenizer`] or [`MPNetTokenizerFast`] (MPNet model)\n",
      "    - **mt5** -- [`MT5TokenizerFast`] (MT5 model)\n",
      "    - **mvp** -- [`MvpTokenizer`] or [`MvpTokenizerFast`] (MVP model)\n",
      "    - **nezha** -- [`BertTokenizer`] or [`BertTokenizerFast`] (Nezha model)\n",
      "    - **nllb** -- [`NllbTokenizerFast`] (NLLB model)\n",
      "    - **nystromformer** -- [`AlbertTokenizerFast`] (Nyströmformer model)\n",
      "    - **oneformer** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OneFormer model)\n",
      "    - **openai-gpt** -- [`OpenAIGPTTokenizer`] or [`OpenAIGPTTokenizerFast`] (OpenAI GPT model)\n",
      "    - **opt** -- [`GPT2Tokenizer`] (OPT model)\n",
      "    - **owlvit** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (OWL-ViT model)\n",
      "    - **pegasus** -- [`PegasusTokenizerFast`] (Pegasus model)\n",
      "    - **pegasus_x** -- [`PegasusTokenizerFast`] (PEGASUS-X model)\n",
      "    - **perceiver** -- [`PerceiverTokenizer`] (Perceiver model)\n",
      "    - **phobert** -- [`PhobertTokenizer`] (PhoBERT model)\n",
      "    - **plbart** --  (PLBart model)\n",
      "    - **prophetnet** -- [`ProphetNetTokenizer`] (ProphetNet model)\n",
      "    - **qdqbert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (QDQBert model)\n",
      "    - **rag** -- [`RagTokenizer`] (RAG model)\n",
      "    - **realm** -- [`RealmTokenizer`] or [`RealmTokenizerFast`] (REALM model)\n",
      "    - **reformer** -- [`ReformerTokenizerFast`] (Reformer model)\n",
      "    - **rembert** -- [`RemBertTokenizerFast`] (RemBERT model)\n",
      "    - **retribert** -- [`RetriBertTokenizer`] or [`RetriBertTokenizerFast`] (RetriBERT model)\n",
      "    - **roberta** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (RoBERTa model)\n",
      "    - **roberta-prelayernorm** -- [`RobertaTokenizer`] or [`RobertaTokenizerFast`] (RoBERTa-PreLayerNorm model)\n",
      "    - **roc_bert** -- [`RoCBertTokenizer`] (RoCBert model)\n",
      "    - **roformer** -- [`RoFormerTokenizer`] or [`RoFormerTokenizerFast`] (RoFormer model)\n",
      "    - **speech_to_text** --  (Speech2Text model)\n",
      "    - **speech_to_text_2** -- [`Speech2Text2Tokenizer`] (Speech2Text2 model)\n",
      "    - **splinter** -- [`SplinterTokenizer`] or [`SplinterTokenizerFast`] (Splinter model)\n",
      "    - **squeezebert** -- [`SqueezeBertTokenizer`] or [`SqueezeBertTokenizerFast`] (SqueezeBERT model)\n",
      "    - **switch_transformers** -- [`T5TokenizerFast`] (SwitchTransformers model)\n",
      "    - **t5** -- [`T5TokenizerFast`] (T5 model)\n",
      "    - **tapas** -- [`TapasTokenizer`] (TAPAS model)\n",
      "    - **tapex** -- [`TapexTokenizer`] (TAPEX model)\n",
      "    - **transfo-xl** -- [`TransfoXLTokenizer`] (Transformer-XL model)\n",
      "    - **vilt** -- [`BertTokenizer`] or [`BertTokenizerFast`] (ViLT model)\n",
      "    - **visual_bert** -- [`BertTokenizer`] or [`BertTokenizerFast`] (VisualBERT model)\n",
      "    - **wav2vec2** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2 model)\n",
      "    - **wav2vec2-conformer** -- [`Wav2Vec2CTCTokenizer`] (Wav2Vec2-Conformer model)\n",
      "    - **wav2vec2_phoneme** -- [`Wav2Vec2PhonemeCTCTokenizer`] (Wav2Vec2Phoneme model)\n",
      "    - **whisper** --  (Whisper model)\n",
      "    - **xclip** -- [`CLIPTokenizer`] or [`CLIPTokenizerFast`] (X-CLIP model)\n",
      "    - **xglm** -- [`XGLMTokenizerFast`] (XGLM model)\n",
      "    - **xlm** -- [`XLMTokenizer`] (XLM model)\n",
      "    - **xlm-prophetnet** --  (XLM-ProphetNet model)\n",
      "    - **xlm-roberta** -- [`XLMRobertaTokenizerFast`] (XLM-RoBERTa model)\n",
      "    - **xlm-roberta-xl** -- [`XLMRobertaTokenizerFast`] (XLM-RoBERTa-XL model)\n",
      "    - **xlnet** -- [`XLNetTokenizerFast`] (XLNet model)\n",
      "    - **yoso** -- [`AlbertTokenizerFast`] (YOSO model)\n",
      "\n",
      "Params:\n",
      "    pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      "        Can be either:\n",
      "\n",
      "            - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      "              Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      "              user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      "            - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      "              using the [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n",
      "            - A path or url to a single saved vocabulary file if and only if the tokenizer only requires a\n",
      "              single vocabulary file (like Bert or XLNet), e.g.: `./my_model_directory/vocab.txt`. (Not\n",
      "              applicable to all derived classes)\n",
      "    inputs (additional positional arguments, *optional*):\n",
      "        Will be passed along to the Tokenizer `__init__()` method.\n",
      "    config ([`PretrainedConfig`], *optional*)\n",
      "        The configuration object used to dertermine the tokenizer class to instantiate.\n",
      "    cache_dir (`str` or `os.PathLike`, *optional*):\n",
      "        Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      "        standard cache should not be used.\n",
      "    force_download (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to force the (re-)download the model weights and configuration files and override the\n",
      "        cached versions if they exist.\n",
      "    resume_download (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      "        file exists.\n",
      "    proxies (`Dict[str, str]`, *optional*):\n",
      "        A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      "        'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      "        identifier allowed by git.\n",
      "    subfolder (`str`, *optional*):\n",
      "        In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      "        facebook/rag-token-base), specify it here.\n",
      "    use_fast (`bool`, *optional*, defaults to `True`):\n",
      "        Use a [fast Rust-based tokenizer](https://huggingface.co/docs/tokenizers/index) if it is supported for\n",
      "        a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer\n",
      "        is returned instead.\n",
      "    tokenizer_type (`str`, *optional*):\n",
      "        Tokenizer type to be loaded.\n",
      "    trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
      "        should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      "        execute code present on the Hub on your local machine.\n",
      "    kwargs (additional keyword arguments, *optional*):\n",
      "        Will be passed to the Tokenizer `__init__()` method. Can be used to set special tokens like\n",
      "        `bos_token`, `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      "        `additional_special_tokens`. See parameters in the `__init__()` for more details.\n",
      "\n",
      "Examples:\n",
      "\n",
      "```python\n",
      ">>> from transformers import AutoTokenizer\n",
      "\n",
      ">>> # Download vocabulary from huggingface.co and cache.\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "\n",
      ">>> # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      "\n",
      ">>> # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"./test/bert_saved_model/\")\n",
      "\n",
      ">>> # Download vocabulary from huggingface.co and define model-specific arguments\n",
      ">>> tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
      "```\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\n",
      "\u001b[1;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "AutoTokenizer.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'HATTokenizer' has no attribute 'register_for_auto_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27812\\499909014.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kiddothe2b/hierarchical-transformer-base-4096\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\huuta\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    643\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_file\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".py\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 )\n\u001b[1;32m--> 645\u001b[1;33m                 \u001b[0mtokenizer_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_for_auto_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0muse_fast\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mconfig_tokenizer_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fast\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'HATTokenizer' has no attribute 'register_for_auto_class'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"kiddothe2b/hierarchical-transformer-base-4096\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a56309926f45d397d74814dd6e67fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/105k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767313138d7d4ce2aa46d8bdaea08cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/766M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"kiddothe2b/hierarchical-transformer-base-4096\",\n",
    "                                             trust_remote_code=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_dataset import get_data_loader\n",
    "from model.framework_model import CorpusEncoder, ClassificationHead, MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_01\",\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.,\n",
    "\n",
    "    \"batch_size\": 2,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.3,\n",
    "\n",
    "    \"separate\": True,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ce = CorpusEncoder(method=config[\"method\"],\n",
    "                   separate=config[\"separate\"],\n",
    "                   dropout=config[\"dropout\"]).to(device)\n",
    "clf = ClassificationHead(\n",
    "    corpus_emb_dim=ce.corpus_emb_dim, nontext_dim=nb_nontextfeatures,\n",
    "    layers=config[\"max_corpus_len\"], dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "my_model = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed output successfully. ce ouput = \n",
      " tensor([[ 0.1655,  0.5805,  0.1516,  0.1980,  0.1799, -0.0704,  0.0039,  0.2648,\n",
      "         -0.1464, -0.4822, -0.0637, -0.0381,  0.1207,  0.0595, -0.1349, -0.0389,\n",
      "         -0.4814, -0.4484, -0.2856,  0.5924, -0.0681,  0.0799, -0.0182,  0.1669,\n",
      "          0.0192, -0.2786, -0.0716, -0.0894,  0.2287, -0.2295, -0.1108,  0.1308,\n",
      "          0.0775, -0.4293, -0.0793,  0.3174,  0.0321,  0.2328,  0.2982, -0.0495,\n",
      "         -0.1027,  0.1390,  0.2921,  0.2360, -0.2802,  0.0855, -0.0006, -0.1196,\n",
      "          0.3278, -0.4202,  0.2727,  0.1209, -0.0837,  0.3232,  0.0220, -0.0669,\n",
      "          0.0854, -0.3017,  0.1084, -0.0993,  0.3060,  0.1382,  0.2276, -0.0841],\n",
      "        [ 0.2190,  0.6202,  0.0863,  0.1179,  0.2088,  0.0798, -0.0274,  0.3590,\n",
      "         -0.0182, -0.4043, -0.0105, -0.1185,  0.1783,  0.1159, -0.1525, -0.0539,\n",
      "         -0.5589, -0.4120, -0.4387,  0.4396, -0.0862, -0.0060, -0.0561,  0.2117,\n",
      "         -0.0965, -0.3359,  0.0290, -0.1410,  0.2277, -0.2775, -0.1915,  0.0551,\n",
      "          0.1653, -0.4295, -0.1160,  0.2208,  0.0043,  0.2019,  0.2048, -0.0903,\n",
      "         -0.1066,  0.2121,  0.1849,  0.3053, -0.1786,  0.0934, -0.0403, -0.1244,\n",
      "          0.3416, -0.4095,  0.3889,  0.0263, -0.0370,  0.3483,  0.1895, -0.0984,\n",
      "          0.0357, -0.3831,  0.0522, -0.1371,  0.2980,  0.0304,  0.2247, -0.0367]],\n",
      "       device='cuda:0')\n",
      "corpus encoder ouput shape =  torch.Size([2, 64]) \n",
      "corpus embed dim =  64\n",
      "torch.Size([2, 19])\n",
      "Classifier output =  \n",
      " tensor([0.4514, 0.4619], device='cuda:0')\n",
      "torch.Size([2, 2, 512])\n",
      "torch.Size([2, 2, 512])\n",
      "tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Test output\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ce.eval()\n",
    "    X_ecb = batch[\"X_ecb\"].to(device)\n",
    "    X_ecb_att = batch[\"X_ecb_mask\"].to(device)\n",
    "    X_fed = batch[\"X_fed\"].to(device)\n",
    "    X_fed_att = batch[\"X_fed_mask\"].to(device)\n",
    "    X_ind =  batch[\"X_ind\"].to(device)\n",
    "    y = batch[\"label\"]\n",
    "    X_text = (X_ecb, X_fed)\n",
    "    X_att = (X_ecb_att, X_fed_att)\n",
    "    ce_output = ce(X_text, X_att)\n",
    "    print(\"Computed output successfully. ce ouput = \\n\", ce_output)\n",
    "    print(\"corpus encoder ouput shape = \", ce_output.size(), \"\\ncorpus embed dim = \", ce.corpus_emb_dim)\n",
    "    print(X_ind.size())\n",
    "\n",
    "\n",
    "    clf_output = clf(ce_output, X_ind)\n",
    "    \n",
    "    print(\"Classifier output =  \\n\", clf_output)\n",
    "    my_model_output = my_model(X_text, X_att, X_ind)\n",
    "\n",
    "print(X_ecb.size())\n",
    "print(X_ecb_att.size())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_01\",\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.,\n",
    "\n",
    "    \"batch_size\": 2,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.5,\n",
    "\n",
    "    \"separate\": False,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ce = CorpusEncoder(method=config[\"method\"],\n",
    "                   separate=config[\"separate\"],\n",
    "                   dropout=config[\"dropout\"]).to(device)\n",
    "clf = ClassificationHead(\n",
    "    corpus_emb_dim=ce.corpus_emb_dim, nontext_dim=nb_nontextfeatures,\n",
    "    layers=config[\"max_corpus_len\"], dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "my_model = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huuta\\AppData\\Local\\Temp\\ipykernel_28708\\1290859485.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  z = torch.range(0, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.range(0, 15)\n",
    "print(z)\n",
    "z.view(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed output successfully. ce ouput = \n",
      " tensor([[ 0.0680, -0.4876, -0.3666, -0.1956,  0.1132,  0.0946,  0.1756, -0.0340,\n",
      "         -0.0052,  0.3330,  0.1639, -0.0179, -0.0024,  0.3258, -0.1272,  0.1548,\n",
      "          0.0966, -0.0413, -0.2932,  0.2296, -0.7422,  0.1561, -0.1577,  0.0229,\n",
      "          0.6316, -0.1441,  0.0673, -0.0629, -0.0406,  0.0709, -0.5268, -0.0249],\n",
      "        [ 0.0360, -0.5765, -0.3424, -0.1569,  0.1195,  0.1085,  0.2071, -0.0058,\n",
      "         -0.0093,  0.2382,  0.0986,  0.0924,  0.0303,  0.3314, -0.1299,  0.1669,\n",
      "          0.0670, -0.0543, -0.2332,  0.2621, -0.5938,  0.0526, -0.2652,  0.1079,\n",
      "          0.6401, -0.0916,  0.1027,  0.0037,  0.0228,  0.1233, -0.5413,  0.0193]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "corpus encoder ouput shape =  torch.Size([2, 32]) \n",
      "corpus embed dim =  32\n",
      "torch.Size([2, 19])\n",
      "Classifier output =  \n",
      " tensor([0.5068, 0.5185], device='cuda:0')\n",
      "tensor([[[  101,  1996, 23889,  ...,  4675,  7011,   102],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101, 21658,  3242,  ...,  3258,  1998,   102],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0]]], device='cuda:0')\n",
      "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "tensor([0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Test output\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ce.eval()\n",
    "    X_text = (batch[\"X_text\"].to(device),)\n",
    "    X_mask = (batch[\"X_mask\"].to(device),)\n",
    "    X_ind =  batch[\"X_ind\"].to(device)\n",
    "    y = batch[\"label\"]\n",
    "    ce_output = ce(X_text, X_mask)\n",
    "    print(\"Computed output successfully. ce ouput = \\n\", ce_output)\n",
    "    print(\"corpus encoder ouput shape = \", ce_output.size(), \"\\ncorpus embed dim = \", ce.corpus_emb_dim)\n",
    "    print(X_ind.size())\n",
    "\n",
    "\n",
    "    clf_output = clf(ce_output, X_ind)\n",
    "    \n",
    "    print(\"Classifier output =  \\n\", clf_output)\n",
    "    my_model_output = my_model(X_text, X_mask, X_ind)\n",
    "\n",
    "print(X_ecb)\n",
    "print(X_ecb_att)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   2%|▏         | 58/2684 [01:16<55:43,  1.27s/batch, accuracy=49.1, loss=0.703]  "
     ]
    }
   ],
   "source": [
    "train(my_model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
    "      device=device, max_epochs=1, eval_every=1, name=\"dummy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4c350da27618d5732fc58ebcab8d2c0381c51b7361f332741f21e30512bbdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
