{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from preprocessing.preprocessing import ecb_pipeline_en, fast_detect\n",
    "\n",
    "import time\n",
    "\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"data/train_series.csv\"\n",
    "FILENAME_ECB = \"data/ecb_data.csv\"\n",
    "FILENAME_FED = \"data/fed_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.read_csv(FILENAME, index_col=0)\n",
    "ecb = pd.read_csv(FILENAME_ECB, index_col=0)\n",
    "fed = pd.read_csv(FILENAME_FED, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = pd.get_dummies(returns, columns=[\"Index Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[\"Sign\"] = (returns[\"Index + 1\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index - 9</th>\n",
       "      <th>Index - 8</th>\n",
       "      <th>Index - 7</th>\n",
       "      <th>Index - 6</th>\n",
       "      <th>Index - 5</th>\n",
       "      <th>Index - 4</th>\n",
       "      <th>Index - 3</th>\n",
       "      <th>Index - 2</th>\n",
       "      <th>Index - 1</th>\n",
       "      <th>Index - 0</th>\n",
       "      <th>...</th>\n",
       "      <th>Index Name_CVIX Index</th>\n",
       "      <th>Index Name_EURUSD Curncy</th>\n",
       "      <th>Index Name_EURUSDV1M Curncy</th>\n",
       "      <th>Index Name_MOVE Index</th>\n",
       "      <th>Index Name_SPX Index</th>\n",
       "      <th>Index Name_SRVIX Index</th>\n",
       "      <th>Index Name_SX5E Index</th>\n",
       "      <th>Index Name_V2X Index</th>\n",
       "      <th>Index Name_VIX Index</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.003832</td>\n",
       "      <td>-0.027519</td>\n",
       "      <td>-0.103565</td>\n",
       "      <td>-0.045086</td>\n",
       "      <td>-0.011265</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.054050</td>\n",
       "      <td>0.015779</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021497</td>\n",
       "      <td>0.007891</td>\n",
       "      <td>-0.013175</td>\n",
       "      <td>-0.008436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026303</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001872</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-0.011304</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004980</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>-0.001083</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.001893</td>\n",
       "      <td>0.005579</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.001623</td>\n",
       "      <td>-0.002350</td>\n",
       "      <td>-0.006444</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index - 9  Index - 8  Index - 7  Index - 6  Index - 5  Index - 4  \\\n",
       "0   0.001045   0.005841   0.003832  -0.027519  -0.103565  -0.045086   \n",
       "1  -0.021497   0.007891  -0.013175  -0.008436   0.000000   0.026303   \n",
       "2  -0.001872  -0.008154   0.023588   0.004086   0.003493   0.003300   \n",
       "3   0.004980  -0.000864   0.001677   0.000000   0.006030  -0.001083   \n",
       "4   0.000360  -0.001893   0.005579  -0.003056  -0.001171  -0.001623   \n",
       "\n",
       "   Index - 3  Index - 2  Index - 1  Index - 0  ... Index Name_CVIX Index  \\\n",
       "0  -0.011265   0.005164   0.054050   0.015779  ...                     0   \n",
       "1   0.000556   0.001455   0.007422   0.000000  ...                     0   \n",
       "2   0.000885  -0.011304   0.005040   0.000156  ...                     0   \n",
       "3   0.000419   0.001492   0.001018  -0.002582  ...                     0   \n",
       "4  -0.002350  -0.006444  -0.000729  -0.000365  ...                     0   \n",
       "\n",
       "  Index Name_EURUSD Curncy  Index Name_EURUSDV1M Curncy  \\\n",
       "0                        0                            0   \n",
       "1                        0                            0   \n",
       "2                        0                            0   \n",
       "3                        0                            0   \n",
       "4                        1                            0   \n",
       "\n",
       "   Index Name_MOVE Index  Index Name_SPX Index  Index Name_SRVIX Index  \\\n",
       "0                      0                     0                       0   \n",
       "1                      1                     0                       0   \n",
       "2                      0                     1                       0   \n",
       "3                      0                     1                       0   \n",
       "4                      0                     0                       0   \n",
       "\n",
       "   Index Name_SX5E Index  Index Name_V2X Index  Index Name_VIX Index  Sign  \n",
       "0                      0                     1                     0     1  \n",
       "1                      0                     0                     0     1  \n",
       "2                      0                     0                     0     1  \n",
       "3                      0                     0                     0     1  \n",
       "4                      0                     0                     0     1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = returns[\"Sign\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4930\n",
       "1    4016\n",
       "Name: Sign, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = returns.drop([\"Sign\", \"Index + 1\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index - 9', 'Index - 8', 'Index - 7', 'Index - 6', 'Index - 5',\n",
       "       'Index - 4', 'Index - 3', 'Index - 2', 'Index - 1', 'Index - 0',\n",
       "       'index ecb', 'index fed', 'Index Name_CVIX Index',\n",
       "       'Index Name_EURUSD Curncy', 'Index Name_EURUSDV1M Curncy',\n",
       "       'Index Name_MOVE Index', 'Index Name_SPX Index',\n",
       "       'Index Name_SRVIX Index', 'Index Name_SX5E Index',\n",
       "       'Index Name_V2X Index', 'Index Name_VIX Index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontextual_cols = ['Index - 9',\n",
    " 'Index - 8',\n",
    " 'Index - 7',\n",
    " 'Index - 6',\n",
    " 'Index - 5',\n",
    " 'Index - 4',\n",
    " 'Index - 3',\n",
    " 'Index - 2',\n",
    " 'Index - 1',\n",
    " 'Index - 0',\n",
    " 'Index Name_CVIX Index',\n",
    " 'Index Name_EURUSD Curncy',\n",
    " 'Index Name_EURUSDV1M Curncy',\n",
    " 'Index Name_MOVE Index',\n",
    " 'Index Name_SPX Index',\n",
    " 'Index Name_SRVIX Index',\n",
    " 'Index Name_SX5E Index',\n",
    " 'Index Name_V2X Index',\n",
    " 'Index Name_VIX Index']\n",
    "nb_nontextfeatures = len(nontextual_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% train, 20% val, 20% test\n",
    "\n",
    "returns_, returns_test, y_, y_test = train_test_split(\n",
    "    returns, y, test_size=0.2, train_size=0.8,\n",
    "    random_state=0, stratify=y\n",
    "    )\n",
    "\n",
    "returns_train, returns_val, y_train, y_val = train_test_split(\n",
    "    returns_, y_, test_size=0.25, train_size=0.75,\n",
    "    random_state=42, stratify=y_\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del returns, y\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comments by Yves Mersch at Financial Services ...</td>\n",
       "      <td>Yves Mersch</td>\n",
       "      <td>Comments by Yves Mersch at Financial Service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Securing sustained economic growth in the euro...</td>\n",
       "      <td>Vítor Constâncio</td>\n",
       "      <td>Securing sustained economic growth in the eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The role of monetary policy in addressing the ...</td>\n",
       "      <td>Mario Draghi</td>\n",
       "      <td>The role of monetary policy in addressing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pandemic emergency: the three challenges f...</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>SPEECH  The pandemic emergency: the three c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transmission channels of monetary policy in th...</td>\n",
       "      <td>Peter Praet</td>\n",
       "      <td>Transmission channels of monetary policy in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           speaker  \\\n",
       "0  Comments by Yves Mersch at Financial Services ...       Yves Mersch   \n",
       "1  Securing sustained economic growth in the euro...  Vítor Constâncio   \n",
       "2  The role of monetary policy in addressing the ...      Mario Draghi   \n",
       "3  The pandemic emergency: the three challenges f...    Philip R. Lane   \n",
       "4  Transmission channels of monetary policy in th...       Peter Praet   \n",
       "\n",
       "                                                text  \n",
       "0    Comments by Yves Mersch at Financial Service...  \n",
       "1    Securing sustained economic growth in the eu...  \n",
       "2    The role of monetary policy in addressing th...  \n",
       "3     SPEECH  The pandemic emergency: the three c...  \n",
       "4    Transmission channels of monetary policy in ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Importance of Economic Education and Finan...</td>\n",
       "      <td>Governor Frederic S. Mishkin</td>\n",
       "      <td>As ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial Innovation and Consumer Protection</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>The concept of financial innovation, it seems...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implementing Basel II in the United States</td>\n",
       "      <td>Governor Randall S. Kroszner</td>\n",
       "      <td>Good afternoon. I would like to thank Standar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Assessment of the U.S. Economy</td>\n",
       "      <td>Vice Chair for Supervision Randal K. Quarles</td>\n",
       "      <td>Thank you for the opportunity to take part in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monetary Policy since the Onset of the Crisis</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>When we convened in Jackson Hole in August 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The Importance of Economic Education and Finan...   \n",
       "1       Financial Innovation and Consumer Protection   \n",
       "2         Implementing Basel II in the United States   \n",
       "3                  An Assessment of the U.S. Economy   \n",
       "4      Monetary Policy since the Onset of the Crisis   \n",
       "\n",
       "                                        speaker  \\\n",
       "0                  Governor Frederic S. Mishkin   \n",
       "1                      Chairman Ben S. Bernanke   \n",
       "2                  Governor Randall S. Kroszner   \n",
       "3  Vice Chair for Supervision Randal K. Quarles   \n",
       "4                      Chairman Ben S. Bernanke   \n",
       "\n",
       "                                                text  \n",
       "0                                             As ...  \n",
       "1   The concept of financial innovation, it seems...  \n",
       "2   Good afternoon. I would like to thank Standar...  \n",
       "3   Thank you for the opportunity to take part in...  \n",
       "4   When we convened in Jackson Hole in August 20...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"text_\"] = ecb.apply(ecb_pipeline_en, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"text\"].fillna(\"\", inplace=True)\n",
    "ecb[\"speaker\"].fillna(\"Unknown\", inplace=True)\n",
    "fed[\"speaker\"].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                         Auf neuen Wegen zum alten Ziel\n",
       "speaker                                          Yves Mersch\n",
       "text         Auf neuen Wegen zum alten Ziel   Rede von Yv...\n",
       "text_      Rede von Yves Mersch, Mitglied des Direktorium...\n",
       "Name: 151, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text in french\n",
    "ecb.loc[138]\n",
    "# Text in german\n",
    "ecb.loc[151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb[\"lang\"] = ecb[\"text_\"].apply(fast_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comments by Yves Mersch at Financial Services ...</td>\n",
       "      <td>Yves Mersch</td>\n",
       "      <td>Comments by Yves Mersch at Financial Service...</td>\n",
       "      <td>Sustainable economic growth in the real econom...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Securing sustained economic growth in the euro...</td>\n",
       "      <td>Vítor Constâncio</td>\n",
       "      <td>Securing sustained economic growth in the eu...</td>\n",
       "      <td>Ladies and Gentlemen, Thank you for inviting m...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The role of monetary policy in addressing the ...</td>\n",
       "      <td>Mario Draghi</td>\n",
       "      <td>The role of monetary policy in addressing th...</td>\n",
       "      <td>There was a time, not too long ago, when centr...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pandemic emergency: the three challenges f...</td>\n",
       "      <td>Philip R. Lane</td>\n",
       "      <td>SPEECH  The pandemic emergency: the three c...</td>\n",
       "      <td>Today, I will discuss the monetary policy meas...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transmission channels of monetary policy in th...</td>\n",
       "      <td>Peter Praet</td>\n",
       "      <td>Transmission channels of monetary policy in ...</td>\n",
       "      <td>Ladies and Gentlemen, Since the onset of the f...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title           speaker  \\\n",
       "0  Comments by Yves Mersch at Financial Services ...       Yves Mersch   \n",
       "1  Securing sustained economic growth in the euro...  Vítor Constâncio   \n",
       "2  The role of monetary policy in addressing the ...      Mario Draghi   \n",
       "3  The pandemic emergency: the three challenges f...    Philip R. Lane   \n",
       "4  Transmission channels of monetary policy in th...       Peter Praet   \n",
       "\n",
       "                                                text  \\\n",
       "0    Comments by Yves Mersch at Financial Service...   \n",
       "1    Securing sustained economic growth in the eu...   \n",
       "2    The role of monetary policy in addressing th...   \n",
       "3     SPEECH  The pandemic emergency: the three c...   \n",
       "4    Transmission channels of monetary policy in ...   \n",
       "\n",
       "                                               text_ lang  \n",
       "0  Sustainable economic growth in the real econom...   en  \n",
       "1  Ladies and Gentlemen, Thank you for inviting m...   en  \n",
       "2  There was a time, not too long ago, when centr...   en  \n",
       "3  Today, I will discuss the monetary policy meas...   en  \n",
       "4  Ladies and Gentlemen, Since the onset of the f...   en  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed[\"lang\"] = fed[\"text\"].apply(fast_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Importance of Economic Education and Finan...</td>\n",
       "      <td>Governor Frederic S. Mishkin</td>\n",
       "      <td>As ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Financial Innovation and Consumer Protection</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>The concept of financial innovation, it seems...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Implementing Basel II in the United States</td>\n",
       "      <td>Governor Randall S. Kroszner</td>\n",
       "      <td>Good afternoon. I would like to thank Standar...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Assessment of the U.S. Economy</td>\n",
       "      <td>Vice Chair for Supervision Randal K. Quarles</td>\n",
       "      <td>Thank you for the opportunity to take part in...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monetary Policy since the Onset of the Crisis</td>\n",
       "      <td>Chairman Ben S. Bernanke</td>\n",
       "      <td>When we convened in Jackson Hole in August 20...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The Importance of Economic Education and Finan...   \n",
       "1       Financial Innovation and Consumer Protection   \n",
       "2         Implementing Basel II in the United States   \n",
       "3                  An Assessment of the U.S. Economy   \n",
       "4      Monetary Policy since the Onset of the Crisis   \n",
       "\n",
       "                                        speaker  \\\n",
       "0                  Governor Frederic S. Mishkin   \n",
       "1                      Chairman Ben S. Bernanke   \n",
       "2                  Governor Randall S. Kroszner   \n",
       "3  Vice Chair for Supervision Randal K. Quarles   \n",
       "4                      Chairman Ben S. Bernanke   \n",
       "\n",
       "                                                text lang  \n",
       "0                                             As ...   en  \n",
       "1   The concept of financial innovation, it seems...   en  \n",
       "2   Good afternoon. I would like to thank Standar...   en  \n",
       "3   Thank you for the opportunity to take part in...   en  \n",
       "4   When we convened in Jackson Hole in August 20...   en  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_langs = ecb[\"lang\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    1646\n",
       "de      75\n",
       "fr      31\n",
       "es      16\n",
       "it       4\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecb[\"lang\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test hierarchical BERT here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid.\n",
      "Your token has been saved to C:\\Users\\huuta\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"kiddothe2b/hierarchical-transformer-base-4096\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"kiddothe2b/hierarchical-transformer-base-4096\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HATConfig {\n",
       "  \"_name_or_path\": \"kiddothe2b/hierarchical-transformer-base-4096\",\n",
       "  \"architectures\": [\n",
       "    \"HATForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"configuration_hat.HATConfig\",\n",
       "    \"AutoModel\": \"modelling_hat.HATModel\",\n",
       "    \"AutoModelForMaskedLM\": \"modelling_hat.HATForMaskedLM\",\n",
       "    \"AutoModelForMultipleChoice\": \"modelling_hat.HATForMultipleChoice\",\n",
       "    \"AutoModelForQuestionAnswering\": \"modelling_hat.HATForQuestionAnswering\",\n",
       "    \"AutoModelForSequenceClassification\": \"modelling_hat.HATForSequenceClassification\",\n",
       "    \"AutoModelForTokenClassification\": \"modelling_hat.HATForTokenClassification\",\n",
       "    \"AutoTokenizer\": \"tokenization_hat.HATTokenizer\"\n",
       "  },\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"encoder_layout\": {\n",
       "    \"0\": {\n",
       "      \"document_encoder\": false,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"1\": {\n",
       "      \"document_encoder\": false,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"10\": {\n",
       "      \"document_encoder\": false,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"11\": {\n",
       "      \"document_encoder\": true,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"2\": {\n",
       "      \"document_encoder\": true,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"3\": {\n",
       "      \"document_encoder\": false,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"4\": {\n",
       "      \"document_encoder\": false,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"5\": {\n",
       "      \"document_encoder\": true,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"6\": {\n",
       "      \"document_encoder\": false,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"7\": {\n",
       "      \"document_encoder\": false,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"8\": {\n",
       "      \"document_encoder\": true,\n",
       "      \"sentence_encoder\": true\n",
       "    },\n",
       "    \"9\": {\n",
       "      \"document_encoder\": false,\n",
       "      \"sentence_encoder\": true\n",
       "    }\n",
       "  },\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 130,\n",
       "  \"max_sentence_length\": 128,\n",
       "  \"max_sentence_size\": 128,\n",
       "  \"max_sentences\": 32,\n",
       "  \"model_max_length\": 4096,\n",
       "  \"model_type\": \"hierarchical-transformer\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"parameters\": 136350720,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.21.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Some weights of the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 were not used when initializing HATModel: ['hi_transformer.encoder.layer.1.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.8.document_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.1.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.5.document_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.11.document_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.2.document_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.1.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.1.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.10.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.self.value.weight', 'lm_head.decoder.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.11.document_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.8.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.4.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.11.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.2.document_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.8.document_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.8.document_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.9.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.5.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.11.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.9.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.2.document_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.5.document_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.2.document_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.2.position_embeddings.weight', 'hi_transformer.encoder.layer.8.position_embeddings.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.11.document_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.8.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.embeddings.token_type_embeddings.weight', 'hi_transformer.encoder.layer.5.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.1.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.8.document_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.2.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.9.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.2.document_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.5.document_encoder.output.dense.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.11.document_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.2.document_encoder.output.LayerNorm.bias', 'lm_head.decoder.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.output.dense.weight', 'hi_transformer.embeddings.LayerNorm.bias', 'hi_transformer.encoder.layer.5.document_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.9.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.7.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.5.document_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.2.document_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.5.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.5.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.3.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.self.key.weight', 'lm_head.dense.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.2.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.3.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.11.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.11.document_encoder.output.dense.weight', 'hi_transformer.encoder.layer.9.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.11.document_encoder.output.dense.bias', 'hi_transformer.embeddings.position_embeddings.weight', 'hi_transformer.encoder.layer.2.document_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.9.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.8.document_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.5.document_encoder.output.dense.bias', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.3.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.8.document_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.5.document_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.2.document_encoder.output.dense.bias', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.11.document_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.2.document_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.8.document_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.3.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.5.document_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.11.document_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.5.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.embeddings.word_embeddings.weight', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.5.document_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.8.document_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.1.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.11.document_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.5.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.7.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.8.document_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.5.document_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.2.document_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.3.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.11.document_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.7.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.8.document_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.5.position_embeddings.weight', 'lm_head.layer_norm.bias', 'hi_transformer.encoder.layer.5.document_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.embeddings.position_ids', 'hi_transformer.encoder.layer.7.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.5.document_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.2.document_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.11.position_embeddings.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.5.document_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.10.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.1.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.5.document_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.11.document_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.11.document_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.3.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.self.key.bias', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.11.document_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.8.document_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.4.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.2.document_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.5.document_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.8.document_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.output.LayerNorm.weight', 'lm_head.dense.bias', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.output.dense.weight', 'hi_transformer.embeddings.LayerNorm.weight', 'hi_transformer.encoder.layer.5.sentence_encoder.attention.output.dense.weight', 'lm_head.layer_norm.weight', 'hi_transformer.encoder.layer.8.document_encoder.output.dense.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.6.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.output.LayerNorm.bias', 'hi_transformer.encoder.layer.8.document_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.7.sentence_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.0.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.2.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.2.document_encoder.output.dense.weight', 'hi_transformer.encoder.layer.11.document_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.8.document_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.6.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.6.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.output.LayerNorm.bias', 'lm_head.bias', 'hi_transformer.encoder.layer.6.sentence_encoder.intermediate.dense.bias', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.self.query.weight', 'hi_transformer.encoder.layer.2.document_encoder.attention.output.dense.weight', 'hi_transformer.encoder.layer.7.sentence_encoder.attention.output.dense.bias', 'hi_transformer.encoder.layer.0.sentence_encoder.intermediate.dense.weight', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.3.sentence_encoder.attention.self.value.bias', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.8.document_encoder.output.dense.weight', 'hi_transformer.encoder.layer.9.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.4.sentence_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.4.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.5.document_encoder.attention.self.query.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.2.sentence_encoder.output.dense.bias', 'hi_transformer.encoder.layer.11.sentence_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.1.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.10.sentence_encoder.attention.output.LayerNorm.weight', 'hi_transformer.encoder.layer.11.document_encoder.output.LayerNorm.bias', 'hi_transformer.encoder.layer.8.sentence_encoder.attention.self.value.weight', 'hi_transformer.encoder.layer.7.sentence_encoder.output.dense.weight', 'hi_transformer.encoder.layer.2.document_encoder.output.LayerNorm.weight', 'hi_transformer.encoder.layer.11.sentence_encoder.attention.self.key.weight', 'hi_transformer.encoder.layer.11.document_encoder.attention.self.query.weight']\n",
      "- This IS expected if you are initializing HATModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HATModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HATModel were not initialized from the model checkpoint at kiddothe2b/hierarchical-transformer-base-4096 and are newly initialized: ['encoder.layer.7.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.8.document_encoder.output.LayerNorm.weight', 'encoder.layer.5.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.9.sentence_encoder.output.dense.bias', 'encoder.layer.8.position_embeddings.weight', 'encoder.layer.2.document_encoder.intermediate.dense.weight', 'encoder.layer.10.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.5.sentence_encoder.output.dense.weight', 'encoder.layer.0.sentence_encoder.attention.self.key.weight', 'encoder.layer.8.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.2.document_encoder.attention.output.dense.bias', 'encoder.layer.8.document_encoder.intermediate.dense.weight', 'encoder.layer.11.document_encoder.attention.output.dense.bias', 'encoder.layer.4.sentence_encoder.attention.self.key.bias', 'encoder.layer.11.document_encoder.attention.output.LayerNorm.bias', 'encoder.layer.5.sentence_encoder.attention.self.query.bias', 'encoder.layer.4.sentence_encoder.attention.self.key.weight', 'encoder.layer.6.sentence_encoder.attention.self.key.weight', 'encoder.layer.10.sentence_encoder.attention.output.dense.weight', 'encoder.layer.6.sentence_encoder.attention.self.key.bias', 'encoder.layer.0.sentence_encoder.output.dense.bias', 'encoder.layer.2.document_encoder.output.dense.bias', 'encoder.layer.8.document_encoder.intermediate.dense.bias', 'encoder.layer.1.sentence_encoder.intermediate.dense.weight', 'encoder.layer.4.sentence_encoder.intermediate.dense.weight', 'encoder.layer.8.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.9.sentence_encoder.intermediate.dense.bias', 'encoder.layer.5.document_encoder.intermediate.dense.weight', 'encoder.layer.8.sentence_encoder.intermediate.dense.weight', 'encoder.layer.5.document_encoder.output.LayerNorm.bias', 'encoder.layer.5.sentence_encoder.attention.self.query.weight', 'encoder.layer.7.sentence_encoder.attention.output.dense.weight', 'encoder.layer.0.sentence_encoder.output.dense.weight', 'encoder.layer.8.sentence_encoder.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.5.sentence_encoder.attention.output.dense.weight', 'encoder.layer.2.sentence_encoder.intermediate.dense.weight', 'encoder.layer.3.sentence_encoder.attention.self.query.bias', 'encoder.layer.8.document_encoder.attention.output.dense.weight', 'encoder.layer.7.sentence_encoder.attention.self.key.weight', 'encoder.layer.11.document_encoder.attention.self.value.weight', 'encoder.layer.0.sentence_encoder.attention.output.dense.weight', 'encoder.layer.3.sentence_encoder.attention.self.value.weight', 'encoder.layer.11.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.2.document_encoder.attention.output.dense.weight', 'encoder.layer.2.sentence_encoder.attention.self.query.weight', 'encoder.layer.11.sentence_encoder.attention.self.query.bias', 'encoder.layer.11.sentence_encoder.attention.self.value.weight', 'encoder.layer.0.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.5.document_encoder.attention.self.key.bias', 'encoder.layer.6.sentence_encoder.intermediate.dense.weight', 'encoder.layer.10.sentence_encoder.attention.self.value.weight', 'encoder.layer.5.sentence_encoder.intermediate.dense.bias', 'encoder.layer.2.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.4.sentence_encoder.attention.output.dense.weight', 'encoder.layer.7.sentence_encoder.intermediate.dense.weight', 'encoder.layer.5.document_encoder.attention.self.query.weight', 'encoder.layer.11.document_encoder.output.dense.weight', 'encoder.layer.2.document_encoder.attention.output.LayerNorm.weight', 'encoder.layer.8.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.4.sentence_encoder.attention.self.value.bias', 'encoder.layer.8.document_encoder.output.LayerNorm.bias', 'encoder.layer.8.sentence_encoder.output.dense.bias', 'encoder.layer.10.sentence_encoder.attention.output.dense.bias', 'encoder.layer.11.sentence_encoder.attention.self.key.bias', 'encoder.layer.9.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.2.document_encoder.attention.self.key.weight', 'encoder.layer.11.sentence_encoder.output.dense.bias', 'encoder.layer.11.sentence_encoder.attention.self.value.bias', 'encoder.layer.2.document_encoder.attention.self.query.bias', 'encoder.layer.10.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.3.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.2.sentence_encoder.output.dense.weight', 'encoder.layer.6.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.4.sentence_encoder.output.dense.bias', 'encoder.layer.1.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.11.document_encoder.attention.self.value.bias', 'encoder.layer.2.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.3.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.9.sentence_encoder.intermediate.dense.weight', 'encoder.layer.8.document_encoder.attention.self.value.bias', 'encoder.layer.9.sentence_encoder.attention.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.8.sentence_encoder.attention.output.dense.bias', 'encoder.layer.11.document_encoder.output.dense.bias', 'encoder.layer.0.sentence_encoder.intermediate.dense.bias', 'encoder.layer.2.document_encoder.attention.self.value.weight', 'encoder.layer.2.sentence_encoder.attention.self.query.bias', 'encoder.layer.3.sentence_encoder.attention.self.key.bias', 'encoder.layer.1.sentence_encoder.attention.output.dense.bias', 'encoder.layer.7.sentence_encoder.attention.self.query.bias', 'encoder.layer.8.document_encoder.output.dense.bias', 'encoder.layer.1.sentence_encoder.attention.self.query.weight', 'encoder.layer.9.sentence_encoder.attention.self.query.bias', 'encoder.layer.11.sentence_encoder.intermediate.dense.weight', 'encoder.layer.11.document_encoder.attention.output.LayerNorm.weight', 'encoder.layer.2.document_encoder.intermediate.dense.bias', 'encoder.layer.8.sentence_encoder.output.dense.weight', 'encoder.layer.7.sentence_encoder.attention.self.value.bias', 'encoder.layer.3.sentence_encoder.attention.output.dense.bias', 'encoder.layer.1.sentence_encoder.output.dense.weight', 'encoder.layer.11.document_encoder.intermediate.dense.weight', 'encoder.layer.10.sentence_encoder.output.dense.weight', 'encoder.layer.9.sentence_encoder.attention.self.value.weight', 'encoder.layer.11.document_encoder.attention.self.key.bias', 'encoder.layer.6.sentence_encoder.output.dense.weight', 'encoder.layer.4.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.5.document_encoder.output.LayerNorm.weight', 'encoder.layer.5.document_encoder.intermediate.dense.bias', 'encoder.layer.11.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.8.document_encoder.attention.self.key.bias', 'encoder.layer.9.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.3.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.4.sentence_encoder.attention.output.dense.bias', 'encoder.layer.7.sentence_encoder.intermediate.dense.bias', 'encoder.layer.7.sentence_encoder.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.sentence_encoder.attention.self.value.bias', 'encoder.layer.0.sentence_encoder.attention.self.value.weight', 'encoder.layer.11.document_encoder.attention.output.dense.weight', 'encoder.layer.5.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.11.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.11.document_encoder.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.sentence_encoder.attention.output.dense.bias', 'encoder.layer.3.sentence_encoder.attention.self.value.bias', 'encoder.layer.1.sentence_encoder.attention.self.value.bias', 'encoder.layer.5.document_encoder.attention.self.value.bias', 'encoder.layer.5.document_encoder.attention.output.dense.weight', 'encoder.layer.10.sentence_encoder.intermediate.dense.weight', 'encoder.layer.2.document_encoder.output.LayerNorm.weight', 'encoder.layer.3.sentence_encoder.intermediate.dense.weight', 'encoder.layer.4.sentence_encoder.attention.self.value.weight', 'encoder.layer.5.sentence_encoder.attention.self.value.weight', 'encoder.layer.5.document_encoder.attention.self.value.weight', 'encoder.layer.8.sentence_encoder.attention.self.query.weight', 'encoder.layer.9.sentence_encoder.attention.output.dense.weight', 'encoder.layer.11.document_encoder.intermediate.dense.bias', 'encoder.layer.11.sentence_encoder.attention.output.dense.weight', 'encoder.layer.0.sentence_encoder.attention.output.dense.bias', 'encoder.layer.6.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.6.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.2.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.8.sentence_encoder.attention.self.key.bias', 'encoder.layer.5.position_embeddings.weight', 'encoder.layer.5.sentence_encoder.attention.self.key.weight', 'encoder.layer.5.sentence_encoder.attention.output.dense.bias', 'encoder.layer.8.document_encoder.attention.output.LayerNorm.weight', 'encoder.layer.0.sentence_encoder.attention.self.query.bias', 'encoder.layer.8.sentence_encoder.attention.output.dense.weight', 'encoder.layer.10.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.1.sentence_encoder.attention.self.value.weight', 'encoder.layer.11.document_encoder.output.LayerNorm.weight', 'encoder.layer.5.document_encoder.output.dense.weight', 'encoder.layer.5.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.1.sentence_encoder.attention.self.query.bias', 'encoder.layer.3.sentence_encoder.attention.self.query.weight', 'encoder.layer.0.sentence_encoder.attention.self.value.bias', 'encoder.layer.0.sentence_encoder.attention.self.query.weight', 'encoder.layer.7.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.2.document_encoder.attention.self.value.bias', 'encoder.layer.5.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.5.sentence_encoder.attention.self.key.bias', 'encoder.layer.6.sentence_encoder.attention.self.value.bias', 'encoder.layer.6.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.7.sentence_encoder.output.dense.bias', 'encoder.layer.5.document_encoder.attention.self.query.bias', 'encoder.layer.4.sentence_encoder.attention.self.query.weight', 'encoder.layer.2.sentence_encoder.attention.output.dense.weight', 'encoder.layer.2.sentence_encoder.attention.self.value.weight', 'encoder.layer.2.document_encoder.output.dense.weight', 'encoder.layer.7.sentence_encoder.attention.self.query.weight', 'encoder.layer.1.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.2.sentence_encoder.attention.output.dense.bias', 'encoder.layer.3.sentence_encoder.attention.self.key.weight', 'encoder.layer.8.sentence_encoder.intermediate.dense.bias', 'encoder.layer.5.document_encoder.attention.output.LayerNorm.weight', 'encoder.layer.11.sentence_encoder.intermediate.dense.bias', 'encoder.layer.5.sentence_encoder.intermediate.dense.weight', 'encoder.layer.10.sentence_encoder.attention.self.key.weight', 'encoder.layer.11.document_encoder.output.LayerNorm.bias', 'encoder.layer.8.document_encoder.attention.output.dense.bias', 'encoder.layer.5.sentence_encoder.output.dense.bias', 'encoder.layer.5.document_encoder.attention.self.key.weight', 'encoder.layer.2.document_encoder.output.LayerNorm.bias', 'encoder.layer.5.document_encoder.attention.output.dense.bias', 'encoder.layer.6.sentence_encoder.output.dense.bias', 'encoder.layer.10.sentence_encoder.attention.self.query.weight', 'encoder.layer.4.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.7.sentence_encoder.attention.self.value.weight', 'encoder.layer.1.sentence_encoder.attention.self.key.weight', 'encoder.layer.1.sentence_encoder.intermediate.dense.bias', 'encoder.layer.2.sentence_encoder.attention.self.key.weight', 'encoder.layer.7.sentence_encoder.attention.output.dense.bias', 'encoder.layer.9.sentence_encoder.attention.self.key.weight', 'encoder.layer.9.sentence_encoder.attention.self.key.bias', 'encoder.layer.1.sentence_encoder.attention.self.key.bias', 'encoder.layer.2.sentence_encoder.output.dense.bias', 'encoder.layer.8.document_encoder.attention.self.query.bias', 'encoder.layer.3.sentence_encoder.output.dense.weight', 'encoder.layer.2.document_encoder.attention.output.LayerNorm.bias', 'encoder.layer.6.sentence_encoder.intermediate.dense.bias', 'encoder.layer.2.document_encoder.attention.self.query.weight', 'encoder.layer.10.sentence_encoder.attention.self.query.bias', 'encoder.layer.1.sentence_encoder.attention.output.dense.weight', 'encoder.layer.2.sentence_encoder.attention.self.value.bias', 'encoder.layer.8.document_encoder.attention.self.query.weight', 'encoder.layer.10.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.0.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.8.document_encoder.attention.output.LayerNorm.bias', 'encoder.layer.2.sentence_encoder.attention.self.key.bias', 'encoder.layer.6.sentence_encoder.attention.self.query.weight', 'encoder.layer.2.document_encoder.attention.self.key.bias', 'encoder.layer.8.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.11.sentence_encoder.attention.output.dense.bias', 'encoder.layer.5.document_encoder.output.dense.bias', 'encoder.layer.8.sentence_encoder.attention.self.query.bias', 'encoder.layer.10.sentence_encoder.attention.self.value.bias', 'encoder.layer.0.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.10.sentence_encoder.intermediate.dense.bias', 'encoder.layer.0.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.8.document_encoder.attention.self.key.weight', 'encoder.layer.8.document_encoder.output.dense.weight', 'encoder.layer.9.sentence_encoder.attention.self.query.weight', 'encoder.layer.7.sentence_encoder.attention.self.key.bias', 'encoder.layer.0.sentence_encoder.intermediate.dense.weight', 'encoder.layer.4.sentence_encoder.output.dense.weight', 'encoder.layer.9.sentence_encoder.attention.self.value.bias', 'encoder.layer.3.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.1.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.2.position_embeddings.weight', 'encoder.layer.3.sentence_encoder.attention.output.dense.weight', 'encoder.layer.4.sentence_encoder.attention.self.query.bias', 'encoder.layer.6.sentence_encoder.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.7.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.11.document_encoder.attention.self.key.weight', 'encoder.layer.1.sentence_encoder.output.dense.bias', 'encoder.layer.5.document_encoder.attention.output.LayerNorm.bias', 'encoder.layer.11.position_embeddings.weight', 'encoder.layer.0.sentence_encoder.attention.self.key.bias', 'encoder.layer.2.sentence_encoder.output.LayerNorm.weight', 'encoder.layer.1.sentence_encoder.attention.output.LayerNorm.weight', 'encoder.layer.3.sentence_encoder.intermediate.dense.bias', 'encoder.layer.9.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.11.sentence_encoder.attention.self.query.weight', 'encoder.layer.11.sentence_encoder.attention.self.key.weight', 'encoder.layer.6.sentence_encoder.attention.output.dense.bias', 'encoder.layer.6.sentence_encoder.attention.output.dense.weight', 'encoder.layer.4.sentence_encoder.intermediate.dense.bias', 'encoder.layer.6.sentence_encoder.attention.self.query.bias', 'encoder.layer.10.sentence_encoder.output.dense.bias', 'encoder.layer.4.sentence_encoder.output.LayerNorm.bias', 'encoder.layer.8.document_encoder.attention.self.value.weight', 'encoder.layer.10.sentence_encoder.attention.self.key.bias', 'encoder.layer.8.sentence_encoder.attention.self.key.weight', 'encoder.layer.11.sentence_encoder.output.dense.weight', 'encoder.layer.4.sentence_encoder.attention.output.LayerNorm.bias', 'encoder.layer.2.sentence_encoder.intermediate.dense.bias', 'encoder.layer.9.sentence_encoder.output.dense.weight', 'encoder.layer.7.sentence_encoder.output.dense.weight', 'encoder.layer.8.sentence_encoder.attention.self.value.bias', 'encoder.layer.3.sentence_encoder.output.dense.bias', 'encoder.layer.11.document_encoder.attention.self.query.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"kiddothe2b/hierarchical-transformer-base-4096\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m   \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mType:\u001b[0m        HATTokenizer\n",
      "\u001b[1;31mString form:\u001b[0m <transformers_modules.kiddothe2b.hierarchical-transformer-base-4096.97ba9b93529bf90c29e36389eb079caa2b203dd8.tokenization_hat.HATTokenizer object at 0x00000202FC8D8FD0>\n",
      "\u001b[1;31mLength:\u001b[0m      50265\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\huuta\\.cache\\huggingface\\modules\\transformers_modules\\kiddothe2b\\hierarchical-transformer-base-4096\\97ba9b93529bf90c29e36389eb079caa2b203dd8\\tokenization_hat.py\n",
      "\u001b[1;31mDocstring:\u001b[0m   <no docstring>"
     ]
    }
   ],
   "source": [
    "tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 31414, 232, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# One must provide a list of strings to the tokenizer. A single string won't work.\n",
    "tokens = tokenizer([\"Hello world\"], padding='max_length', max_length=4096)\n",
    "print(tokens[\"input_ids\"])\n",
    "print(tokens[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huuta\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\transformers\\modeling_utils.py:713: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithSentenceAttentions(last_hidden_state=tensor([[[ 0.2913, -1.4626, -0.0767,  ...,  1.8455,  0.2612,  1.3971],\n",
      "         [ 1.2999, -1.1006, -0.3852,  ...,  1.4813,  0.9634,  0.9905],\n",
      "         [-0.0073, -1.4696, -0.8785,  ...,  1.0659,  0.5045,  1.4827],\n",
      "         ...,\n",
      "         [ 1.2753, -1.0961, -1.6010,  ...,  1.0138, -0.2057,  1.3753],\n",
      "         [ 1.2753, -1.0961, -1.6010,  ...,  1.0138, -0.2057,  1.3753],\n",
      "         [ 1.2753, -1.0961, -1.6010,  ...,  1.0138, -0.2057,  1.3753]]],\n",
      "       grad_fn=<AsStridedBackward0>), hidden_states=None, attentions=None, sentence_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "output = model(torch.Tensor(tokens[\"input_ids\"]).int(), attention_mask = torch.Tensor(tokens[\"attention_mask\"]).int())\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m      \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mType:\u001b[0m           HATForMaskedLM\n",
      "\u001b[1;31mString form:\u001b[0m   \n",
      "HATForMaskedLM(\n",
      "           (hi_transformer): HATModel(\n",
      "           (embeddings): HATEmbeddings(\n",
      "           (word_embedd <...> mentwise_affine=True)\n",
      "           (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "           )\n",
      "           )\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\huuta\\.cache\\huggingface\\modules\\transformers_modules\\kiddothe2b\\hierarchical-transformer-base-4096\\97ba9b93529bf90c29e36389eb079caa2b203dd8\\modelling_hat.py\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "HAT Model with a `language modeling` head on top.\n",
      "\n",
      "This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
      "library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
      "etc.)\n",
      "\n",
      "This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
      "Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
      "and behavior.\n",
      "\n",
      "Parameters:\n",
      "    config ([`HATConfig`]): Model configuration class with all the parameters of the\n",
      "        model. Initializing with a config file does not load the weights associated with the model, only the\n",
      "        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule."
     ]
    }
   ],
   "source": [
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentivePooling(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = config.hidden_dropout_prob\n",
    "        self.lin_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.v = nn.Linear(config.hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lin_out = self.lin_proj(inputs)\n",
    "        attention_weights = torch.tanh(self.v(lin_out)).squeeze(-1)\n",
    "        attention_weights_normalized = torch.softmax(attention_weights, -1)\n",
    "        return torch.sum(attention_weights_normalized.unsqueeze(-1) * inputs, 1)\n",
    "\n",
    "\n",
    "class HATPooler(nn.Module):\n",
    "    def __init__(self, config, pooling='max'):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.pooling = pooling\n",
    "        if self.pooling == 'attentive':\n",
    "            self.attentive_pooling = AttentivePooling(config)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.max_sentence_length = config.max_sentence_length\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        if self.pooling == 'attentive':\n",
    "            pooled_output = self.attentive_pooling(hidden_states)\n",
    "        else:\n",
    "            pooled_output = torch.max(hidden_states, dim=1)[0]\n",
    "        pooled_output = self.dense(pooled_output)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.framework_dataset import get_data_loader\n",
    "from model.framework_model import CorpusEncoder, ClassificationHead, MyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_01\",\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.,\n",
    "\n",
    "    \"batch_size\": 2,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.3,\n",
    "\n",
    "    \"separate\": True,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ce = CorpusEncoder(method=config[\"method\"],\n",
    "                   separate=config[\"separate\"],\n",
    "                   dropout=config[\"dropout\"]).to(device)\n",
    "clf = ClassificationHead(\n",
    "    corpus_emb_dim=ce.corpus_emb_dim, nontext_dim=nb_nontextfeatures,\n",
    "    layers=config[\"max_corpus_len\"], dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "my_model = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed output successfully. ce ouput = \n",
      " tensor([[ 0.5188,  0.3291,  0.1633, -0.1473,  0.1349, -0.4494, -0.3119,  0.1545,\n",
      "          0.0786, -0.2876,  0.3910,  0.3875, -0.3660, -0.3608,  0.2498, -0.2504,\n",
      "         -0.2098,  0.4205,  0.4584, -0.0644,  0.0358,  0.1918, -0.0308,  0.3077,\n",
      "          0.1890, -0.0861,  0.0858, -0.8284, -0.1828,  0.4115,  0.1117,  0.2787,\n",
      "         -0.2774,  0.3394,  0.0273,  0.5999,  0.1513,  0.1244,  0.1631, -0.2311,\n",
      "         -0.3805,  0.0701,  0.0895,  0.0474,  0.1385, -0.0742, -0.2884,  0.4224,\n",
      "         -0.0044, -0.3157, -0.0460, -0.1186, -0.3529, -0.1792,  0.4930,  0.2769,\n",
      "          0.0461, -0.0694, -0.0678, -0.1934,  0.0267,  0.0373,  0.0128, -0.3020],\n",
      "        [ 0.4916,  0.3016,  0.1952,  0.0704,  0.1516, -0.3817, -0.3201,  0.0201,\n",
      "          0.0241, -0.2107,  0.4107,  0.4672, -0.2213, -0.2495,  0.1668, -0.1596,\n",
      "         -0.0287,  0.3923,  0.3666, -0.0917,  0.0404,  0.1947, -0.0886,  0.3141,\n",
      "          0.0573, -0.0093,  0.1364, -0.6961, -0.1473,  0.2655,  0.0778,  0.4400,\n",
      "         -0.1724,  0.3525,  0.0302,  0.4705,  0.1628,  0.0807, -0.0063,  0.0027,\n",
      "         -0.3088, -0.0487, -0.0259,  0.1001,  0.0841,  0.0080, -0.2609,  0.5771,\n",
      "         -0.0967, -0.2778,  0.0108,  0.0454, -0.2893, -0.1868,  0.4714,  0.1398,\n",
      "          0.1346, -0.2019, -0.1702, -0.0063,  0.1228,  0.0223, -0.0443, -0.2832]])\n",
      "corpus encoder ouput shape =  torch.Size([2, 64]) \n",
      "corpus embed dim =  64\n",
      "torch.Size([2, 19])\n",
      "Classifier output =  \n",
      " tensor([0.5504, 0.5033])\n",
      "torch.Size([2, 2, 512])\n",
      "torch.Size([2, 2, 512])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Test output\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ce.eval()\n",
    "    X_ecb = batch[\"X_ecb\"].to(device)\n",
    "    X_ecb_att = batch[\"X_ecb_mask\"].to(device)\n",
    "    X_fed = batch[\"X_fed\"].to(device)\n",
    "    X_fed_att = batch[\"X_fed_mask\"].to(device)\n",
    "    X_ind =  batch[\"X_ind\"].to(device)\n",
    "    y = batch[\"label\"]\n",
    "    X_text = (X_ecb, X_fed)\n",
    "    X_att = (X_ecb_att, X_fed_att)\n",
    "    ce_output = ce(X_text, X_att)\n",
    "    print(\"Computed output successfully. ce ouput = \\n\", ce_output)\n",
    "    print(\"corpus encoder ouput shape = \", ce_output.size(), \"\\ncorpus embed dim = \", ce.corpus_emb_dim)\n",
    "    print(X_ind.size())\n",
    "\n",
    "\n",
    "    clf_output = clf(ce_output, X_ind)\n",
    "    \n",
    "    print(\"Classifier output =  \\n\", clf_output)\n",
    "    my_model_output = my_model(X_text, X_att, X_ind)\n",
    "\n",
    "print(X_ecb.size())\n",
    "print(X_ecb_att.size())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    \"method\": \"model_01\",\n",
    "\n",
    "    \"learning_rate\": 0.001,\n",
    "\n",
    "    \"weight_decay\": 0.,\n",
    "\n",
    "    \"batch_size\": 2,\n",
    "\n",
    "    \"layers\": 3,\n",
    "\n",
    "    \"dropout\": 0.5,\n",
    "\n",
    "    \"separate\": False,\n",
    "    \n",
    "    \"max_corpus_len\": 2\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_train, ecb, fed, y_train, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "val_set, val_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_val, ecb, fed, y_val, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")\n",
    "\n",
    "test_set, test_loader, tokenizer, steps = get_data_loader(\n",
    "    returns_test, ecb, fed, y_test, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], max_corpus_len=config[\"max_corpus_len\"],\n",
    "    batch_size=config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ce = CorpusEncoder(method=config[\"method\"],\n",
    "                   separate=config[\"separate\"],\n",
    "                   dropout=config[\"dropout\"]).to(device)\n",
    "clf = ClassificationHead(\n",
    "    corpus_emb_dim=ce.corpus_emb_dim, nontext_dim=nb_nontextfeatures,\n",
    "    layers=config[\"max_corpus_len\"], dropout=config[\"dropout\"]\n",
    ").to(device)\n",
    "my_model = MyModel(\n",
    "    nontext_dim=nb_nontextfeatures, method=config[\"method\"],\n",
    "    separate=config[\"separate\"], dropout=config[\"dropout\"]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ready\\AppData\\Local\\Temp\\ipykernel_4736\\1290859485.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  z = torch.range(0, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.range(0, 15)\n",
    "print(z)\n",
    "z.view(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed output successfully. ce ouput = \n",
      " tensor([[ 0.4157,  0.1194,  0.3883,  0.0341,  0.0856,  0.1622,  0.0121,  0.0527,\n",
      "         -0.2364, -0.3298,  0.0536, -0.1610, -0.0075, -0.1486, -0.4683, -0.4899,\n",
      "         -0.3891,  0.0252,  0.0037, -0.3633, -0.2859, -0.0357,  0.0311,  0.3272,\n",
      "          0.0782,  0.2936, -0.2265,  0.0849,  0.4000, -0.1206, -0.0824,  0.0014],\n",
      "        [ 0.4882,  0.1265,  0.3807,  0.0256,  0.0367,  0.0750, -0.0288,  0.0552,\n",
      "         -0.3381, -0.3272, -0.0165, -0.1988,  0.0848, -0.1289, -0.4908, -0.5434,\n",
      "         -0.3220,  0.0030, -0.0315, -0.3173, -0.3220,  0.0066,  0.0303,  0.3315,\n",
      "          0.1331,  0.2751, -0.2566,  0.1179,  0.3966, -0.0763, -0.0906,  0.0263]],\n",
      "       dtype=torch.float64)\n",
      "corpus encoder ouput shape =  torch.Size([2, 32]) \n",
      "corpus embed dim =  32\n",
      "torch.Size([2, 19])\n",
      "Classifier output =  \n",
      " tensor([0.5048, 0.5103])\n",
      "tensor([[[  101, 21658,  3242,  ...,  2008, 17653,   102],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101,  1996, 18144,  ...,  2093,  7909,   102],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0]]])\n",
      "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]]])\n",
      "tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Test output\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    ce.eval()\n",
    "    X_text = (batch[\"X_text\"].to(device),)\n",
    "    X_mask = (batch[\"X_mask\"].to(device),)\n",
    "    X_ind =  batch[\"X_ind\"].to(device)\n",
    "    y = batch[\"label\"]\n",
    "    ce_output = ce(X_text, X_mask)\n",
    "    print(\"Computed output successfully. ce ouput = \\n\", ce_output)\n",
    "    print(\"corpus encoder ouput shape = \", ce_output.size(), \"\\ncorpus embed dim = \", ce.corpus_emb_dim)\n",
    "    print(X_ind.size())\n",
    "\n",
    "\n",
    "    clf_output = clf(ce_output, X_ind)\n",
    "    \n",
    "    print(\"Classifier output =  \\n\", clf_output)\n",
    "    my_model_output = my_model(X_text, X_mask, X_ind)\n",
    "\n",
    "print(X_ecb)\n",
    "print(X_ecb_att)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 10/2684 [01:43<7:40:26, 10.33s/batch, accuracy=55, loss=0.687] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(my_model, train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader,config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m      2\u001b[0m       device\u001b[39m=\u001b[39;49mdevice, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, eval_every\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdummy\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\IPP\\NLP_Natixis_Git\\nlp-challenge-x-natixis\\train.py:116\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, config, device, max_epochs, eval_every, name)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39m# print(output)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y)\n\u001b[1;32m--> 116\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    117\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    119\u001b[0m \u001b[39m# Computing predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ready\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ready\\anaconda3\\envs\\nlp-env\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(my_model, train_loader=train_loader, val_loader=val_loader,config=config,\n",
    "      device=device, max_epochs=1, eval_every=1, name=\"dummy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a4c350da27618d5732fc58ebcab8d2c0381c51b7361f332741f21e30512bbdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
